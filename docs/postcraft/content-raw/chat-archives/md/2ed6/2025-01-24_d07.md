# [Troubleshooting Test Configuration Issues](https://claude.ai/chat/2ed64c7c-b9aa-4ff3-865f-daee65d11987)

d07277c3-dbe5-4801-9249-0303e40e5659

 Looking at the source code, the default configuration uses:

- Chat model: `qwen2:1.5b`
- Embedding model: `nomic-embed-text`

Let's update the Ollama integration tests accordingly:
```
Viewing artifacts created via the Analysis Tool web feature preview isnâ€™t yet supported on mobile.
```



Before running tests, ensure these models are installed:

```bash
ollama pull nomic-embed-text
ollama pull qwen2:1.5b
```

Q1: Should we add a test configuration option to use mock LLM responses instead of requiring Ollama?

Q2: Would you like to implement a fallback mechanism for when Ollama is unavailable?

Q3: Should we add validation for model availability before running tests?

Q4: Would you like to create separate test suites for different LLM providers?

---

* **sourceFile** : /home/danny/github-danny/hyperdata/docs/chat-archives/data-2025-01-25-12-45-58/conversations.json
* **appName** : claude-json-converter
* **appPath** : claude-json-converter
* **subtask** : [undefined]
* **rootDir** : /home/danny/github-danny/transmissions/src/applications/claude-json-converter
* **dataDir** : /home/danny/github-danny/transmissions/src/applications/claude-json-converter/data
* **targetPath** : /home/danny/github-danny/transmissions/src/applications/claude-json-converter/data
* **tags** : ccc10.ccc20.ccc30.ccc40.ccc50
* **done** : [undefined]
* **meta** : {
  "conv_uuid": "2ed64c7c-b9aa-4ff3-865f-daee65d11987",
  "conv_name": "Troubleshooting Test Configuration Issues",
  "updated_at": "2025-01-24T16:28:41.994847Z"
}
* **filepath** : /home/danny/github-danny/hyperdata/docs/postcraft/content-raw/chat-archives/md/2ed6/2025-01-24_d07.md