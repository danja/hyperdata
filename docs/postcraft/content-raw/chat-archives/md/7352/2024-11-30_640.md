# [Extending Event-Driven Processors to Support Streaming](https://claude.ai/chat/7352ddcc-bebe-4ba3-b94b-48f2e20746ab)

6408ec61-a411-437c-9c29-628044c00383

 I'll create comprehensive integration tests for the complete streaming pipeline.

<antArtifact identifier="streaming-pipeline-integration-test" type="application/vnd.ant.code" language="javascript" title="StreamingPipeline.integration.spec.js">
import { expect } from 'chai';
import path from 'path';
import fs from 'fs/promises';
import { createReadStream, createWriteStream } from 'fs';
import StreamingPipelineExample from '../../src/examples/StreamingPipelineExample.js';
import StreamingFileReader from '../../src/processors/fs/StreamingFileReader.js';
import StreamMonitor from '../../src/utils/StreamMonitor.js';
import StreamBufferManager from '../../src/utils/StreamBufferManager.js';
import { pipeline } from 'stream/promises';

describe('Streaming Pipeline Integration', () => {
    const TEST_DIR = path.join(process.cwd(), 'test-data');
    const INPUT_FILE = path.join(TEST_DIR, 'input-large.txt');
    const OUTPUT_FILE = path.join(TEST_DIR, 'output-large.txt');
    const CHUNK_SIZE = 64 * 1024; // 64KB chunks
    
    // Setup test data
    before(async () => {
        await fs.mkdir(TEST_DIR, { recursive: true });
        await generateTestFile(INPUT_FILE, 10 * 1024 * 1024); // 10MB test file
    });

    // Cleanup after tests
    after(async () => {
        try {
            await fs.rm(TEST_DIR, { recursive: true });
        } catch (err) {
            console.error('Cleanup error:', err);
        }
    });

    async function generateTestFile(filePath, size) {
        const writeStream = createWriteStream(filePath);
        const chunkSize = 1024;
        const iterations = Math.ceil(size / chunkSize);
        
        for (let i = 0; i < iterations; i++) {
            const chunk = Buffer.from('x'.repeat(Math.min(chunkSize, size - (i * chunkSize))));
            if (!writeStream.write(chunk)) {
                await new Promise(resolve => writeStream.once('drain', resolve));
            }
        }
        writeStream.end();
        await new Promise(resolve => writeStream.once('finish', resolve));
    }

    describe('Complete Pipeline', () => {
        let pipeline;

        beforeEach(() => {
            pipeline = new StreamingPipelineExample({
                chunkSize: CHUNK_SIZE,
                highWaterMark: CHUNK_SIZE * 4
            });
        });

        it('should process large file correctly', async () => {
            const metrics = await pipeline.process(INPUT_FILE, OUTPUT_FILE);
            
            // Verify file sizes match
            const inputStats = await fs.stat(INPUT_FILE);
            const outputStats = await fs.stat(OUTPUT_FILE);
            expect(outputStats.size).to.equal(inputStats.size);

            // Verify metrics
            expect(metrics.input.throughput).to.be.above(0);
            expect(metrics.buffer.overflowEvents).to.be.at.most(10);
            expect(metrics.output.chunksProcessed).to.be.above(0);
        }).timeout(30000);

        it('should handle backpressure effectively', async () => {
            const slowPipeline = new StreamingPipelineExample({
                chunkSize: CHUNK_SIZE,
                highWaterMark: CHUNK_SIZE * 2,
                artificial_delay: 10 // ms delay per chunk
            });

            const metrics = await slowPipeline.process(INPUT_FILE, OUTPUT_FILE);
            
            // Verify backpressure was managed
            expect(metrics.buffer.pauseCount).to.be.above(0);
            expect(metrics.buffer.resumeCount).to.be.above(0);
            expect(metrics.buffer.maxBufferUsage).to.be.below(2 * 1024 * 1024);
        }).timeout(60000);
    });

    describe('Component Integration', () => {
        it('should connect components correctly', async () => {
            const reader = new StreamingFileReader({ chunkSize: CHUNK_SIZE });
            const monitor1 = new StreamMonitor({ name: 'input' });
            const bufferManager = new StreamBufferManager({
                maxBufferSize: 1024 * 1024,
                highWaterMark: 819200
            });
            const monitor2 = new StreamMonitor({ name: 'output' });

            // Create writable stream for testing
            const chunks = [];
            const testWritable = new WritableStream({
                write(chunk) {
                    chunks.push(chunk);
                    return Promise.resolve();
                }
            });

            await pipeline(
                reader.initStream(),
                monitor1,
                bufferManager,
                monitor2,
                testWritable
            );

            // Verify component metrics
            const inputMetrics = monitor1.getMetrics();
            const bufferMetrics = bufferManager.getMetrics();
            const outputMetrics = monitor2.getMetrics();

            expect(inputMetrics.chunksProcessed).to.equal(outputMetrics.chunksProcessed);
            expect(bufferMetrics.overflowEvents).to.be.at.most(5);
        });

        it('should handle errors gracefully', async () => {
            const reader = new StreamingFileReader({ chunkSize: CHUNK_SIZE });
            const monitor = new StreamMonitor({ name: 'error-test' });
            const errorBuffer = new StreamBufferManager({
                maxBufferSize: 1024
            });

            // Inject error
            errorBuffer._transform = function(chunk, encoding, callback) {
                if (this.currentBufferSize > 512) {
                    callback(new Error('Simulated error'));
                } else {
                    callback(null, chunk);
                }
            };

            try {
                await pipeline(
                    reader.initStream(),
                    monitor,
                    errorBuffer
                );
                expect.fail('Should have thrown error');
            } catch (err) {
                expect(err.message).to.equal('Simulated error');
                expect(monitor.getMetrics().chunksProcessed).to.be.above(0);
            }
        });
    });

    describe('Performance Tests', () => {
        it('should maintain consistent throughput', async () => {
            const pipeline = new StreamingPipelineExample({
                chunkSize: CHUNK_SIZE,
                highWaterMark: CHUNK_SIZE * 8
            });

            const metrics = await pipeline.process(INPUT_FILE, OUTPUT_FILE);
            
            // Calculate throughput stability
            const throughputVariance = calculateVariance(metrics.monitor1.throughputHistory);
            expect(throughputVariance).to.be.below(1000000); // Allow some variation

            // Verify processing time is reasonable
            const processingTime = metrics.output.lastTimestamp - metrics.input.startTime;
            const throughputMBps = (metrics.input.bytesProcessed / 1024 / 1024) / (processingTime / 1000);
            expect(throughputMBps).to.be.above(5); // Minimum 5MB/s
        }).timeout(30000);
    });
});

function calculateVariance(numbers) {
    const mean = numbers.reduce((a, b) => a + b) / numbers.length;
    return numbers.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / numbers.length;
}

class WritableStream extends require('stream').Writable {
    constructor(options) {
        super({ objectMode: true, ...options });
        this._write = options.write;
    }
}
</antArtifact>

This integration test suite:

1. Tests complete pipeline functionality:
   - End-to-end processing
   - Backpressure handling
   - Memory usage
   - Error scenarios

2. Verifies component interactions:
   - Correct connection of components
   - Data flow between components
   - Error propagation

3. Tests performance characteristics:
   - Throughput consistency 
   - Memory usage patterns
   - Processing time

4. Includes utilities for:
   - Test data generation
   - Metrics validation
   - Error simulation

The tests are designed to catch:
- Memory leaks
- Backpressure issues
- Component interaction problems
- Performance degradation
- Error handling problems

q1: Would you like additional stress tests for extreme conditions?

q2: Should we add more specific metrics validation?

q3: Would you like tests for different data types/formats?

q4: Should we add benchmarking against non-streaming implementation?

---

* **sourceFile** : /home/danny/github-danny/hyperdata/docs/chat-archives/data-2025-01-25-12-45-58/conversations.json
* **appName** : claude-json-converter
* **appPath** : claude-json-converter
* **subtask** : [undefined]
* **rootDir** : /home/danny/github-danny/transmissions/src/applications/claude-json-converter
* **dataDir** : /home/danny/github-danny/transmissions/src/applications/claude-json-converter/data
* **targetPath** : /home/danny/github-danny/transmissions/src/applications/claude-json-converter/data
* **tags** : ccc10.ccc20.ccc30.ccc40.ccc50
* **done** : [undefined]
* **meta** : {
  "conv_uuid": "7352ddcc-bebe-4ba3-b94b-48f2e20746ab",
  "conv_name": "Extending Event-Driven Processors to Support Streaming",
  "updated_at": "2024-11-30T22:29:56.938448Z"
}
* **filepath** : /home/danny/github-danny/hyperdata/docs/postcraft/content-raw/chat-archives/md/7352/2024-11-30_640.md