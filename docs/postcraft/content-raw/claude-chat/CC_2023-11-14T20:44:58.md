# Untitled

## 0

{
  "uuid": "92fe1a32-122e-442e-ab41-4ce31dfe44ce",
  "text": "    Act as an academic research expert. Draft a comprehensive research paper outline on \"Sociological aspects of Fake News in Current Affairs\". The outline should be well-structured, starting with a compelling introduction that states the problem or question, the relevance of the topic, and the objectives of the research. It should then detail the main points and subpoints, incorporating relevant literature and methods of investigation. Ensure that the proposed research is sound, innovative, and contributes to the existing body of knowledge in the field. Conclude the outline with expected results or implications, and a bibliography of primary and secondary sources. Ensure that all sources are credible and up-to-date. The final product should serve as a roadmap for drafting a full-fledged research paper.",
  "sender": "human",
  "created_at": "2023-11-14T20:20:59.471832Z",
  "updated_at": "2023-11-14T20:20:59.471832Z",
  "attachments": [
    {
      "file_name": "0309Lazer_DRAFT.pdf",
      "file_size": 21016,
      "file_type": "pdf",
      "extracted_content": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/323650280\nThe science of fake news\nArticle in Science · March 2018\nDOI: 10.1126/science.aao2998\nCITATIONS\n3,041\nREADS\n55,116\n16 authors, including:\nMatthew Baum\nHarvard University\n199 PUBLICATIONS 9,198 CITATIONS\nSEE PROFILE\nFilippo Menczer\nIndiana University Bloomington\n283 PUBLICATIONS 25,329 CITATIONS\nSEE PROFILE\nMiriam Metzger\nUniversity of California, Santa Barbara\n15 PUBLICATIONS 3,115 CITATIONS\nSEE PROFILE\nGordon Pennycook\nCornell University\n205 PUBLICATIONS 22,649 CITATIONS\nSEE PROFILE\nAll content following this page was uploaded by Steven A. Sloman on 25 June 2018.\nThe user has requested enhancement of the downloaded file.\n\nsciencemag.org SCIENCE\nILLUSTRATION: TK\nBy David Lazer, Matthew Baum, Yochai\nBenkler, Adam Berinsky, Kelly Greenhill,\nFilippo Menczer, Miriam Metzger,\nBrendan Nyhan, Gordon Pennycook, David\nRothschild, Michael Schudson, Steven\nSloman, Cass Sunstein, Emily Thorson,\nDuncan Watts, Jonathan Zittrain\nT\nhe rise of fake news highlights the\nerosion of long-standing institutional\nbulwarks against misinformation in\nthe internet age. Concern over the\nproblem is global. However, little is\nknown regarding fundamental issues\nabout the vulnerabilities of individuals, in-\nstitutions, and society to manipulations by\nmalicious actors. Below we discuss extant\nsocial and computer science research regard-\ning belief in fake news and the mechanisms\nby which it spreads. We focus on unanswered\nscientific questions raised by the prolifera-\ntion of its most recent, politically oriented in-\ncarnation. Beyond selected references in the\ntext, suggested further reading can be found\nin the supplementary materials.\nWHAT IS FAKE NEWS?\nWe define “fake news” to be fabricated in-\nformation that mimics news media content\nin form but not in organizational process or\nintent. Fake news outlets, in turn, lack the\nnews media’s editorial norms and processes\nfor ensuring the accuracy and credibility of\ninformation. Fake news overlaps with other\ninformation disorders, such as misinforma-\ntion (false or misleading information) and\ndisinformation (false information that is pur-\nposely spread to deceive people).\nFake news has primarily drawn recent at-\ntention in a political context, but it also has\nbeen documented in information promul-\ngated about topics such as vaccination, nu-\ntrition, and stock values. It is particularly\npernicious in that it is parasitic on standard\nnews outlets, simultaneously benefiting from\nand undermining their credibility.\nOur definition of fake news makes no\nassumptions about the characteristics of\nsources or amplification strategies. Some—\nnotably First Draft and Facebook favor “false\nnews” because of the use of fake news as a\npolitical weapon (1). We have retained it be-\ncause of its value as a scientific construct, and\nbecause its political salience draws attention\nto an important subject.\nTHE HISTORICAL SETTING\nJournalistic norms of objectivity and balance\narose as a backlash among journalists against\nthe widespread use of propaganda in World\nWar I (particularly their own role in propa-\ngating it) and the rise of corporate public\nrelations in the 1920s. Local and national oli-\ngopolies created by the dominant 20th cen-\ntury technologies of information distribution\n(print and broadcast) sustained these norms.\nThe internet has reduced many of those con-\nstraints on news dissemination. This allows\noutlets that do not embody these norms to\ncompete online with those that do so more ef-\nfectively than was possible offline. It has con-\ntributed to the abandonment of traditional\nnews sources that had enjoyed high levels of\npublic trust and credibility. General trust in\nSOCIAL SCIENCE\nThe science of fake news\nAddressing fake news requires a multidisciplinary effort\nINSIGHTS\nP OLICY FORUM\nThe list of author affiliations is provided in the supplementary\nmaterials. Email: d.lazer@neu.edu\nIllustration will be sent to authors\n2 8 MARCH 2018 • VOL 359 ISSUE 6380\n\nSCIENCE sciencemag.org\nthe mass media has collapsed to historic lows\nin 2016, with 51% of Democrats and 14% of\nRepublicans expressing “a fair amount” or “a\ngreat deal” of trust in mass media as a news\nsource (2).\nThe United States has had a parallel evo-\nlution in its geo-socio-political environment.\nGeographic polarization of partisan prefer-\nences has dramatically increased over the\npast 40 years, reducing opportunities for\ncross-cutting political interaction. Homoge-\nneous social networks, in turn, reduce toler-\nance for alternative views, amplify attitudinal\npolarization, boost the likelihood of accepting\nideologically compatible news, and increase\nclosure to new information. Dislike of the\n“other side” (affective polarization) has also\nrisen. These trends have created a context in\nwhich fake news can attract a mass audience.\nPREVALENCE AND IMPACT\nHow common is fake news, and what is its\nimpact on individuals? Solutions should be\ncommensurate to the magnitude of the prob-\nlem, but there are surprisingly few scientific\nanswers to these basic questions.\nIn evaluating the fake news prevalence, we\nadvocate focusing on the original sources—\nthe publishers—rather than individual sto-\nries, because we view the defining element of\nfake news to be the intent and processes of\nthe publisher. A focus on publishers also al-\nlows us to avoid the morass of trying to evalu-\nate the accuracy of every single news story.\nOne study evaluating the dissemination of\nprominent fake news stories estimated that\nthe average American encountered between\none and three stories from known publish-\ners of fake news during the month before\nthe 2016 election (3). This likely is a conser-\nvative estimate because the study tracked\nonly 156 fake news stories. Another study\nreported that false information on Twitter\nis typically retweeted by many more people,\nand far more rapidly, than true informa-\ntion, especially when the topic is politics (4).\nFacebook has estimated that manipulations\nby malicious actors accounted for less than\none-tenth of 1% of civic content shared on the\nplatform (5), although it has not presented\ndetails of its analysis.\nBy liking, sharing, and searching for infor-\nmation, social bots (automated accounts im-\npersonating humans) can magnify the spread\nof fake news by orders of magnitude. By one\nrecent estimate—that classified accounts\nbased on observable features such as shar-\ning behavior, number of ties, and linguistic\nfeatures—between 9 and 15% of active Twit-\nter accounts are bots (6). Facebook estimated\nthat as many as 60 million bots (7) may be\ninfesting its platform. They were responsible\nfor a substantial portion of political content\nposted during the 2016 U.S. campaign, and\nsome of the same bots were later used to at-\ntempt to influence the 2017 French election\n(8). Bots are also deployed to manipulate al-\ngorithms used to predict potential engage-\nment with content by a wider population.\nIndeed, a Facebook white paper reports wide-\nspread efforts to carry out this sort of manip-\nulation during the 2016 U.S. election (5).\nHowever, in the absence of methods to\nderive representative samples of bots and\nhumans on a given platform, any point esti-\nmates of bot prevalence must be interpreted\ncautiously. Bot detection will always be a\ncat-and-mouse game in which a large, but\nunknown, number of humanlike bots may go\nundetected. Any success at detection, in turn,\nwill inspire future countermeasures by bot\nproducers. Identification of bots will there-\nfore be a major ongoing research challenge.\nWe do know that, as with legitimate news,\nfake news stories have gone viral on social\nmedia. However, knowing how many indi-\nviduals encountered or shared a piece of fake\nnews is not the same as knowing how many\npeople read or were affected by it. Evaluating\nmedium-to-long–run impact on political be-\nhavior of exposure to fake news (for example,\nwhether and how to vote) is essentially non-\nexistent in the literature. The impact might\nbe small—evidence suggests that efforts by\npolitical campaigns to persuade individuals\nmay have limited effects (9). However, me-\ndiation of much fake news via social media\nmight accentuate its effect because of the\nimplicit endorsement that comes with shar-\ning. Beyond electoral impacts, what we know\nabout the effects of media more generally\nsuggests many potential pathways of influ-\nence, from increasing cynicism and apathy\nto encouraging extremism. There exists little\nevaluation of the impacts of fake news in\nthese regards.\nPOTENTIAL INTERVENTIONS\nWhat interventions might be effective at\nstemming the flow and influence of fake\nnews? We identify two categories of inter-\nventions: (i) those aimed at empowering\nindividuals to evaluate the fake news they\nencounter, and (ii) structural changes aimed\nat preventing exposure of individuals to fake\nnews in the first instance.\nEmpowering individuals\nThere are many forms of fact checking, from\nwebsites that evaluate factual claims of news\nreports, such as PolitiFact and Snopes, to\nevaluations of news reports by credible news\nmedia, such as the Washington Post, to con-\ntextual information regarding content in-\nserted by intermediaries, such as those used\nby Facebook.\nDespite the apparent elegance of fact\nchecking, science supporting its efficacy is, at\nbest, mixed. This may reflect broader tenden-\ncies in collective cognition, as well as struc-\ntural changes in our society. Individuals tend\nnot to question the credibility of information\nunless it violates their preconceptions or they\nare incentivized to do so. Otherwise, they\nmay accept information uncritically. People\nalso tend to align their beliefs with the values\nof their community.\nResearch also further demonstrates that\npeople prefer information that confirms\ntheir preexisting attitudes (selective expo-\nsure), view information consistent with their\npreexisting beliefs as more persuasive than\ndissonant information (confirmation bias),\nand are inclined to accept information that\npleases them (desirability bias). Prior parti-\nsan and ideological beliefs might prevent ac-\nceptance of fact checking of a given fake news\nstory.\nFact checking might even be counterpro-\nductive under certain circumstances. Re-\nsearch on fluency—the ease of information\nrecall—and familiarity bias in politics shows\nthat people tend to remember information,\nor how they feel about it, while forgetting the\ncontext within which they encountered it.\nMoreover, they are more likely to accept fa-\nmiliar information as true (10). There is thus\na risk that repeating false information, even\nin a fact-checking context, may increase an\nindividual’s likelihood of accepting it as true.\nThere is mixed evidence about effectiveness\nof claim repetition in fact checking (11).\nAlthough experimental and survey re-\nsearch have confirmed that the perception of\ntruth increases when misinformation is re-\npeated, this may not occur if the misinforma-\ntion is paired with a valid retraction. Some\nresearch suggests that repetition of the mis-\ninformation before its correction may even\nbe beneficial. Further research is needed to\nreconcile these contradictions and determine\nthe conditions under which fact-checking in-\nterventions are most effective.\nAnother longer-run approach seeks to\nimprove individual evaluation of the quality\nof information sources through education.\nThere has been a proliferation of efforts to\ninject training of critical-information skills\nin primary and secondary schools (12). How-\never, it is uncertain whether such efforts im-\nprove assessments of information credibility\nor if any such effects will persist over time. An\nemphasis on fake news might also have the\nunintended consequence of reducing the per-\nceived credibility of real-news outlets. There\nis a great need for rigorous program evalu-\nation of different educational interventions.\nPlatform-based detection and intervention:\nAlgorithms and bots\nInternet platforms have become the most\nimportant enablers and primary conduits of\n8 MARCH 2018 • VOL 359 ISSUE 6380 3\n\nsciencemag.org SCIENCE\nINSIGHTS \n| \nPOLICY F ORUM\nfake news. It is inexpensive to create a web-\nsite that has the trappings of a professional\nnews organization. It has also been easy to\nmonetize content through online ads and\nsocial media dissemination. The internet\nnot only provides a medium for publishing\nfake news but offers tools to actively promote\ndissemination.\nAbout 47% of Americans overall report get-\nting news from social media often or some-\ntimes, with Facebook as, by far, the dominant\nsource (13). Social media are key conduits for\nfake news sites (3). Indeed, Russia success-\nfully manipulated all of the major platforms\nduring the 2016 U.S. election, according to\nrecent testimony (7).\nHow might the internet and social media\nplatforms help reduce the spread and impact\nof fake news? Google, Facebook, and Twitter\nare often mediators not only of our relation-\nship with the news media but also with our\nfriends and relatives. Generally, their busi-\nness model relies on monetizing attention\nthrough advertising. They use complex statis-\ntical models to predict and maximize engage-\nment with content (14). It should be possible\nto adjust those models to increase emphasis\non quality information.\nPlatforms could provide con-\nsumers with signals of source\nquality that could be incorpo-\nrated into the algorithmic rank-\nings of content. They could\nminimize the personalization of\npolitical information relative to\nother types of content (reducing\nthe creation of “echo chambers”).\nFunctions that emphasize cur-\nrently trending content could seek to exclude\nbot activity from measures of what is trend-\ning. More generally, platforms could curb the\nautomated spread of news content by bots\nand cyborgs (users who automatically share\nnews from a set of sources, with or without\nreading them), although for the foreseeable\nfuture, bot producers will likely be able to de-\nsign effective countermeasures.\nThe platforms have attempted each of\nthese steps and others (5, 15). Facebook an-\nnounced an intent to shift its algorithm to\naccount for “quality” in its content curation\nprocess. Twitter announced that it blocked\ncertain accounts linked to Russian misinfor-\nmation and informed users exposed to those\naccounts that they may have been duped.\nHowever, the platforms have not provided\nenough detail for evaluation by the research\ncommunity or subjected their findings to\npeer review, making them problematic for\nuse by policy-makers or the general public.\nWe urge platforms to collaborate with\nindependent academics on evaluating the\nscope of the fake news issue and the design\nand effectiveness of interventions. There is\nlittle research focused on fake news and no\ncomprehensive data-collection system to\nprovide a dynamic understanding of how\npervasive systems of fake news provision\nare evolving. It is impossible to recreate the\nGoogle of 2010. Google could not do so even\nif it had the underlying code, because the\npatterns emerge from a complex interaction\namong code, content, and users. However, it\nis possible to record what the Google of 2018\nis doing. More generally, researchers need to\nconduct a rigorous, ongoing audit of how the\nmajor platforms filter information.\nThere are challenges to scientific collabo-\nration from the perspectives of industry and\nacademia. Yet, there is an ethical and social\nresponsibility, transcending market forces,\nfor the platforms to contribute what data\nthey uniquely can to a science of fake news.\nPossible effectiveness of platform-based\npolicies suggests either self-regulation by the\nplatforms or government intervention. Direct\ngovernment regulation of an area as sensitive\nas news carries its own risks, constitutional\nand otherwise. For instance, could govern-\nment regulators maintain (and, as important,\nbe seen as maintaining) impartiality in defin-\ning, imposing, and enforcing any\nrequirements? Generally, any di-\nrect intervention by government\nor the platforms that prevents\nusers from seeing content raises\nconcerns about either govern-\nment or corporate censorship.\nAn alternative to direct gov-\nernment regulation would be to\nenable tort lawsuits alleging, for\nexample, defamation by those di-\nrectly and concretely harmed by the spread\nof fake news. To the extent that an online\nplatform assisted in the spreading of a mani-\nfestly false (but still persuasive) story, there\nmight be avenues for liability consistent with\nexisting constitutional law, which, in turn,\nwould pressure platforms to intervene more\nregularly. In the U.S. context, however, a pro-\nvision of the 1996 Communications Decency\nAct offers near-comprehensive immunity to\nplatforms for false or otherwise actionable\nstatements penned by others. Any change to\nthis legal regime would raise thorny issues\nabout the extent to which platform content\n(and content-curation decisions) should be\nsubject to second guessing by people alleging\ninjury. The European “right to be forgotten”\nin search engines is testing these issues.\nStructural interventions generally raise\nlegitimate concerns about respecting private\nenterprise and human agency. But just as the\nmedia companies of the 20th century shaped\nthe information to which individuals were\nexposed, the far-more-vast internet oligopo-\nlies are already shaping human experience\non a global scale. The questions before us are\nhow those immense powers are being—and\nshould be—exercised and how to hold these\nmassive companies to account.\nA FUTURE AGENDA\nOur call is to promote interdisciplinary re-\nsearch to reduce the spread of fake news\nand to address the underlying pathologies\nit has revealed. Failures of the U.S. news me-\ndia in the early 20th century led to the rise\nof journalistic norms and practices that, al-\nthough imperfect, generally served us well by\nstriving to provide objective, credible infor-\nmation. We must redesign our information\necosystem in the 21st century. This must be\nglobal in scope, as many countries, some of\nwhich have never developed a robust news\necosystem, face challenges around fake and\nreal news that are more acute than in the\nUnited States. More broadly, we must answer\na fundamental question: How can we create\na news ecosystem and culture that values and\npromotes truth? j\nREFERENCES AND NOTES\n1. C. Wardle, H. Derakhshan, “Information disorder: Toward\nan interdisciplinary framework for research and policy\nmaking” [Council of Europe policy report DGI(2017)09,\nCouncil of Europe, 2017]; https://firstdraftnews.com/\nwp-content/uploads/2017/11/PREMS-162317-GBR-\n2018-Report-de%CC%81sinformation-1.pdf?x29719.\n2. A. Swift, Americans’ trust in mass media sinks to new low\n(Gallup, 2016); www.gallup.com/poll/195542/americans-\ntrust-mass-media-sinks-new-low.aspx.\n3. H. Allcott, M. Gentzkow, J. Econ. Perspect. 31, 211 (2017).\n4. S. Vosoughi et al., Science 359, XXX (2018).\n5. J. Weedon et al., Information operations and Facebook\n(Facebook, 2017); https://fbnewsroomus.files.wordpress.\ncom/2017/04/facebook-and-information-operations-v1.\npdf.\n6. O. Varol et al., in Proceedings of the 11th AAAI Conference on\nWeb and Social Media (Association for the Advancement of\nArtificial Intelligence, Montreal, 2017), pp. 280–289.\n7. Senate Judiciary Committee, Extremist content\nand Russian disinformation online: Working with\ntech to find solutions (Committee on the Judiciary,\n2017); www.judiciary.senate.gov/meetings/\nextremist-content-and-russian-disinformation-online-\nworking-with-tech-to-find-solutions.\n8. E. Ferrara, First Monday 22, 2017 (2017).\n9. J. L. Kalla, D. E. Broockman, Am. Polit. Sci. Rev. 112, 148\n(2018).\n10. B. Swire et al., J. Exp. Psychol. Learn. Mem. Cogn. 43, 1948\n(2017).\n11. U. K. H. Ecker et al., J. Appl. Res. Mem. Cogn. 6, 185 (2017).\n12. C. Jones, Bill would help California schools teach about\n“fake news,” media literacy (EdSource, 2017); https://\nedsource.org/2017/bill-would-help-california-schools-\nteach-about-fake-news-media-literacy/582363.\n13. Gottfried, E. Shearer, News use across social\nmedia platforms 2017, Pew Research Center, 7\nSeptember 2017; www.journalism.org/2017/09/07/\nnews-use-across-social-media-platforms-2017/.\n14. E. Bakshy et al., Science 348, 1130 (2015).\n15. C. Crowell, Our approach to bots & misinformation,\nTwitter, 14 June 2017; https://blog.twitter.com/official/\nen_us/topics/company/2017/Our-Approach-Bots-\nMisinformation.html.\nACKNOWLEDGMENTS\nWe acknowledge support from the Shorenstein Center at\nthe Harvard Kennedy School and the NULab for Texts, Maps,\nand Networks at Northeastern University. D.L. acknowledges\nsupport by the Economic and Social Research Council ES/\nN012283/1. D.L. and M.B. contributed equally to this article.\nSUPPLEMENTARY MATERIAL\nwww.sciencemag.org/content/359/xxx/xxx/suppl/DC1\n10.1126/science.aao2998\nPullquote or\nliftout quote\npiece tops\non baseline\nas shown a\n4 8 MARCH 2018 • VOL 359 ISSUE 6380\nView publication stats"
    },
    {
      "file_name": "Fake_news_as_a_social_phenomenon_in_the.pdf",
      "file_size": 58339,
      "file_type": "pdf",
      "extracted_content": "3\nSociologie Românească\nISSN: 2668-1455 (print), ISSN: 1220-5389 (electronic)\nFAKE NEWS AS A SOCIAL PHENOMENON IN THE DIGITAL AGE:\nA SOCIOLOGICAL RESEARCH AGENDA\nGabriel-Alexandru TOMA\nSociologie Românească, 2021, vol. 19, Issue 1, pp. 134-153\nhttps://doi.org/10.33788/sr.19.1.7\nPublished by:\nExpert Projects Publishing House\nOn behalf of:\nAsociația Română de Sociologie\nexpert projects\npublishing\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n134\nFAKE NEWS AS A SOCIAL\nPHENOMENON IN THE DIGITAL AGE:\nA SOCIOLOGICAL RESEARCH AGENDA\nGabriel-Alexandru TOMA\n1\nAbstract\nThis paper proposes a sociological research agenda for analysing the spreading\nmechanisms of misinformation in contemporary society. The contemporary\nfake news phenomenon is approached as an emergent outcome of inter-related\ntechnological, economic, socio-cultural and political factors that have made\nsociety vulnerable to misinformation. Those factors are understood as generators\nof various social dynamics rather than as direct causal determinants. In order\nto better acknowledge the conditions under which fake news is propagated and\nlegitimated in digital society, news should be approached not only as a commodity\nthat functions in a market-driven economy, but also as a social institution that\nregulates political discourse and public debate. Based on these considerations,\nthe conclusions support the necessity of a more prominent sociological focus on\nmedia studies, which could build awareness of the performative role of language\nin the context of technologically mediated realities.\nKeywords: fake news; misinformation; propaganda; attention economy.\nIntroduction\nMisinformation is an important issue on the public agenda. Studies show that\nthe circulation of misleading information on social media played an important role\nboth in the 2016 United States presidential election and in the Brexit referendum\n(Bastos, & Mercea, 2017; Fourney et al., 2017). Many countries are facing similar\nissues in terms of misinformation as a means of propaganda (Khaldarova, & Pantti,\n2016; Wasserman, 2020). False stories and deceptive reports are employed by\nvarious interest groups to increase their image capital and gain social or political\ninfluence. Therefore, fake news has a significant impact on democratic processes\nby influencing citizens’ capacity to act responsibly based on informed decisions.\n1 \nNational University of Political Science and Public Administration & Association\nfor Social Entrepreneurship “Plus One”, Bucharest, ROMANIA. E-mail:\ntomagabriel2003@gmail.com.\n\n135\nThe phenomenon of fake news is linked to various forms of social polarisation\nthat spread extremist attitudes and generate latent conflicts between groups with\nantagonistic ideologies (Spohr, 2017). In this context, the phenomenon of fake\nnews threatens social stability and negatively affects the prestige of the journalism\nprofession.\nThis paper proposes some directions for analysing fake news from a sociological\nperspective that could complement and strengthen media studies by contributing\nto an enhanced understanding of how misinformation ecosystems emerge in\ncurrent digital worlds. Various sociological approaches might be employed to\nsystematically address the problem of misinformation and formulate explanations\nof why fake news is a reality today and how it comes to play important social and\npolitical roles. After justifying the relevance of a conceptualization of fake news\nas a social phenomenon, we propose some tracks to analyse fake news through the\nsociological lens by focusing on different sociological branches: (1) sociology of\ninfluence and persuasion, (2) sociology of journalistic practice and media industry,\n(3) sociology of online communities and digitally-mediated worlds, (4) sociology\nof culture and discourse, and (5) social psychology.\nFake news as a social phenomenon\nNews as a genre is an institutionalised form of communication that operates\naccording to rigid stylistic and rhetorical conventions (Miller, 1984). News is an\nobject with symbolic authority and integrates clear norms about what is being\ncommunicated and how it is communicated. Therefore, the relationship between\nnews producers and a public is based on norms of relevance and trustworthiness.\nThese norms are collectively acknowledged and are sometimes taken for granted\nin news consumption practices. In general, news represents a mediator between\nhumans and their social environment. By signalling what is important in the world,\nnews delivers a definition of the world and provides people with interpretative\nrepertoires to make sense of specific social realities. Consequently, news shapes\nthe knowledge of the world in powerful ways.\nThe media of information have undergone multiple transformations related\nto the digital revolution (Meltzer, 2009). Foremost among these is the fact that\nfake news emerged as a particular subgenre. As understood in the present paper,\nfake news represents content that resembles the style, form and rhetoric of news\nas a form of institutionalised communication but is detached from the norms and\nconventions that approach reliable information as a collective good. Fake news\nis very difficult to differentiate from factual news due to several reasons. On one\nhand, people are not always able to verify the accuracy of the content. On the other\nhand, the evidence itself can be manipulated. Furthermore, the boundaries between\nfake and accurate news are difficult to draw, since both forms of communication\ncirculate within the same channels and are based on similar consumption and\nSTUDII\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n136\ndistribution practices (Fernandez, 2017): fake news uses the social legitimacy of\nfactual news to influence public opinion.\nThis paper discusses fake news as a social phenomenon. Fake news can be\nconsidered a social phenomenon from several perspectives. First, fake news\nis widespread across social and cultural contexts, with digital technologies\ncontributing to the rapid dissemination of misleading content in a public space.\nSecondly, fake news has impact on social order: misleading information influences\nvoting behaviour, causes social movements, imposes issues on the political agenda,\nchanges public perception and ultimately changes reality. Thirdly, fake news is a\ncollective accomplishment: fake news results from various collective processes\nand practices, including not only the production of false stories but also their\ndistribution and consumption and people’s receptivity to the stories. Fake news is\nco-produced between news producers, news media and the audience that performs\nan act of authentication. Without people believing deceptive content, fake news\nwould remain a work of fiction (Marchi, 2012; Tandoc et al., 2017). The individual’s\nbehaviour alone cannot enhance the impact of fake news, but the coordinated\nactions and technological affordances that enable the spread of misinformation do.\nFake news is influential because people rely on it to orient themselves in the world.\nFourthly, fake news emerges through joint actions that work upon knowledge\nstructures and social experiences. From this perspective, fake news is a social\nfact (Durkheim, 1982) because it exerts constraining influences on individuals’\nexperience of the world.\nSociology of influence and persuasion\nRevisiting classical theories of media influence\nThe number of publishing platforms has grown, and the news ecosystem has\nundergone significant transformations. One of these is related to the development\nof alternative sources of information that are outside the control of conventional\ngatekeepers as social actors meant to safeguard the credibility and quality of\ncontent (Bro, & Wallberg, 2014). The internet enables numerous content providers\nto enter the online news landscape. Since content production and marketing costs\nare affordable, users can reach a mass audience and influence targeted groups\n(Tandoc Jr. et al., 2018). In particular, the virtue of the online space lies in its\nform of organisation based on principles of accessibility, equality and pluralism.\nNamely, the internet allows laypersons to participate in the production of content,\ninform a general public and spread their views at little cost (Coleman, 2017). The\nonline environment is socio-technically organised to allow virtually everyone to\nproduce and distribute digital media content. Despite the numerous opportunities\nfor civic activism and mass mobilisation, the ease of information creation and\ndistribution in the online environment poses great challenges for the quality of\n\n137\nSTUDII\nthe online news industry, an industry that has become vulnerable to fake news as\na means of propaganda and manipulation.\nWhile substitute forms of gate keeping practices emerged in the digital sphere,\nthe legacy media started being criticised for allowing the editors’ ideological biases\nand commercial interests to influence decisions about which issues to publish\n(Haselmayer et al., 2017; Kaye, & Johnson, 2016). This lowered people’s trust in\nthe legacy media, which has contributed to the proliferation of yellow journalism\nunder the form of websites that publish information unselectively.\nIn general, news media have the power to focus collective attention on key\npublic issues (agenda setting) and to influence how different problems are defined\nand represented (attribute agenda setting). In a diffuse media ecology, fake news\nis highly influential in the creation of public awareness by establishing a dominant\ndiscourse regarding a wide range of policy topics (Brenner, 1988). People tend to\nallocate more importance to issues covered in the news, since these issues become\npart of their proximate social environments. Even if fake news is detached from\nevents in the real world, it still pushes issues onto the public agenda and dictates\nthe tone in which those issues are to be thought about. Fake news gained influence\nin a context in which legacy media are intricately connected with digital media.\nA comprehensive study of the media environment documented the agenda-setting\npower of fake news (Vargo, Guo, & Amazeen, 2018). The results show that fake\nnews is responsive to the agendas of partisan media. Moreover, the same study\nevidences that both partisan and emerging media are susceptible to fake news’ lack\nof fact-checking. Therefore, we assist at a process of intermedia agenda setting\nin which fake news has real consequences by influencing the agenda of various\nplatforms. In these circumstances, the issue addressed by fake news becomes\nimportant rather than the facts. Since the media agenda is a result of the influence\nthat some groups exert in society, fake news depends on the interest of those\ngroups to legitimate particular social issues in order to gain political support for\ncontroversial reforms.\nTaking stock of the pressure to innovate in the persuasion industry\nThe contemporary fake news phenomenon appeared in a context in which\nthe persuasion industry faced increased pressure to innovate. News is a form of\ncommunication that appeals to an authoritative and influential voice to transmit a\nmessage of collective significance. Public relations specialists and other experts\nin strategic communication rely on the news format to promote particular ideas\nand offer more legitimacy to their points (Russel, 2015). Fake news, however, has\nbeen used as an alternative to traditional advertising and electoral spots due to\nthe need for a more persuasive voice to speak for the public by using a familiar\nand accessible medium.\nPublic opinion is a powerful force in modern societies. With the development\nof survey research and polls, public opinion has turned into a universe to be\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n138\nknown and understood. Public relations, advertising and propaganda are social\ninstitutions that have developed on these grounds. Since political actors extract\npower by shaping and controlling public opinion, the development of effective\nstrategies to manipulate people’s beliefs and desires has also become a key priority\nin electoral campaigns. Media play an important role in the formation of public\nopinion by providing resources to address the general population, along with\npersuasive symbols to distribute political influence. However, public opinion is a\nsocial fiction used to designate forms of collective agency and make sense of the\nworld as a governable realm in a democratic society (Wilson, 2013). Fake news\nhas become a political instrument to build authoritative messages in a context in\nwhich public opinion is understood as an object that acts and is acted upon.\nDeconstructing the politics of elections\nThe contemporary fake news phenomenon emerged in a context in which\nemotions are considered a pivotal factor in persuasive political communication.\nTelevision and digital media rely on a visual imaginary that favours a more\npersonalised coverage of politics. Political debate is turned into a spectacle, with\ntelegenic and charismatic political leaders as the main protagonists. In this process,\nvoters are reduced to the role of spectators, and voting behaviour is understood as\na decision-making process governed by emotions rather than by critical reasoning\nor instrumental motives (Garry, 2013). Visual and interactive media appeal to\ncompelling and impactful messages able to generate political capital by promoting\nan emotional experience of the world.\nTherefore, fake news appeared in a context in which politics is disconnected\nfrom policy. In the post-truth society of today, emotions rather than factual\nfoundations are the basis on which political power is distributed (Rochlin, 2017).\nWebsites, blogs, posts and other similar sources make it difficult for people to\nfilter out biased content. The actual information ecosystem poses some challenges\nregarding how to select and identify a reliable source. Expert opinions are losing\ntheir authority and trustworthiness, as digital media has changed what counts as\nevidence. The rhetoric emerged in a post-truth world is based on a facticity of\nstatements that are detached from foundational realities. Political platforms built\non ambitious governmental projects are less influential than political platforms\nthat manipulate human emotions and fears (Bakir, & McStay, 2018). Furthermore,\nthe ideological groupings across the left-right spectrum are no longer meaningful\nto describe political orientations in a world in which the political game is played\nwith new rules and strategies.\n\n139\nSTUDII\nSociology of journalistic practice and media industry\nUnderstanding the business cultures of news production and dissemination\nLegacy journalism faces significant challenges in the digital area. Social\nmedia have absorbed some of the editorial functions of legacy news media. The\nproliferation of digital journalism has given people the opportunity to access free\nnews online. This has led to a decrease in the readership of major newspapers\nand audiences of radio and television stations, with media institutions being\nconfronted with drops in advertising revenue. In this context, legacy journalism\nas an industry has not always been able to develop sustainable business models\n(Cook, & Sirkkunen, 2013). Since legacy journalism is a market-driven economy,\nsensitive to contextual factors and dependent on public or private sources of\nincome, the transformations in the digital environment have brought about a crisis\nin the business model of legacy journalism organisations, which have accordingly\nbecome less secure (Schlesinger, & Doyle, 2014). Consequently, many media\norganisations have decreased the number of editorial and journalist staff positions.\nThe pressure to adapt to the imperatives of the digital world has impeded legacy\nmedia organisations from investing in accurate reporting.\nMedia agencies face pressure to develop business models that keep them\ncompetitive in an uncertain digital arena. This has led to the development of a\nbusiness culture that rejects investments that do not bring immediate financial\nbenefits. Nowadays, media companies have switched from protective strategies\nto opportunities-based strategies (Franklin, 2017), a process in which the quality\nof the news is negatively affected. In an ever-changing digital world, reputation\nis volatile and does not represent an economically value asset. The development\nof short-term strategies to increase profitability has reduced the importance of\nbuilding long-term reputation (Allcott, & Gentzkow, 2017). Fake news has evolved\nin a context in which the interest in developing an attractive format for storytelling\nwas more important than building a long-term relationship with the public based\non trust. Therefore, fake news has appeared as a value creation attempt in a highly\ncompetitive environment driven by instant gratification.\nThe magnitude of the fake news phenomenon invites us to rethink journalism\nand find a sustainable business model in news delivery and consumption. One\npossibility would be to switch from a profit-driven to a community-driven\njournalism. In such a model, users would deliberately pay to support a social cause\nby deciding what type of content to consume. This might build loyal audiences\nand establish an editorial commitment to quality.\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n140\nExploring the identity of the journalistic profession\nCollaborative editorial policy proved favourable for the development of the\nfake news phenomenon, especially in a context in which there was no systematic\nand powerful response from the journalist community (Richardson, 2017).\nFurthermore, a growing number of online information publishers resulted in a de-\nprofessionalisation of journalistic work. Journalism has started losing its identity\nas a community of practice, and nowadays, the boundaries between journalism,\npropaganda and activism are difficult to draw.\nCitizen journalism appeared as a response to media corporations dominated by\neconomic logic. Citizen journalism integrates forms of news reporting that rely on\nless institutionalised practices and allow people without professional journalism\ntraining to inform one another. Citizen journalism is a pivotal ‘watchdog’ practice\nin democratic societies (Watson, 2011). Driven by a desire to protect public\ninterests, citizen journalism practices might prevent abuses of power and draw\nattention to governmental wrongdoings, especially when mainstream media\nseem loyal to political parties and other interest groups. However, the rise of\nthe fake news phenomenon may be interpreted as an unintended consequence of\ncitizen journalism. Since potentially all citizen can be involved in disseminating\ninformation, the distinction between formal and informal discourse has become\nmore difficult to draw. Moreover, the authority of the source loses its referential\nfunction in the context of participatory user-generated content. Fact-checking\nplatforms and stance-detection algorithms aim to recover the authority of the\nsources as an orientation strategy for news consumers (Verma, Fleischmann, &\nKoltai, 2017).\nAssessing the regulatory frameworks in the diffusion of information\nThe rapid spread of fake news in digital environments caught society unprepared.\nThe phenomenon of fake news has gained momentum in the context of a loose\nregulation system that was unable to preserve freedom of speech, while still\ndiscouraging the production of deceptive content. Nevertheless, the actors that\norchestrated disinformation campaigns could barely be held accountable for the\nconsequences of their actions, since there is no law to address the phenomenon\nin appropriate legal terms. The publishers of falsities can only be liable under\ndefamation or commercial law if it is proven either that someone suffered\nreputational damage or that injurious falsehood brought financial loss to an\nenterprise (Pearson, 2017). Furthermore, fake news has become an influential\nphenomenon not strictly because someone produced deceptive content but because\nusers shared and diffused unverified information in a network. In this context, it\nis virtually impossible to apply sanctions for the spread of misleading stories,\nyet this is basically the key practice that guarantees the propagandistic success\nof fake news.\n\n141\nSTUDII\nSociology of online communities and digitally-mediated worlds\nComprehending the consequences of anonymity on the internet\nThe growth of user-generated content represents a factor that has increased\nthe amount of false information put into circulation with various aims: fun, profit,\npropaganda. Various interest groups have exploited the autarchic nature of the\ninternet and have deliberately disseminated deceptive content to accomplish their\nhidden agendas. The anonymous and pseudonymous features of online media have\nencouraged the propagation of fake news (Pew Research Center, 2017). Authors\nof misleading stories and reports can remain hidden under an avatar, thus making\nit difficult for consumers to identify the true origin of a content and decode the\nmessage properly. This situation erodes important norms of social communication\nby making content creators less accountable for the information they spread\n(Persily, 2017). Also, anonymity can lead to abusive behaviour and inflammatory\ndialogue, which may create an emotional collective drive that increases the impact\nof false information. However, anonymity is essential in preserving the freedom of\nexpression and right to privacy that characterise the internet as a democratic space.\nAnalysing the role of networks in mass communication and content\ncirculation\nThe internet has democratised the production and distribution of information in\nall forms, so people have a larger variety of sources to access. In today’s dynamic\ndigital environment, content is circulated more than ever before. Online social\nnetworks allow rapid dissemination of information to a mass audience. The impact\nof this is a double-edged sword. On one side, online media are effective tools for\nconstructive communication: they empower minority groups to have a voice; they\nmake people aware of the latest scientific advances; they boost aid campaigns\nfor people in need and so on. On the other side, online media feed fake news by\ntaking advantage of the freedoms that characterise a democratic process. Once\nestablished, false or deceptive statements are difficult to correct due to the rapid,\nwidespread and uncontrolled dissemination of false information (Mustafaraj, &\nMetaxas, 2017). As Mustafaraj and Metaxas (2017) reveal, the process of fake\nnews circulation leaves online traces that continue to shape public thinking on\nissues with social and political relevance even after debunking.\nOnline social networks originated as tools for social interaction. However, they\nhave progressively become used for entertainment purposes, marketing campaigns\nand ultimately for news consumption. Now, social networks are important sources\nof information about politics and public issues (Allcott, & Gentzkow, 2017). In\nthis context, online media have promoted novel formats and models of news\nconsumption that have favoured the spread of fake news and its increased social\nimpact. Online media has changed the way news looks. For example, social\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n142\nnetworks have unfolded a decontextualized space of meaning by favouring short-\nlength articles and visual imaginaries. Moreover, smartphone technology has\npromoted a fugitive mode of news consumption in which users are more prone\nto react to an article’s headline without reading the entire text. This kind of\nconsumption practice makes it difficult for people to judge the accuracy of news\nreports, and usually the information is believed at face value (McQueen, 2018).\nMuch of the current media do not support reflection and favour an incidental model\nof news consumption, thereby paving the road that has turned fake news into a\nphenomenon with social and political implications.\nNews as part of the journalistic genre has always integrated in its production\nprocess the pressure to get information to the audience quickly. Online media\nprovide instantaneous coverage, which the legacy media cannot always afford. The\nculture of social media platforms favours instantaneity and produces a drive for\nimmediacy that has established the grounds on which the fake news phenomenon\nhas emerged (Bakir, & McStay, 2018). The interest to get stories immediately, as\nwell as the rapid and widespread circulation of content make people (journalists\nand news consumers alike) pay less effort to time-consuming practices such as\nfact-checking or tracking down sources. The instantaneity of news delivery is\ncomplemented by the instantaneity of news consumption. Online media not only\nprovide instantaneous information to their users; they also allow for synchronicity\nin communication (Mustafaraj, & Metaxas, 2017). Audiences can react immediately\nin support of a misleading content whose power is exerted in the very process of\nimmediate consumption.\nSocial networks are organised on homophilic considerations based on geographic\nand ideological similarities between members. This form of social organisation\nmediates “social worlds in a way that has powerful implications for the information\n[users] receive, the attitudes they form, and the interactions they experience”\n(McPherson, Smith-Lovin, & Cook, 2001, 415). Therefore, the digital media divide\npeople into ‘echo chambers’, where users are exposed to arguments in one specific\ndirection. This fragmented media environment implicitly invites people to rely on\nself-confirming sources when judging aspects of social reality, thereby reinforcing\nexisting beliefs and legitimating misleading information (Spohr, 2017). Selective\nexposure to ideologically biased content and the technological mediation of echo\nchambers have fuelled fake news stories and amplified their impact.\nAcknowledging the social power of algorithms\nPeople have multiple sources of information in the digital world. In this context,\nhuman attention has become a scarce commodity that can be traded, bought or sold\nto make profit (Thrift, 2006). As a consequence, online companies are economic\nagents that operate on an attention market “in which platforms acquire time\nfrom consumers and sell access to that time to advertisers who want to deliver\nmessages to those consumers” (Evans, 2017, 1). Since attention economics shapes\n\n143\nSTUDII\nbusiness models in the digital era, fake news has emerged as a result of how online\ncompanies generate revenue. Richard Lanham considers that attracting attention\nis a matter of style: “[T]he devices that regulate attention are stylistic devices”\n(Richard, 2006, xi). According to this perspective, style is the currency of the new\neconomy. Under the logic of online connectivity, we assist to a transformation of\nstyle in order to maximise users’ interest. Outrage, anger and fear are influential\nrhetorical components in a digital information ecosystem fraught with fake news.\nOn the one hand, events happen at a rapid pace, and users are not prepared to\nparticipate responsibly in this attention economy. On the other hand, business\nalgorithms are not neutral to ethics and morals: algorithms are social technologies\nthat have the power to define what is important in the world and thus enforce a\nsystem of values by focusing public attention on socio-technically emerging codes\nand canons (Verbeek, 2008). Profitability as a business value of tech companies\nhas become a force that has taken over the quest for objectivity and reliability.\nFake news is also understood as a consequence of the business models that\nconnect advertisers with website publishers by negotiating the amount of money\npaid per click (Braun, & Eklund, 2019). Digital platforms integrate ranking\nalgorithms and real-time auction systems to distribute content in a network.\nThese kinds of tools allow companies and other online agents to make profit by\nstrategically placing digital advertising. Consequentially, the business models\nused by technology companies have facilitated the development of online spaces\nthat encourage visitors to click in order to generate advertising money (Graham,\n2017). This has provoked a rush for engaged audiences and incentivised web\npublishers to deliberately post objectionable content that generates revenue. The\nfake news phenomenon has been amplified by platforms that feature misleading\nsensational articles on trending ideas and topics. The information digital ecosystem\nis organised to nudge users to profitable behaviour by provoking affective responses\nfrom them. Fake news has proven profitable as a form of ‘clickbait journalism’\ndeveloped under the regime of affective capitalism. Affective capitalism unfolds\na social order in which emotions are treated as commodities. Namely, emotions\nare activated and circulated, since they generate viewing time, which is converted\ninto profit. Fake news has appeared as a form of manifestation in a context in\nwhich emotions are monetised and affects are exploited to produce economic\nvalue (Bakir, & McStay, 2018).\nFake news can be understood as a cultural implication of how search engines\nfunction. The most common search engines use business models based on various\nmethods to estimate the costs of words and phrases. By monetising web content,\nwords become economic resources: keywords and expressions are transformed\ninto commodities that are sold on a linguistic market. This gives rise to a form of\nlinguistic capitalism that has contributed to the spread of fake news by incentivising\nthe use of profitable formulations (Graham, 2017). The algorithms of search\nengines have influenced language use on the web and marked off a decline of\nonline language diversity by prioritising the vocabularies with more economic\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n144\nvalue over others. Therefore, the financial models have significant effects on online\ndiscourse, since such models aim “to develop intimate and sustainable linguistic\nrelationships with the largest possible number of users in order to model linguistic\nchange accurately and mediate linguistic expression systematically” (Kaplan,\n2014, 60-61). By taking into consideration that our language is interlinked with\nthinking, we can talk about an algorithmic mediation of what we know about the\nworld and how we know it. Since our knowledge is technologically mediated, the\nlinguistic biases of the search engines’ algorithms have generated the language of\nfake news with a performative role in the making of social realities.\nSearch engines, social networks and other digital platforms are designed to track\nonline behaviour and record users’ preferences and needs. These kinds of platforms\nare based on market segmentation techniques that can help commercial agents or\npolitical organisations easily identify potential customers or supporters. Algorithms\nprioritise information and provide customised content in order to enhance user-\nexperience and increase the effectiveness of the information distribution circle,\nin a process that aligns the interests of various stakeholders. The technical and\nfunctional affordances of digital media favour a more convenient model of targeted\nadvertising, which increases the level of precision in strategic communication by\nallowing the delivery of relevant messages to targeted audiences (Livingstone,\n2019). Even if the actual information ecosystem improves communication\nefficiency and performance, the ecosystem still poses some challenges and social\nrisks. Driven by financial gains, online platforms allow advertisers to tailor their\nmessages to different audiences. Specifically, the same advertiser could say different\nthings to different people, thus producing an altered sense of reality. Confusion\ncarried an anomic state as a social setting that encouraged the growth of fake news.\nReviewing instruments of audio and video manipulation\nNew technologies have changed the practices of knowledge production and\nprovided new tools for digital content editing (Carlson, 2009). Software for photo,\naudio or video manipulation enhances the impact of fake news by functioning as\ninstruments to produce fabricated but credible evidence in support of deceitful\nreports. Bots, paid commentators and fake accounts are resources used in propaganda\ncampaigns to give a sense of widespread acceptance and circulation of content.\nBy working as joint voices in support of various propagandistic initiatives, these\nfabricated reactions increase the capacity for fake news to spread and produce\nreal effects in the social world (Shao et al., 2018). The possibility to emulate the\nillusion of factual news and authentic debate through socio-technical manipulation\nhas played a considerable role in making fake news influential in the contemporary\nworld.\n\n145\nSTUDII\nSociology of culture and discourse\nEnquiring into the changing nature of values\nThe changing philosophy of truth has encouraged a new commitment to\nnotions of objectivity and neutrality. Contemporary culture is described by an\nincreased awareness of the contingent nature of social reality. Moral, aesthetic\nand cognitive relativism are significant philosophical positions, postulating the\nimpossible existence of a metaphysically valid standpoint that could function as an\nabsolute standard of truth (Harsin, 2015). We do not live in a world in which truth\nis the ultimate goal that might be achieved through instrumentation and rational\ndeliberation; rather, we live in a world in which truth is established relative to\nthe position of a particular social actor in a status system. The contemporary fake\nnews phenomenon has appeared in a cultural context in which the relativity of\nknowledge is assumed as the main epistemic stance. As Nick Rocklin puts is, “the\ntruth of the post-truth is that truth does not matter” (Rochlin, 2017): reasoning and\ncognition cannot be separated from moral judgments and value-driven approaches,\nthus weakening the distinction between facts and values.\nAn increase of public trust in pseudoscience has also opened the road to\nthe contemporary fake news phenomenon. Science as a social institution has\nstarted to lose its legitimacy and cultural authority, so that the ability of scientific\nfacts and discourses to influence public debate has been challenged accordingly\n(Lidskog, 1996). Testimonial and anecdotal evidence has become more and more\ninfluential in shaping an understanding of the world. The facticity of scientific\ninquiry is obstructed by the sensationalist and emotional rhetoric found within\nonline forums and participatory environments (Introne et al, 2018). Fake news\nis just a specific manifestation of a more extensive (dis)information ecosystem\npenetrated by pseudoscientific theories. In this ecosystem, scientific claims with no\npredictive and explanatory power, published in predatory journals, are appropriated\nas rigorous and reliable evidence; this process fuels yellow journalism practices.\nScrutinizing the performative role of language\nNews sharing over social networks can be understood as a form of communicative\naction. According to Jurgen Habermas, communicative action is “the use of\nlanguage as a medium for a kind of reaching understanding, in the course of which\nparticipants, through relating to a world, reciprocally raise validity claims that can\nbe accepted or contested” (Habermas, 1981, 99). Communicative action is based\non an inherent communicative rationality as a sense-making scheme “oriented\nto achieving, sustaining and reviewing consensus - and indeed a consensus that\nrests on the intersubjective recognition of criticisable validity claims” (Habermas,\n1981, 17).\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n146\nLanguage is performative in the sense that “saying something is doing\nsomething” (Austin, 1962). The interpretation of a situation shapes reality by\ncausing actions that produce social effects: if people define situations as real, those\nsituations can become real in their consequences (Thomas, & Thomas, 1928).\nWords designate a world that becomes real according to how the world is defined.\nIn that sense, fake news represents a form of communication that is performative.\nFake news instructs audiences how to think about public issues, thus mediating\nmodes of knowing social reality. This is important, since people act according to\nthe understanding they have of the world. Digital media operate under particular\n“regimes of truth” (Foucault, 1991) instantiated through manifold online discursive\npractices. Power is embodied in discourse and forms of knowledge, so fake news\nplays a political role by defining and redefining what counts as true. However, the\nsocio-technical organisation of the digital world makes it difficult to differentiate\nbetween true and false statements and generates a sense of confusion that can be\nused for political gains.\nSocial psychology\nDisclosing cognitive biases in information processing\nThe digital environment contains an overabundance of information, contradictory\ncontent and data of questionable accuracy. In the context of information overload,\ninternet users are required to make judgments under conditions of uncertainty and\ntime pressure. Therefore, people rely on heuristics and mental shortcuts to process\nand interpret the information they receive through various media channels. This\nleads to different cognitive biases as vulnerability factors for misinformation\n(De keersmaecker, & Roets, 2017). People tend to use adaptative strategies and\ncreate their subjective social reality based on various errors: confirmation bias,\nillusionary correlation, selective exposure, belief perseverance and so on. These\nerrors represent sociopsychological phenomena and organising principles of\nhuman cognition that have played a key role in the spread of fake news. Fake\nnews functions as pieces of information that introduce cognitive clarification\nand help people make sense of the world. In the absence of relevant resources or\nexperiences, false rumours are powerful devices in the communication enterprise,\nsince they help people overcome a knowledge deficit and navigate the complexity\nof the social world. Accordingly, fake news assimilates powerful cognitive models\nof motivated reasoning that reinforce pre-existing beliefs and ideas.\nGrasping the dynamics of group behaviour in digital networks\nThe digital environment has changed the way news is consumed: news\nconsumption today is a participatory and collaborative practice. Citizens are\n\n147\nSTUDII\nempowered to comment on and debate the news instantly, thus negotiating the\nmeaning of the account by using both an instrumental and normative understanding\nof the social world. Digital media accommodate social interactions and the\nprocesses through which fake news is validated and authenticated discursively\n(but not factually). Validity claims are vehiculated in technologically mediated\ndiscussions, thereby turning fake news into credible and legitimate information to\nconsider. Moreover, social networks support the formation of social relationships\nbased on trust, so fake news is distributed in a social space in which people trust\neach other. Consequentially, users of the social networks do not assess the validity\nof the shared information because they assume that other actors have carried the\nsymbolic costs of validation (Tandoc et al., 2017). In this context, the diffusion of\nresponsibility as a social process has contributed to the consolidation of the fake\nnews phenomenon in actual society.\nIn explaining how society makes a transition from mass behaviour to\nsocial networks, Michel Maffesoli describes the postmodern era as an era of\nneotribalism (Maffesoli, 1996). By using the metaphor of tribes, Maffesoli points\nto the development of digital tribes as transient and fluid social groups whose\nmembers gather based on shared interests, beliefs and aesthetics. In this context,\nnews sharing can be understood not only as a communicative act but also as a\ndramaturgical action (Goffman, 1956). News sharing is part of the impression\nmanagement practices through which people express their self and consolidate\ntheir image. News sharing facilitates online interactions that create a dynamic of\ngroup life. Even if digital tribes are characterised by their transitory nature, they\nfunction as discursive communities guided by communal ethics through which\nmembers express their views and participate in a culture. News sharing is a social\nactivity that brings people together, thus consolidating a sense of solidarity and\ncollective identity (Polletta, & Callahan, 2017). Fake news exploits the sense\nof togetherness found in already established virtual worlds. Since emotions are\nessential components of group life, fake news reinforces various values and\nbeliefs by manipulating anger and fear to serve the interests of political parties,\ncompanies or other agents.\nTracking down the emergence of social movements\nThe preference for opinionated rather than impartial news has fostered novel\ndefinitions about what it means to be informed (Marchi, 2012). News that makes\na normative point has become more influential than news guided by the principle\nof neutrality. Embracing civic attitudes in online areas is now equivalent to\nexpressing concerns for issues that reflect personal values and experiences. Digital\nmedia have affected the conception of citizenship by providing alternative forms\nof political action. In this process, digitally mediated platforms have redefined\nthe dynamics that drive political commitment and have changed what constitutes\ncivic engagement (Bennet, 2008). Digital communication has created not only\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n148\nthe foundation for making governments responsive to people’s concerns though\ne-democracy initiatives but also the other way around: individuals have become\nmore vulnerable to propagandistic attempts. Therefore, the decline of conventional\npolitical participation created a context that has favoured the spread of deceptive\ncontent as new connections to government have been established. Engagement\nin political discussion has taken different forms, and the digital opportunities for\npolitical self-expression trigger social movements to either challenge or enforce\npower relation.\nConclusion\nFake news is a complex social phenomenon whose determinants cannot be\nneatly separated in categories if misinformation resides in the interdependence\nbetween journalistic organisations, communication technologies, receptive\naudiences and governmental regulations. So far, media studies have approached\nthe phenomenon of fake news largely from the perspective of cultivation theory;\nmany empirical endeavours were conducted to explore how a particular medium\ndetermines a particular message with focus on the processes through which various\nmessages induce effects on their audiences. By relying on traditional models of\ncommunication, media studies have somehow been ignorant to the structural\nprocesses that lay behind fake news. In this context, a more visible, coherent and\ntheoretically sound sociological perspective in media studies might contribute\nto a better understanding of the nature of misinformation in present-day society\nespecially when it is complemented by methodologies that consider fake news\nprimarily as a social phenomenon and secondly as a communicational outcome\nand product of cultural consumption.\nThis paper has proposed a sociological research agenda to explore fake news\nas a social phenomenon in the digital age. Firstly, we consider that a sociological\napproach to fake news should revisit the classical theories of media influences and\nrethink the processes that coagulate the intermedia agenda setting. In a context in\nwhich media industry faces an increased pressure to innovate and many sources of\ninformation are outside the control of gate keeping instances, media studies should\nreveal the methods to deconstruct discourses as main drivers of persuasion and the\ntechniques to interpret strategies that act on public opinion in a post-truth reality.\nSecondly, sociologically informed media studies should take into consideration the\nchanges that characterize both the practice of journalism and the social organization\nof news media. The business cultures of news production and dissemination\nare based on economic models that give rise to less institutionalized practices\nand regulatory frameworks, which necessities a close scientific examination and\nan attentive sociological inquiry. Thirdly, media studies would benefit from a\nsociological approach by considering the role of digital technologies in changing\nglobal media industries and cultural consumption practices. In this sense, future\n\n149\nSTUDII\nresearch should consider the role of algorithms in shaping the objectivity and\nreliability of information in a networked culture. Issues of surveillance capitalism,\nlinguistic capitalism, economy of attention and experience economy should be\nfurther taken into consideration. Moreover, an interdisciplinary study of fake news\nphenomena could not ignore the performativity of language and the regimes of\ntruth that are built through communicative actions in online discourse communities.\nFinally, media studies should integrate a sociological focus by exploring fake news\nas a function of group dynamics emerged in digital networks. This implies an\ninquiry on processes that are heterogeneous in nature by generating new cognitive\nbiases, by redefining issues of accountability and responsibility for content creation\nand distribution, as well as by enabling new practices of social identity formation.\nNowadays, fake news is considered a social problem. As such, it has become a\ntop priority for technology companies and governments. Various initiatives have\nbeen established to cope with the production and distribution of misleading and\ndeceptive information (Huckle, & White, 2017; Sethi, 2017). The actions taken to\ncontrol the challenges of a post-truth media culture are built on multiple layers.\nThey include various sorts of initiatives such as 1) grass root initiatives (e.g.\ncreating a black list of websites that produce fake news); 2) specialised initiatives\n(e.g. developing news curators through algorithms and crawling techniques); and\n3) governmental initiatives (e.g. legislative undertakings meant to censor some\nmessages and impose sanctions on media trusts that vehiculate untrustworthy\ninformation). The initiatives rely on different political philosophies: some of them\nare based on liberal principles by favouring participation and dialogue, while\nothers seriously threaten the freedom of expression necessary in a democratic\nsociety. The current legislative initiatives to target fake news are contested, as they\nfoster authoritarian regimes of censorship. Regulations might silence dissenting\nand critical voices and give rise to abuses rather than safeguarding the public from\nfalse information. Other initiatives that rely on computational or fact-checking\napproaches usually have minimal impact, since such approaches face difficulties in\ndisseminating corrections. Also, they rely on a philosophy detached from current\nsocial realities: evidence-based reasoning is not efficient in the post-truth world.\nTherefore, current efforts to combat fake news reveal a misunderstanding of the\nissue.\nWe argue that the popular discourse focuses predominantly on the role of\ntechnologies in the creation and spread of misleading information but ignores the\ninfluence of other factors that have contributed substantially to the emergence of\nthe fake news phenomenon as we know it today. Social networks and other online\nplatforms operated in a context in which society has been already vulnerable to\npropaganda and other similar actions by amplifying the risks that derive from the\nuse of technologies. There is no single factor responsible for the spread of fake\nnews, but the fake news phenomenon has emerged through the joint influence of\ntechnological, economic, sociocultural and political factors on human interactions\nand practices.\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n150\nMany initiatives might be subject to failure because they miss an important\npiece in the puzzle: namely, there are multiple causes of the fake news phenomenon\nthat might be noticed by embracing a more prominent sociological focus in media\nstudies. Fake news does not only depend on technologies and their business models\nbut also on the rationality on which politics is done and on various social processes\nand dynamics. While the current initiatives are focused on handling misleading\ntextual content in the news industry, new advances in artificial intelligence and\ncomputer graphics pose important challenges in the dissemination of deceptive\nnews in credible forms. In this context, there is a critical need for measures to\nimprove people’s media literacy. However, the most common measures do little to\neducate the public about the consequences of fake news on the lives of ordinary\npeople.\nFake news is a complex social phenomenon that appears at the intersection of\nvarious contextual factors that influence information processing at a social level.\nThis discussion in the paper also provides insights that support the necessity to\ncomplement fact-checking and computational measures through initiatives that\nempower news consumers and build awareness of various social configurations\nand technologically mediated realities.\nAcknowledgment\nPaper developed with support from Digital News Initiative.\nReferences\nAllcott, H., & Gentzkow, M. (2017). Social Media and Fake News in the 2016 Election,\nJournal of Economic Perspectives, 31(2), 211-236.\nAustin, J. L. (1962). How to Do Things with Words. Oxford: Clarendon Press.\nBakir, V., & McStay, A. (2018). Fake News and The Economy of Emotions, Digital\nJournalism, 6(2), 154-175.\nBastos, M., & Mercea, D. (2017). The Brexit Botnet and User Generated Hyperpartisan\nNews, Social Science Computer Review, 37(1), 38-54.\nBennet, L. (2008). Changing Citizenship in the Digital Age, in L. Bennet (ed.), Civic\nLife Online: Learning How Digital Media Can Engage Youth, Cambridge: MIT\nPress,1-24.\nBraun, J. A., & Eklund, J. L. (2019). Fake News, Real Money: Ad Tech Platforms, Profit-\nDriven Hoaxes, and the Business of Journalism, Digital Journalism, 7(1), 1-21.\nBrenner, D. J. (1988). Toward a paradigm of media agenda-setting effect: Agenda-setting as\na process AU - Megwa, Eronini R, Howard Journal of Communications, 1(1), 39-56.\nBro, P., & Wallberg, F. (2014). Digital Gatekeeping, Digital Journalism, 2(3), 446-454.\nCarlson, M. (2009). The Reality of Fake Image. News norms, photojournalistic craft, and\nBrian Walski’s fabricated photograph, Journalism Practice, 3(2), 125-139.\nColeman, S. (2017). Can the Internet Strengthen Democracy? Malden: Polity Press.\n\n151\nSTUDII\nCook, C., & Sirkkunen, E. (2013). What’s in a Niche? Exploring the Business Model of\nOnline Journalism, Journal of Media Business Studies, 10(4), 63-82.\nDe keersmaecker, J., & Roets, A. (2017). ‘Fake news’: Incorrect, but hard to correct. The\nrole of cognitive ability on the impact of false information on social impressions,\nIntelligence, 65, 107-110.\nDurkheim, E. (1982). The Rules of Sociological Method. New York: The Free Press.\nEvans, D. S. (2017). The Economics of Attention Markets. Available at https://ssrn.com/\nabstract=3044858.\nFernandez, P. (2017). The technology behind fake news, Library Hi Tech News, 34(7), 1-5.\nFoucault, M. (1991). Discipline and punish. The Birth of the Prison. New York: Vintage\nBooks.\nFourney, A., Racz, M. Z., Ranade, G., Mobius, M., & Horvitz, E. (2017). Geographic and\nTemporal Trends in Fake News Consumption During the 2016 US Presidential\nElection, in CIKM, 2071-2074.\nFranklin, B. (2017). The Future of Journalism: In an Age of Digital Media and Economic\nUncertainty. Routledge.\nGarry, J. (2013). Emotions and voting in EU referendums, European Union Politics,\n15(2), 235-254.\nGoffman, E. (1956). The presentation of the self in everyday life. University of Edinburgh.\nGraham, R. (2017). Google and advertising: digital capitalism in the context of Post-Fordism,\nthe reification of language, and the rise of fake news, Palgrave Communications, 3.\nHabermas, J. (1981). The Theory of Communicative Action, Volume 1: Reason and the\nRationalization of Society. Boston: Beacon Press.\nHarsin, J. (2015). Regimes of Posttruth, Postpolitics, and Attention Economies,\nCommunication, Culture and Critique, 8(2), 327-333.\nHaselmayer, M., Wagner, M., & Meyer, T. M. (2017). Partisan Bias in Message Selection:\nMedia Gatekeeping of Party Press Releases, Political Communication, 34(3), 367-\n384.\nHuckle, S., & White, M. (2017). Fake News: A Technological Approach to Proving the\nOrigins of Content, Using Blockchains, Big Data, 5(4), 356-371.\nIntrone, J., Gokce Yildirim, I., Iandoli, L., DeCook, J., & Elzeini, S. (2018). How People\nWeave Online Information Into Pseudoknowledge, Social Media + Society, 4(3),\n1-15.\nKaplan, F. (2014). Linguistic Capitalism and Algorithmic Mediation, Representations,\n127(1), 57-63.\nKaye, B. K., & Johnson, T. J. (2016). Across the Great Divide: How Partisanship and\nPerceptions of Media Bias Influence Changes in Time Spent with Medi, Journal\nof Broadcasting & Electronic Media, 60(4), 604-623.\nKhaldarova, I., & Pantti, M. (2016). Fake news: The narrative battle over the Ukrainian\nconflict, Journalism Practice, 10(7), 891-901.\nLidskog, R. (1996). In Science We Trust? On the Relation Between Scientific Knowledge,\nRisk Consciousness and Public Trust, Acta Sociologica, 39(1), 31-56.\nLivingstone, S. (2019). Audiences in an Age of Datafication: Critical Questions for Media\nResearch, Television & New Media, 20(2), 170-183.\nMaffesoli, M. (1996). The Time of the Tribes: The Decline of Individualism in Mass Society.\nLondon: Sage Publications.\n\nSOCIOLOGIE ROMÂNEASCĂ - VOLUMUL 19(1)/2021\n152\nMarchi, R. (2012). With Facebook, Blogs, and Fake News, Teens Reject Journalistic\n“Objectivity.”, Journal of Communication Inquiry, 36(3), 246-262.\nMcPherson, M., Smith-Lovin, L., & Cook, J. (2001). Birds of a Feather: Homophily in\nSocial Networks, Annual Review of Sociology, 27, 415-444.\nMcQueen, S. (2018). From Yellow Journalism to Tabloids to Clickbait: The Origins of Fake\nNews in the United States, in D. Agosto (ed.), Information Literacy and Libraries\nin the Age of Fake News, United States of America: Abc-Clio, 12-35.\nMeltzer, K. (2009). The Hierarchy of Journalistic Cultural Authority. Journalists’\nperspectives according to news medium, Journalism Practice, 3(1), 59-74.\nMiller, C. (1984). Genre as social action, Quarterly Journal of Speech, 70(2), 151-167.\nMustafaraj, E., & Metaxas, P. T. (2017). The Fake News Spreading Plague: Was it\nPreventable?, in WebSci, 236-239.\nPearson, M. (2017). Teaching Media Law in a Post-truth Context: Strategies for Enhancing\nLearning about the Legal Risks of Fake News and Alternative Facts, Asia Pacific\nMedia Educator, 27(1), 17-26.\nPersily, N. (2017). The 2016 U.S. Election: Can Democracy Survive the Internet? Journal\nof Democracy, 28(2), 63-76.\nPew Research Center (2017). The Future of Free Speech, Trolls, Anonymity, and Fake\nNews Online. [Online]: Pew Research Center.\nPolletta, F., & Callahan, J. (2017). Deep stories, nostalgia narratives, and fake news:\nStorytelling in the Trump era, American Journal of Cultural Sociology, 5(3), 392-\n408.\nRichard, L. (2006). The Economics of Attention. Style and Substance in the Age of\nInformation. The University of Chicaco Press.\nRichardson, N. (2017). Fake News and Journalism Education, Asia Pacific Media Educator,\n27(1), 1-9.\nRochlin, N. (2017). No Fake news: belief in post-truth, Library Hi Tech, 35(3), 386-392.\nRussel, F. (2015). No Caveat Lector: Fake News as Folklore, The Journal of American\nFolklore, 128(509), 315-332.\nSchlesinger, P., & Doyle, G. (2014). From organizational crisis to multi-platform salvation?\nCreative destruction and the recomposition of news media, Journalism, 16(3),\n305-323.\nSethi, R. (2017). Crowdsourcing the Verification of Fake News and Alternative Facts, in\nHT ‘17: Proceedings of the 28th ACM Conference on Hypertext and Social Media,\n315-316).\nShao, C., Ciampaglia, G. L., Varol, O., Yang, K.-C., Flammini, A., & Menczer, F. (2018).\nThe spread of low-credibility content by social bots, Nature Communications,\n9(1), 4787.\nSpohr, D. (2017). Fake news and ideological polarization. Filter bubbles and selective\nexposure on social media, Business Information Review, 34(3), 150-160.\nTandoc, E. C., Ling, R., Westlund, O., Duffy, A., Goh, D., & Zheng Wei, L. (2017).\nAudiences’ acts of authentication in the age of fake news: A conceptual framework,\nNew Media & Society, 20(8), 2745-2763.\nTandoc Jr., E., Lim, Z. W., & Ling, R. (2018). Defining Fake News. A typology of scholarly\ndefinitions, Digital Journalism, 6(2), 137-153.\n\n153\nSTUDII\nThomas, W., & Thomas, D. S. (1928). The child in America; behavior problems and\nprograms. A. A. Knopf.\nThrift, N. (2006). Re-inventing invention: new tendencies in capitalist commodification,\nEconomy and Society, 35(2), 279-306.\nVargo, C., Guo, L., & Amazeen, M. (2018). The agenda-setting power of fake news: A\nbig data analysis of the online media landscape from 2014 to 2016, New Media &\nSociety, 20(5), 2028-2049.\nVerbeek, P.-P. (2008). Morality in Design: Design Ethics and the Morality of Technological\nArtifacts, in P. Vermaas, P. Kroes, A. Light, & S. Moore (Eds.), Philosophy and\nDesign, 91-103, Springer.\nVerma, N., Fleischmann, K., & Koltai, K. S. (2017). Human Values and Trust in Scientific\nJournals, the Mainstream Media and Fake News, in 80th Annual Meeting of the\nAssociation for Information Science & Technology, 426-435.\nWasserman, H. (2020). Fake news from Africa: Panics, politics and paradigms, Journalism,\n21(1), 3-16.\nWatson, H. (2011). Preconditions for Citizen Journalism: A Sociological Assessment,\nSociological Research Online, 16(3), 1-12.\nWilson, F. (2013). A Theory of Public Opinion. New York: Routledge."
    },
    {
      "file_name": "FakeNewsEEG-Peoplewillbelieve.pdf",
      "file_size": 74063,
      "file_type": "pdf",
      "extracted_content": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/328798002\nFake News on Social Media: People Believe What They Want to Believe When it\nMakes No Sense at All\nArticle in SSRN Electronic Journal · January 2018\nDOI: 10.2139/ssrn.3269541\nCITATIONS\n194\nREADS\n24,939\n3 authors, including:\nAlan R. Dennis\nIndiana University Bloomington\n299 PUBLICATIONS 23,164 CITATIONS\nSEE PROFILE\nAll content following this page was uploaded by Alan R. Dennis on 11 February 2019.\nThe user has requested enhancement of the downloaded file.\n\nFake News on Social Media: People Believe What They Want to\nBelieve When it Makes No Sense at All\nPatricia Moravec, Randall K. Minas and Alan R. Dennis\nKelley School of Business Indiana University\nAbstract\nFake news (i.e., misinformation) on social media has sharply increased over the past few years.\nWe conducted an experiment collecting behavioral and EEG data from 83 social media users to\nunderstand whether they could detect fake news on social media, and whether the presence of a\nfake news flag affected their cognition and judgment. We found that the presence of a fake news\nflag triggered increased cognitive activity and users spent more time considering the headline.\nHowever, the flag had no effect on judgments about truth; flagging headlines as false did not\ninfluence users’ beliefs. A post-hoc analysis shows that confirmation bias is pervasive, with\nusers more likely to believe news headlines that align with their political opinions. Headlines that\nchallenge their opinions receive little cognitive attention (i.e., they are ignored) and users are less\nlikely to believe them.\n\n1\nFake News on Social Media: People Believe What They Want to\nBelieve When it Makes No Sense at All\n“There is today a special need for propaganda analysis. America is beset by\na confusion of conflicting propagandas, a Babel of voices, warnings-charges,\ncounter-charges, assertions, and contradictions assailing us continually through\npress, radio and newsreel…” (Institute for Propaganda Analysis 1938, p. 1).\n“We are facing nothing less than a crisis in our democracy – based on the\nsystematic manipulation of data to support the relentless targeting of citizens,\nwithout their consent, by campaigns of disinformation and messages of hate.”\n(House of Commons 2018)\nINTRODUCTION\nIn the early days of the Internet, people argued that the Internet would enable greater\ntransparency of information, which would increase the quality of democracies (Abramson et al.\n1990; Tewksbury 2003). The availability of information from various news sources would\nenable people to find their own information from non-traditional news outlets, and this decreased\nreliance on a narrow set of traditional news sources would improve democracy (defined as “the\nbelief in freedom and equality between people, or a system of government based on this belief”\n(Dictionary 2018)). This vision has been realized with the rise of non-traditional news on social\nmedia, but some might argue that the prevalence of fake news on social media has harmed\ndemocracy, rather than improved it. An editorial in Science calls on the scientific community to\nhelp reporters and the general public better identify and avoid fake news (Weiss 2017).\nSocial media has become a common source for news; more than 50% of American adults\nread news on social media (Gottfried and Shearer 2016). Social media is different from other\nmedia providing news (e.g., TV news, news websites, and mobile phone news apps) because\nusers do not choose the source of all of the articles they see on social media. Instead, proprietary\nalgorithms provide targeted information with little transparency. With other news media, users\npick the source first, and do so with a familiarity of the nature of the source (Rice et al. 2018).\n\n2\nWith social media such as Facebook, articles from a wide variety of sources appear on\nusers’ newsfeeds. News articles are intermixed with sponsored articles (i.e., paid advertisements)\nand posts from family and friends. All of these may be intentionally or unintentionally true or\nfalse, but some are explicitly designed to influence (Shane 2017). For example, Cambridge\nAnalytica developed tools to influence users after it gained access to more than 50 million users’\ndata (Granville 2018). About 23% of social media users report that they have accidently or\nintentionally shared fake news (Barthel et al. 2016). Over 60% say that fake news leaves them\nconfused about what to believe (Barthel et al. 2016).\nSocial media has moved quality control for detecting fake news from trained journalists\nto regular users (Kim and Dennis 2018). About 84% of Americans believe that they can detect\nfake news (Barthel et al. 2016), but how do users detect fake news when most users have no\ndirect knowledge of the facts (i.e., they have not witnessed the events)?\nIn this study, we examine the effect of a Facebook “fake news” flag and how social\nmedia users respond to it. Simply put, we examine the question: are fake news flags effective in\naltering users’ beliefs? We use electroencephalography (EEG) to examine cognitive processes\n(Dimoka et al. 2012; Vance et al. 2018). We found that flagging articles as fake triggered more\ncognitive activity, but it did not change users’ beliefs in them. We further found that articles that\naligned with the user’s a priori opinions triggered increased cognitive activity, with users more\nlikely to believe them; articles that challenged users’ opinions were less thoroughly considered\nand were less likely to be believed. Our findings triangulate around one explanation:\nconfirmation bias; users believe what matches their prior opinions, undeterred by the actual truth\nof an article or a fake news flag. As John Mellencamp said in his 2004 song, Walk Tall, “People\nbelieve what they want to believe/when it makes no sense at all” (Mellencamp 2004).\n\n3\nPRIOR THEORY AND RESEARCH\nNews has always been questionable in its reliability (McGrath 1986). Even before the rise\nof the Internet, certain newspapers were known for their biases and potentially distorted news\n(McGrath 1986). The Internet enables people to access thousands of different news sources,\nrather than being bound by traditional sources, increasing their exposure to biased and distorted\nnews. The 2016 US presidential election was rife with news, both true and false. Compounding\nthis difficulty, social media users had to contend with the intentional production and distribution\nof fake news, whether created to generate revenue from advertisements (Kirby 2016) or to\ninfluence the election (Fisher 2016; Shane 2017; Sydell 2016). Social media platforms have been\ncriticized for not taking sufficient action to prevent the spread of fake news (Maheshwari 2017).\nIn the sections below, we examine why assessing the truthfulness of news on social\nmedia is difficult, and how flagging fake articles as “disputed by third-party fact-checkers” and\nconfirmation bias may influence users’ beliefs. We focus on Facebook as a leader in social\nmedia; Facebook has over 2 billion active users and is a popular source for news (Gottfried and\nShearer 2016; Statista 2018).\nAssessing the Truthfulness of News on Social Media\nContext matters (Johns 2006; Johns 2017). “One way to develop richer theories that\nprovide actionable advice is to take the context into greater consideration” (Hong et al. 2014,\np.2). Much past IS research has examined work contexts, but our focus is on social media. Most\nindividuals use social media for hedonic purposes (Chauhan and Pillai 2013), such as seeking\nentertainment or connecting with friends, rather than utilitarian purposes (Johnson and Kaye\n2015). Following steps 1-4 in Table 1 of Hong et al., we ground our research in two general\ntheories (confirmation bias and cognitive dissonance) and examine how they influence behavior\n\n4\nin this use context. We identified three context-specific factors that make consuming news on\nsocial media different from other contexts in which users view information on the Internet.\nFirst, the user’s mindset is different, which affects how information is processed. The\nconsumption of news on social media is different than the consumption of information elsewhere\non the Internet. For example, it is well known that some product reviews are fake (Dwoskin and\nShaban 2018; Dwoskin and Timberg 2018; Roberts 2013). A key difference between fake\nreviews and fake news is that users do not read product reviews for entertainment; they read\nreviews for information to make a decision, knowing there is a monetary incentive to make the\nbest, most well-informed decision. Thus, users reading fake reviews are in a utilitarian mindset;\ntheir goal is to understand the meaning of the information in the review and to decide which\nreviews should be considered in making a decision. Minas et al (2014) examined a utilitarian\nmindset in virtual team interactions in a decision-making context. The study found confirmation\nbias was present while individuals processed information in a decision-making team-based chat.\nIn contrast, the hedonic mindset when reading social media news means the user’s goal is not to\ndetermine what is true and fake; instead the goal is enjoyment and pleasure. The user will avoid\neffortful activities that feel like work (e.g., thoughtful information processing) and activities that\ndo not bring enjoyment (e.g., reading stories that your favorite sports team lost). Users engage\nwith articles that make them feel good, which tend to be articles supporting their beliefs.\nSecond, the source of the information is not clear. With Internet news and traditional\nnews media, we visit the web site of our favorite news network or open our local newspaper; we\npick the source before we read articles and do so with some understanding of the source’s\nlimitations (Rice et al. 2018). Facebook is different because users do not choose the source of the\narticles; instead, Facebook’s algorithms choose the articles. Although some users subscribe to\n\n5\ncertain sources by following them on social media, many other sources arrive on our newsfeeds\nfrom advertisements, sharing by friends, and algorithmic decisions. Articles from many different\nsources—some reputable, some disreputable—are intermixed. A fake news article may be\npresented between a CNN article and Aunt Martha’s cookies. The source of the story is obscured\n(Kim and Dennis 2018), and users in a hedonic mindset are not motivated to invest effort to find\nand understand the source (Kim and Dennis 2018).\nFinally, the sheer volume of fake news makes it challenging to separate truth from\nfiction. More fake news articles are shared on social media than real news (Silverman 2016).\nMany fake news sites have appeared on Facebook with the express purpose of spreading\ncarefully-crafted propaganda or to discredit a specific person (BBC 2017). Their low cost and\nubiquity is one reason that fake news is common on social media (Barthel et al. 2016).\nThese three contextual factors—a hedonic mindset, a lack of cognizance of the source,\nand the volume of fake news—combine to create a context in which social media users do not\nthink as critically as they should when presented with news on social media. More than half the\narticles shared on Twitter are shared without the user reading them, let alone thinking critically\nabout them (Gabielkov et al. 2016). Nonetheless, research shows that there is a bias toward\ntaking an opinion on contentious social media topics, rather than remaining neutral (Jonas 2001).\nThis is true even when users lack information on the topic—or bother to read an article—which\nhelps the spread of fake news (Jonas 2001).\nFact-Checking Fake News\nIn response to the rise of fake news, fact checking services have become more common.\nMany solutions have been developed to automate fact-checking. Truthy (Ratkiewicz et al. 2011)\nand Hoaxy (Shao et al. 2016) are two such solutions, which can provide relatively quick results\n\n6\nfor news articles. Truthy fact-checks sites that are well-known for verifying the truth of news\narticles, such as Snopes.com, politifact.com, and factcheck.org, as well as checking known\ndisreputable sites for fake news articles. Fact-checking can influence credibility, especially if\ndone by independent fact-checkers (Wintersieck 2017).\nFacebook incorporated fact-checking into its platform and began flagging fake news\narticles in late 2016 by appending a statement that an article was “disputed by 3 \nrd \nparty fact-\ncheckers” when fact-checkers determined an article was fake (Schaedel 2017). Thus, fact-\nchecking was integrated into the presentation of the article; users did not need to invest effort to\nseek out a third-party fact-checking site. Facebook discontinued the flag in late 2017 (Meixler\n2017). One might conclude that Facebook’s actions indicate that fact-checking fake news is not\neffective. However, the undisclosed reasons why a for-profit corporation makes decisions—\nespecially when its goals are unclear (c.f., Zuckerberg 2016)—are not theoretically compelling.\nAfter Facebook discontinued its fake news flag, third parties began offering their own\nfake news flagging services that can be integrated into Facebook. For example, NewsGuard\nprovides a browser plugin using source reliability ratings from teams of expert journalists and\nconsultants for more than 4,500 news sites that account for 98% of the online news in the U.S.\n(NewsGuard 2018). The plugin automatically displays a fake news flag whenever content from a\ndisreputable source is displayed, whether in Facebook or any Web site.\nFact-checking is most important when the user wants to believe a headline; flagging a\nheadline when the user was unlikely to believe it without the flag adds little value. Thus, we\nfocus on the situation where a user is inclined to believe a fake headline, but it is flagged as false.\nThe Effects of Confirmation Bias\nOne factor influencing belief is confirmation bias: people prefer information that matches\n\n7\ntheir prior beliefs (Koriat et al. 1980; Minas et al. 2014; Nickerson 1998). Confirmation bias is a\nbias against information that challenges one’s beliefs (Nickerson 1998); it is driven by the\nfundamental nature of our cognition (Kahneman 2011).\nResearchers have long argued that there are two distinctly different cognitive processes,\nand there are many dual process models of cognition (Evans 2008). Two complementary models\nemerged in the 1980s. The Heuristic-Systematic Model (HSM) (Chaiken 1980; Chaiken and\nEagly 1983) argues that attitudes are formed by the systematic application of considerable\ncognitive effort to comprehend and evaluate the validity of available information (called the\nsystematic route), or by exerting little cognitive effort using simple heuristics on readily\naccessible information (called the heuristic route). The Elaboration Likelihood Model (ELM)\n(Cacioppo et al. 1986; Petty and Cacioppo 1986) argues that attitudes are formed based on\ndeliberate and active consideration of available information to evaluate the true merits of a\nparticular position (called the central route) or as a result of a less cognitively involved\nassessment of simple positive or negative cues in the context (called the peripheral route).\nThere are distinctions between HSM and ELM, but they share a common fundamental\nbasis. Both argue that there are two distinct conscious cognitive processes by which attitudes are\nformed, and that these two processes differ in the amount of cognitive processing expended (e.g.,\na quantitative difference) and in the cognitive approach used to evaluate information (e.g., a\nqualitative difference). Both argue that individuals choose which route to invoke based on their\nability and motivation to engage in extensive cognition. Both have evolved to argue that the\nroutes are not distinct, so cognition is more of a continuum of processing (Kitchen et al. 2014).\nELM is the more popular and is still used today (Cacioppo et al. 2018), although some\nresearchers dispute the notion of dual process models (Melnikoff and Bargh 2018).\n\n8\nMany newer dual process models have been developed (Evans 2008; Evans and\nStanovich 2013), because research suggests that many of the fundamental arguments of HSM\nand ELM (as revised over time in response to criticisms (Kitchen et al. 2014)) are not accurate.\nFor example, the routes are not mutually exclusive (both can be used); the routes are not on a\ncontinuum (they are separate); individuals do not choose the route to use (the heuristic route is\nautomatic); individuals cannot avoid the heuristic route (its use is involuntary); and the\nsystematic route cannot operate by itself (the heuristic route always precedes it) (De Neys 2018;\nEvans 2008; Evans and Stanovich 2013; Kahneman 2011; Pennycook et al. 2018).\nIn this paper, we adopt the widely accepted dual process model of Stanovich (1999) and\nKahneman (2011) who call these separate processes System 1 and System 2. We note that\nStanovich has more recently suggested using the terms Type 1 and Type 2 because the use of the\nword “system” implies there are separate areas in the brain that are dedicated to each type of\ncognition, which is not the case (Evans and Stanovich 2013).\nSystem 1 runs continuously, and delivers conclusions automatically and involuntarily\n(Kahneman 2011). Intuition is System 1 at work (Achtziger and Alós-Ferrer 2013; Dennis and\nMinas 2018). When we receive new information, our System 1 cognition automatically searches\nlong-term memory for confirming evidence and generates a response in less than one second\n(Bargh and Ferguson 2000; Carlston and Skowronski 1994; Fazio et al. 1986). This process is\nnonconscious and unavoidable; we cannot prevent it (Evans and Stanovich 2013; Kahneman\n2011). It supplies these assessments, even though they are not asked for (Bellini-Leite 2013;\nDennis and Minas 2018; Kahneman 2011; Thompson 2013). System 1 is a set of subsystems that\nrun in parallel triggered by different bits of incoming information (Bellini-Leite 2013; Evans\n2008; Evans 2014; Thompson 2013). When the different subsystems produce matching results,\n\n9\nSystem 1 produces a “Feeling of Rightness” (FOR) that says it is confident about its conclusions\n(Bago and De Neys 2017; De Neys 2014; Thompson et al. 2011). When there is conflict among\nsubsystems’ results, FOR creates a sense that something is not right (Bago and De Neys 2017).\nIn contrast, System 2 cognition is single-threaded (Dennis and Minas 2018) and has much\nless processing capacity (Evans 2014). System 2 is under our deliberate control, so we can\nchoose to invoke it, but it is easily overwhelmed. System 2 cognition is effortful (Kahneman\n2011), and most humans are “cognitive misers” who attempt to minimize cognitive effort (Taylor\nand Fiske 1978). Thus, we tend to adopt the conclusions of System 1, often without thought\n(Kahneman 2011). Common triggers causing us to invoke System 2 are a negative stimulus or a\nsurprise (Kahneman 2011), or a FOR that indicates conflicting results (Bago and De Neys 2017).\nThe net result is confirmation bias (Nickerson 1998). When we see new information, our\nSystem 1 automatically, and in less than one second, confirms that it matches our prior\nknowledge and we are inclined to believe it. Or, our System 1 tells us that it does not match and\nwe should not believe it (Kahneman 2011). Unless we are motivated to expend cognitive effort\nand invoke System 2, we simply accept the conclusion of System 1 with little thought\n(Kahneman 2011). And if we were to invoke System 2, how would it help us determine if a news\nstory was true? Unless we have witnessed the events in a story there is no unambiguous way to\ndetermine if the story is true or false. Thus, people are likely to accept their System 1 conclusion\nand believe information that matches pre-existing views (Allcott and Gentzkow 2017).\nConfirmation bias also affects the time taken. System 1’s conclusion that new\ninformation matches our beliefs is produced in less than a second and simply accepting this takes\nlittle time. If our System 1 indicates that we should reject the new information, we are inclined to\nspend only a little longer before discarding it (Haidt 2012; Kahneman 2011). Minas et al (2014)\n\n10\nfound that in a utilitarian mindset, all pieces of information are initially considered (by System\n1), but the only factual information that matched a priori beliefs was selected for System 2\nprocessing. Similarly, Turel and Qahri-Saremi (2016) found that System 1 was linked to\nimpulsive and problematic use of social media and System 2 to more rational and controlled use.\nTwo aspects of the social media context suggest that social media may exacerbate\nconfirmation bias. First, research suggests that individuals in a hedonic mindset may be less\nlikely to critically consider information than those in a utilitarian mindset, as their consumption\nis tied to what they desire reality to be, rather than what they know to be real (Hirschman and\nHolbrook 1982). When in a hedonic mindset we are less likely to expend the cognitive effort to\ninvoke System 2, and more likely to accept System 1’s biased conclusions.\nSecond, social media enables users to choose the news they like and learns their\npreferences so that it deliberately displays more articles matching their choices. This causes a\ndecreased range of information displayed on a user’s newsfeed, so that the news on social media\nis often biased (The Wall Street Journal 2016). Users’ realities on Facebook differ based on what\nthey read and who their friends are (The Wall Street Journal 2016). There are sharp differences\nin liberal and conservative newsfeeds, with fake news aligned with political beliefs more likely\nto be seen and shared by users in “echo chambers” of biased information (Bozdag and van den\nHoven 2015; Cerf 2016; Colleoni et al. 2014). This bias inundates users with news—real and\nfake—that supports their views (Bennett and Iyengar 2008; Knobloch‐Westerwick and Lavis\n2017). Such a stream of biased messages intensifies confirmation bias (Nickerson 1998).\nCreating Cognitive Dissonance\nOne approach to interrupting confirmation bias is to create cognitive dissonance by\nadding a fake news flag to false stories. Cognitive dissonance occurs when users are presented\n\n11\nwith two pieces of conflicting information that both cannot be true (Festinger 1962; Mills 1999),\nwhich in this case is a fake story users want to believe because it aligns with their a priori beliefs\nand a flag that says it is false. System 1 makes an instant judgement but the conflicting\ninformation makes this judgement difficult (Kahneman 2011); the results are unreliable and the\nFOR tells us something is amiss (Bago and De Neys 2017). This contradiction causes cognitive\ndiscomfort (Aronson 1969). The user must decide either to ignore the discomfort or invest effort\nto resolve it. If the issue is unimportant to them, users typically ignore the cognitive dissonance\nand accept what their prior beliefs say (Nickerson 1998). Otherwise, they invest effort by\ninvoking System 2 to decide which piece of conflicting information is true (Aronson 1969;\nKahneman 2011) which takes more time and requires greater cognitive activity.\nWhen System 2 goes to work to resolve the dissonance, it is influenced, sometimes very\nstrongly, by the unreliable results of System 1 (Kahneman 2011). The System 1 results are stored\nin working memory and become part of the problem space (Thompson 2013). System 2 has\nequal access to the information and System 1’s unreliable result, and uses both (Thompson\n2013). Information is often ambiguous and can be interpreted in different ways (Srull and Wyer\n1979). System 2 gives more weight to System 1’s result than to the facts that produced it (Srull\nand Wyer 1980; Srull and Wyer 1983). Thus, an erroneous System 1 result has greater influence\non our subsequent System 2 conclusions than the factual information (Dennis and Minas 2018).\nIn summary, we argue that placing a fake news flag on a story aligned with a user’s\nbeliefs will trigger cognitive dissonance. If the dissonance is strong enough, the user will invoke\nSystem 2 and expend greater cognitive effort to consider the headline and the fake news flag.\nThe use of System 2 cognition will be indicated by the user taking more time to make a judgment\nabout whether to believe the story or not, and by cognitive activity in certain brain regions. Our\n\n12\nstudy uses the neurophysiological responses measured by EEG as an indicator of cognitive\nactivity. We focus on activity in the frontal cortex because it has been linked with cognitive\nactivity associated with what we commonly consider to be “thinking”: arousal, memory\nencoding, memory retrieval, insight and consciousness (Başar et al. 1999; Klimesch 2012;\nKrause et al. 2000; Minas et al. 2017; Pizzagalli 2007). The result of this System 2 cognition is a\njudgment about the credibility of the story, and we theorize that the fake news flag will reduce\ncredibility. Thus, we have three hypotheses:\nH1: Social media users will exhibit increased cognitive activity in the frontal cortex when seeing\na fake news flag on a headline aligned with their beliefs.\nH2: Social media users will spend more time when seeing a fake news flag on a headline aligned\nwith their beliefs.\nH3: Social media users will perceive headlines aligned with their beliefs that are flagged as fake\nas being less credible.\nMETHOD\nParticipants\nEighty-three undergraduates were recruited from a large business core course. All were\nexperienced with social media. Age ranged from 18 to 34 (mean 19.5) and 39% were female.\nThree reported being left-handed and since a third of left-handed people have differences in brain\nstructure, we removed all three participants from our EEG analyses.\nTask\nParticipants read 50 fact-based news headlines and assessed their credibility. The\nheadlines covered 10 topics related to US politics and were actually true or false. Forty headlines\nwere designed to be possibly true or false, though verifiably one or the other (e.g., Trump\ndefunds Planned Parenthood, minimum wage should be $21.72 to keep pace with inflation). Ten\nheadlines were controls intended to be more clearly true (e.g. Trump launches Twitter tirade;\n\n13\nHollywood celebrities oppose Trump). See Appendix A for headlines. Participants spent an\naverage of 10.5 seconds reading each headline before beginning to answer questions about it.\nTreatment\nThe experiment mimicked the Facebook display, although participants were not able to\nlike, comment, or share the story. A flag matching Facebook’s fake news flag was randomly\nassigned to 20 of the 40 non-control headlines (including those actually true) (see Figure 1).\nMeasures\nThe primary behavioral dependent variable was the credibility of the headline, measured\nusing three 7-point items from Beltramini (Beltramini 1988): believability, credibility, and\ntruthfulness. The Cronbach alpha was 0.94, indicating adequate reliability.\nThe second behavioral dependent variable was the time participants took to form their\ncredibility assessment. The time was measured from the initial display of the headline until the\nparticipant clicked a button to display the credibility questions.\nThe alignment of a headline with the participant’s political beliefs was coded as a binary\nvariable; headlines positively supporting the participant’s beliefs were coded as a 1; headlines\nthat did not were coded as 0. We used ten sources of self-reported data to assess the extent to\nwhich a headline aligned with participants’ political beliefs. The participants reported their\npolitical affiliation on a 4-point scale (Democrat, Independent leaning Democrat, Independent\nleaning Republican, and Republication), which was collapsed into either Democrat (first two\nresponses) or Republican (second two responses). They reported who they would vote for\n(Clinton or Trump) if the 2016 presidential election was held today. The election had been held\nfour to six months prior. They also answered eight items (7-point scale, with 4 as neutral)\nmeasuring their political conservatism (Everett 2013) across topics related to the headlines. Our\n\n14\nsample was fairly balanced politically, with 47% being self-reported Republicans and 53%\nDemocrats; 31% reported that they would vote for Trump at the time of the study. See Appendix\nB for more information.\nTwo raters independently matched the headlines to the single most closely matching item\nout of the ten political belief items and agreed on 46 of the 50 headlines (92%); differences were\nresolved. For example, an anti-abortion headline was matched to the anti-abortion item (with\nthose scoring 5-7 being susceptible to confirmation bias). A gun-rights headline was matched to\nthe gun-rights item. A Trump-supporting headline was matched to a Trump voter.\nChanges in cognition were measured using time-frequency analysis of EEG data. EEG is\na neurophysiological tool that enables the examination of neurophysiological changes that occur\nduring information processing on the order of milliseconds (Berger 1929). EEG measures small\nelectrical signals produced in the superficial areas of the underlying cortical regions. These\nelectrical signals form complex wave patterns at specific frequencies that are related to cognitive\nactivity. Berger’s early research showed the importance of the alpha wave (originally called\n“Berger waves”), and its potential to indicate specific mental processes, including arousal,\nmemory and consciousness (Pizzagalli 2007).\nA 2012 review concluded that alpha-band waves (8-13Hz) indicate brain activity across\nmany brain regions (Klimesch 2012). Alpha waves have been shown to change reliably in\nresponse to stimuli (Klimesch 2012). When a region of the brain becomes active, alpha waves\ndesynchronize, leading to lower alpha levels (Cohen 1995); thus alpha wave desynchronization\nindicates higher levels of cognitive activity (Kelly et al. 2006; Klimesch 2012; Makeig et al.\n2002). The upper alpha frequency band (~10-13Hz) shows encoding memory processes in the\nparietal and frontal cortex regions (Kilner et al. 2005; Klimesch et al. 1997; Klimesch et al.\n\n15\n2001; Klimesch et al. 1996; Moretti et al. 2013).\nWe use time-frequency analysis, event-related spectral perturbation (ERSP), to analyze\nevent-related desynchronization (ERD) (Makeig 1993). It is important to note that, despite the\nsimilar acronym, time-frequency analysis (e.g., ERSP) differs from traditional event-related\npotential (i.e., ERP) studies in that it examines a frequency band (e.g., alpha wave) over a\nspecified time-period. Event-related potentials, common in cognitive neuroscience studies,\nexamine specific waveforms that occur at a specified time period (e.g., P300 is a positive spike\nin neural activity that occurs at 300 milliseconds in response to rare events). In time-frequency\nanalysis, we look for a pattern of changes (i.e., spectral changes) over a period of several\nseconds. We analyzed the last 4 seconds the participant viewed the headline to account for the\ntime participants read the headlines. The alpha frequency band was examined for significant\nalpha ERD in the 10 to 13Hz frequency band as has been suggested and done in prior research\n(Minas et al. 2014; Müller-Putz et al. 2015). See Appendix C for more details.\nWe used a 14-channel Emotiv wireless EEG device (see Figure 2). There is a concern\nthat wireless data collection may suffer from dropped packets due to interference. The data\nincludes a variable indicating if any signal loss occurred during data collection and marking\npackets that were interpolated. We removed all artifacts and interpolated data. There has been\ndebate within the cognitive neuroscience community about the validity of low-cost EEG systems\nlike Emotiv. Many studies have scrutinized the Emotiv device in a variety of settings such as,\nexamining working memory (Wang et al. 2015), auditory analysis (Badcock et al. 2013), mobile\nbrain-computer interfaces (Debener et al. 2012), detection of the P300 wave (Ramírez-Cortes et\nal. 2010; Wang et al. 2015), human-computer interaction (Taylor and Schmidt 2012), and\nhemispheric asymmetry (Friedman et al. 2015). These studies have found Emotiv to obtain a\n\n16\nreliable and valid signal of underlying cortical activity as good as larger high-density systems\nalbeit with lower spatial resolution so that the edges of regions are not as sharp and clear.\nRESULTS\nBehavioral Results\nWe used Hierarchical Linear Modeling (HLM) to analyze the credibility and time data;\nsee Table 1. Confirmation bias is present, with participants more likely to believe headlines to be\ncredible when they aligned with the user’s political beliefs (t(4150)=2.46, p=0.014). The fake\nnews flag (Flagged as False) had no effect (t(4150)=0.52, p=0.601). Surprisingly, participants\nwere more likely to believe that true headlines were less credible (t(4150)=2.45, p=.014). We\nnote that participants had difficulty assessing whether headlines were true or false; they correctly\nassessed only 44%. Participants spent 1.4 seconds longer considering a headline when the\nheadline was flagged as false (t(4150)=3.32, p=0.001), and an additional 1.9 seconds when the\nheadline was flagged as false and the headline aligned with their beliefs (t(4150)=2.45, p=0.014).\nThese results support H2 (that users take more time when seeing a fake flag on a headline\naligned with their beliefs). However, H3 was not supported: the fake news flag did not reduce the\ncredibility of headlines aligned with beliefs.\nNeurophysiological Results\nWe examined the cognition triggered by a headline that supported the participant’s\nbeliefs but was flagged as being false. When a brain region is active, desynchronization of neural\nactivity in the alpha band occurs (called “alpha blocking”) (Potter and Bolls 2012), thus event-\nrelated desynchronization (ERD) is an indicator of cognitive activity. Event-related spectral\nperturbation analysis produces a set of areas within the brain (called clusters) showing the\nlocation of ERD and whether there are significant differences between the treatments. With this\n\n17\nanalysis, the researcher does not specify which superficial layers of cortex to test (all regions are\ntested), which is a major strength of this approach; it is not limited to a priori decisions which\nmay or may not fit the reality of participant cognition. The clusters identified by the analysis may\nor may not align with the regions that the researcher has hypothesized about, and may span\nseveral distinct brain regions, making interpretation challenging. However, when a cluster\nincludes a theorized region, it is a powerful signal supporting the theory, because nothing in the\nanalysis directed the software to consider the theorized region; the region emerged from the data.\nThe challenge occurs when a cluster shows significant differences between treatments in\nregions the researcher did not theorize prior to the analysis. In this case, the researcher must\ninterpret what cognitive activity in those region(s) means. Unfortunately, there are often several\nways to interpret activity as each brain region is responsible for a complex set of disparate\nactivities (Poldrack 2011). This has been called the reverse inference problem (Poldrack 2011).\nFor researchers grounded in quantitative positivist traditions, the reverse inference\nproblem is intractable (Fischer 1970) because it is usually impossible to apply deductive\nreasoning to unequivocally produce a result (Fischer 1970). For researchers grounded in\nqualitative interpretivist methods, reverse inference is normal science using abductive reasoning\n(Dubois and Gadde 2002; Peirce 1931-1958). Abduction is a form of scientific reasoning that\nstarts with data and sorts through the possible explanations to find the most appropriate\nexplanation (Dubois and Gadde 2002; Peirce 1931-1958). Quantitative researchers routinely use\nabduction to build theory (Van de Ven 2007), so the question is not whether to use abduction,\nbut rather when to use it – before or after data collection, or both.\nWe strongly advocate for reverse inference using abduction after data collection when the\nanalysis shows significant differences in regions that were not theorized prior to data collection.\n\n18\nAfter all, what is the alternative? Ignore significant effects that were not theorized? Ignoring the\nunexpected is not good science. For us, reverse inference is not a problem; it is an opportunity\nfor discovering the unanticipated. Care must be taken in experimental design to eliminate as\nmany competing explanations as possible, and when interpreting unexpected results, we must use\nabductive reasoning to determine the most likely explanation (Poldrack 2011). Subsequent\nresearch can then theorize and test the newly discovered unexpected results for generalizability.\nOur analysis produced two neurological clusters with significant differences that suggest\nparticipants experienced cognitive dissonance. The first cluster was as hypothesized in frontal\ncortices (see Figure 3). Participants showed significantly more ERD in the frontal cortices for\nheadlines that supported their beliefs but were flagged as false. The differences are across the\nupper alpha band and are spread throughout the time period. Increased ERD in the frontal\ncortices is associated with increased cognitive activity, including arousal, memory, and\nconsciousness (Pizzagalli 2007) and arousal, memory access, and consciousness (Başar et al.\n1999; Krause et al. 2000; Pizzagalli 2007). The frontal cortices are active during deliberate\ncognitive tasks and high-order cognitive processes (Başar et al. 1999; Kilner et al. 2005;\nKlimesch et al. 1997; Klimesch et al. 2001; Klimesch et al. 1996; Krause et al. 2000; Moretti et\nal. 2013). This indicates that participants spent more cognitive activity considering headlines that\nsupported their beliefs but were flagged as false than other headlines.\nThe second cluster also shows ERD in the frontal cortices, but also includes some\nunexpected activity in the right parietal region (Figure 4). Activity in the right parietal can\nindicate encoding and retrieving a stimulus in working memory (Foxe and Snyder 2011; Gevins\net al. 1997; Mevorach et al. 2006). Increased ERD in the right parietal cortex is indicative of\ndirecting attention toward salient stimuli (Foxe and Snyder 2011; Mevorach et al. 2006), and\n\n19\nturning toward a stimulus (rather than away) (Schutter et al. 2001). It has also been linked to\nsustained attention to a stimulus being retained in working memory and encoding or retrieval of\nsemantic memory (Gevins et al. 1997; Klimesch et al. 1997; Klimesch et al. 2001). Both\ninterpretations provide a similar conclusion: individuals paid more attention to headlines\nsupporting their beliefs that were flagged as false.\nTaken together, these two clusters indicate that subjects spent more cognitive effort\nconsidering a headline that supported their beliefs but was flagged as false compared to other\nheadlines (e.g., those supporting their beliefs but not flagged, or challenging their beliefs\n(flagged or not)). This increased cognitive activity also corresponds to the increased time that\nsubjects spent on these headlines (Table 1). H3 is supported.\nPost hoc Analysis on Confirmation Bias\nWe also conducted a post hoc analysis investigating cognition in the presence of\nconfirmation bias, because past research in a different context (virtual team decision making – a\nutilitarian mindset) has found that individuals are more likely to engage in cognitive activity\nwhen they encounter information that supports their opinions and simply ignore information that\nopposes their opinions (Minas et al. 2014). The context in this study is different because social\nmedia users are in a hedonic rather than a utilitarian mindset (Chauhan and Pillai 2013). Hedonic\nand utilitarian motivation have been shown to have differential effects on confirmation bias\n(Borrero and Henao 2017; Stone and Wood 2018).\nThis post hoc analysis found increased cognitive activity in two clusters when\nparticipants saw headlines aligned with their opinions (i.e., when confirmation bias was present):\nthe frontal cortices (Figure 5a); and the right parietal and somatosensory region (Figure 5b). As\nnoted above, increased activity in the frontal cortices is linked to increased higher order\n\n20\ncognitive processes, while increased activity in the right parietal is linked to focusing attention.\nIncreased ERD in the somatosensory region has been linked to motion, planning for motion, or\ntactile sensations (Hari et al. 1998; Porro et al. 1996), which is hard to interpret in this situation.\nWe conclude that once participants realized that a headline supported their political opinions,\nthey directed their cognitive attention to the headline, but when they realized that a headline\nchallenged their opinions, they did not direct attention to it.\nDISCUSSION\nOur study examined whether a fake news flag helped social media users discern true\nnews from fake news. Our results show that the fake news flag did not influence user beliefs,\nalthough it triggered more cognition and increased the time spent considering the headline.\nInstead, users were more likely to believe news headlines they wanted to be true. Table 2\nsummarizes the results.\nThe EEG results show two interesting findings. First, in the absence of a fake news flag,\nsignificantly more event-related alpha desynchronization was observed in the frontal cortices and\nthe right parietal for headlines that supported the participant’s beliefs. In other words, once\nparticipants recognized that a headline supported their beliefs, they devoted attention to it; once\nthey recognized that a headline opposed their beliefs, they did not process it further.\nSecond, when participants viewed headlines that supported their beliefs but were flagged\nas fake, cognitive dissonance occurred. The EEG results showed activation in the frontal cortices\nand right parietal, which suggests that participants engaged in additional cognition to resolve the\ncognitive dissonance. However, this additional cognition only resulted in users deciding to\ndisregard the fake news flag and believe the fake news article.\nWe see three key conclusions in our results. First, the presence of a fake news flag did\n\n21\nnot affect how participants perceived the credibility of the headlines. The time and EEG results\nindicate that the flag caused cognitive dissonance and induced participants to think more deeply\nabout the truth of the headline. However, the cognitive dissonance triggered by the flag was not\nenough to overcome participants’ inherent confirmation bias; although they thought more, this\nadditional thought did not cause them to believe the headline less. The flag was simply not\nstrong enough to make users overcome their a priori beliefs. Perhaps in the era of fake news,\nusers are more likely to dismiss information that challenges their opinions as being fake. This\nfake news flag is an ineffective remedy for fake news; again, we note that Facebook discontinued\nits use in 2017 (Meixler 2017).\nSecond, confirmation bias drives beliefs. Participants were more likely to believe and\nprocess headlines that aligned with their beliefs. This likely did not result from additional\nknowledge; participants were not correct in their perceptions of truth. Rather, participants likely\nfound these topics more credible because any attempt to believe that the headlines were false\nwould result in cognitive dissonance. Rather than expend cognitive effort to consider the actual\ntruth of the article, participants rejected reality in favor of their a priori beliefs (Allcott and\nGentzkow 2017; Koriat et al. 1980; McKenzie 2006; Nickerson 1998). We confirm that social\nmedia is highly subject to confirmation bias.\nThird, the real underlying truth of the headlines had little effect on whether participants\nbelieved the headlines or not. Participants were not more likely to believe headlines that were\nverifiably true. It may be that an increased awareness of fake news may have caused participants\nto be naturally more skeptical of all headlines presented; the mean credibility score of 3.7 across\nall headlines suggests a slight bias towards skepticism.\nLimitations\n\n22\nWe began by theorizing that the context of social media is important; social media use is\noften hedonic as users browse on cell phones while waiting in line, on laptops while watching\nTV, and so on. Yet we studied use in the cold, clinical context of a lab experiment, where we\ncould carefully control exogenous factors. Ecological validity was challenged, as participants\nwere wearing an Emotiv headset, the headlines were displayed on separate pages instead of a\nscrolling homepage, and participants were not on their own social media pages. This setting may\nhave triggered a utilitarian mindset of thinking more deliberately about the headlines than the\nnormal everyday setting of social media use, perhaps weakening the effects we found. Thus, the\neffects observed in the lab may be understated; the real problem may be worse. We note that\nother studies using neurophysiological data have the same limitations (Vance et al. 2018).\nOur study also suffers from the other usual limitations of laboratory studies. Participants\nwere undergraduate students, which may not be representative of the population as a whole\n(Koriat et al. 1980). They had experience with social media, but care should be taken when\ngeneralizing to populations that may not have as much experience. The neuroanatomy of some\nyoung adults changes through their teens into their mid-20s, with neuroplasticity (i.e., changes to\nneural structures and networks) also prevalent in adulthood (Draganski et al. 2004; Sowell et al.\n1999). However, we are unaware of any studies that show systematic differences in alpha\nattenuation between young adults and adults decades older.\nAdditional limitations come from the context of fake news itself. This context is\ncontinually changing, as concerns about fake news wax and wane, types of misinformation\nchange and online social media platforms are revised in response.\nImplications for Research\nFirst and foremost, our research shows that the fake news flag triggered more cognitive\n\n23\nactivity and caused users to spend more time when the flag was placed on headlines they wanted\nto believe. However, it did not change the result of that cognition. At one level, this indicates that\nfact-checking may have promise (because it did trigger deeper cognition to resolve cognitive\ndissonance). But it may be optimistic to believe that a simple “disputed” flag might trigger the\ndeep introspection needed to overcome confirmation bias and resolve cognitive dissonance.\nFuture research needs to develop and test a stronger signaling mechanism for the results of fact-\nchecking, one that might be strong enough to overcome confirmation bias. Perhaps this could be\na different flag with stronger words, or a different type of intervention altogether.\nSecond, the post-hoc analysis indicates that future research needs to understand how we\ncan overcome confirmation bias in the use of social media. Our results show that once users\nrecognize that a headline challenges their a priori beliefs, they stopped thinking about it. In other\nwords, confirmation bias is so strong in social media use that users simply stop thinking about\ninformation they don’t like. We need more research on confirmation bias in social media use.\nThird, we used EEG to complement other sources of data such as self-reported data (i.e.,\nbeliefs in the headlines) and observed data (i.e., time taken), as has been advocated in the use of\nneurophysiological tools in IS research (Dimoka et al. 2012). The primary advantages of\nneurophysiological data are that they are generally not susceptible to subjectivity bias, social\ndesirability bias, and demand effects (Dimoka et al. 2012). The use of three distinct types of data\nenabled us to triangulate across the different sources to better understand the phenomenon\n(Dimoka et al. 2012). We encourage future research to consider using neurophysiological data.\nFinally, our results add to the growing list of evidence that fake news is becoming a\nmajor societal problem (House of Commons 2018; Weiss 2017). Many solutions have been\nproposed, and many pundits have offered opinions. However, we have little empirical research\n\n24\non the effectiveness (or lack thereof) of the many proffered options. We need more research on\nways to improve social media users’ ability to discern truth from fiction, and more research on\nways to induce social media users to invest more time and attention in the news they see and to\nrestrain from spreading fake news without reading it (Gabielkov et al. 2016).\nImplications for Practice\nFacebook’s fake news flag had no effect on beliefs. It did not induce participants to\nconclude that a news article was less credible. They spent more time when a headline they\nsupported was flagged as fake, but the flag did not change their beliefs. Perhaps more\nimportantly, the actual truth of a headline did not influence users’ beliefs; users were generally\nunable to accurately separate true news from fake news. We conclude that we need to develop a\nbetter method for warning social media users of fake news.\nPeople will continue to consume news on social media and will continue to struggle to\ndetermine its truthfulness. The sheer volume of fake news on social media (Silverman 2016)\nmeans that this problem is unlike any we have seen before; \"quantity has a quality all its own.\"\n(attributed to Josef Stalin). There are real and demonstrable consequences from enabling the\nspread of posts that are verifiably false in order to spread disinformation or profit from users’\ngullibility (House of Commons 2018). We believe that Facebook and other social media firms\nhave a responsibility to better enable their users to discern truth from fiction (see House of\nCommons 2018).\n\n25\nFigure 1: Fake News Flag on a Facebook Headline\nFigure 2. Position of the electrodes on the EEG headset with labels along the 10-20\nsystem.\n\n26\nFigure 3 Differences in Frontal Cortices Cluster due to Cognitive Dissonance\nThe top left and middle panel shows alpha activation for headlines we theorize create dissonance (one that support\nthe participant’s beliefs and were flagged as false (middle panel) versus all other headlines (left panel); cooler colors\n(i.e., blue) indicates greater cognition (i.e., greater event-related desynchronization). The right panel shows\nsignificant differences (in red) between the two panels, at p = .01 with a false discovery rate correction for multiple\ncomparisons. In the scalp map, red indicates the regions identified as being active (i.e., contributing the most\nvariance) in the cluster.\n\n27\nFigure 4. Differences in Frontal and Right Parietal Cluster due to Cognitive Dissonance\nThis analysis shows cognition for headlines that supported a participant’s political beliefs and were flagged as false\n(middle panel) versus all other headlines (left panel); blue indicates greater desynchronization. The right panel\nshows significant differences (in red) between the two panels, at p = .01 with a false discovery rate correction.\n\n28\na) Frontal cortices\nb) Right parietal and somatosensory\nFigure 5: Differences due to Confirmation Bias\nHeadlines supporting the participant’s beliefs trigger greater cognitive activity in the frontal cortices (Panel a) and in\nthe right parietal and somatosensory regions (Panel b).\n\n29\nTable 1: Behavioral Outcomes\nNote: * p<.05, ** p<.01, *** p<.001\nTable 2: Summary of Results\nHypothesis Description Supported\nH1 Social media users will exhibit increased cognitive activity\nin the frontal cortex when seeing a fake news flag on a\nheadline aligned with their beliefs.\nYes\nH2 Social media users will spend more time when seeing a fake\nnews flag on a headline aligned with their beliefs.\nYes\nH3 Social media users will perceive headlines aligned with their\nbeliefs that are flagged as fake as being less credible.\nNo\nPerceived Credibility Time Spent in Seconds\nFactor Coefficient P-value Coefficient P-value\nIntercept 3.328*** 0.000 8.166 0.062\nPolitical Party (Democrat=1) 0.312* 0.016 -0.913 0.248\nGender 0.053 0.689 0.169 0.836\nAge 0.012 0.738 0.116 0.598\nAligned with Beliefs 0.177* 0.014 -0.637 0.179\nActually True -0.130* 0.014 -0.064 0.855\nFlagged as False 0.033 0.601 1.364*** 0.001\nFlagged X Aligned with Beliefs -0.035 0.762 1.884* 0.014\n\n30\nREFERENCES\nAbramson, J. B., Orren, G. R., and Arterton, F. C. 1990. Electronic Commonwealth: The Impact\nof New Media Technologies on Democratic Politics. Basic Books, Inc.\nAchtziger, A., and Alós-Ferrer, C. 2013. \"Fast or Rational? A Response-Times Study of\nBayesian Updating,\" Management Science (60:4), pp. 923-938.\nAllcott, H., and Gentzkow, M. 2017. \"Social Media and Fake News in the 2016 Election,\"\nNational Bureau of Economic Research.\nAronson, E. 1969. \"The Theory of Cognitive Dissonance: A Current Perspective,\" Advances in\nexperimental social psychology (4), pp. 1-34.\nBadcock, N. A., Mousikou, P., Mahajan, Y., de Lissa, P., Thie, J., and McArthur, G. 2013.\n\"Validation of the Emotiv Epoc® Eeg Gaming System for Measuring Research Quality\nAuditory Erps,\" PeerJ (1), p. e38.\nBago, B., and De Neys, W. 2017. \"Fast Logic?: Examining the Time Course Assumption of Dual\nProcess Theory,\" Cognition (158), pp. 90-109.\nBargh, J. A., and Ferguson, M. J. 2000. \"Beyond Behaviorism: On the Automaticity of Higher\nMental Processes,\" Psychological bulletin (126:6), p. 925.\nBarthel, M., Mitchell, A., and Holcomb, J. 2016. \"Many Americans Believe Fake News Is\nSowing Confusion,\" Pew Research Center (15).\nBaşar, E., Başar-Eroğlu, C., Karakaş, S., and Schürmann, M. 1999. \"Are Cognitive Processes\nManifested in Event-Related Gamma, Alpha, Theta and Delta Oscillations in the Eeg?,\"\nNeuroscience Letters (259:3), pp. 165-168.\nBBC. 2017. \"Prices for Fake News Campaigns Revealed,\" in: BBC News.\nBellini-Leite, S. d. C. 2013. \"The Embodied Embedded Character of System 1 Processing,\"\nMens Sana Monographs (11:1), pp. 239-252.\nBeltramini, R. F. 1988. \"Perceived Believability of Warning Label Information Presented in\nCigarette Advertising,\" Journal of Advertising (17:2), pp. 26-32.\nBennett, W. L., and Iyengar, S. 2008. \"A New Era of Minimal Effects? The Changing\nFoundations of Political Communication,\" Journal of Communication (58:4), pp. 707-\n731.\nBerger, H. 1929. \"On the Eeg in Humans,\" Arch. Psychiatr. Nervenkr (87), pp. 527-570.\nBorrero, S., and Henao, F. 2017. \"Can Managers Be Really Objective? Bias in Multicriteria\nDecision Analysis,\" Academy of Strategic Management Journal (16:1).\nBozdag, E., and van den Hoven, J. 2015. \"Breaking the Filter Bubble: Democracy and Design,\"\nEthics and Information Technology (17:4), pp. 249-265.\nCacioppo, J. T., Cacioppo, S., and Petty, R. E. 2018. \"The Neuroscience of Persuasion: A\nReview with an Emphasis on Issues and Opportunities,\" Social neuroscience (13:2), pp.\n129-172.\nCacioppo, J. T., Petty, R. E., Kao, C. F., and Rodriguez, R. 1986. \"Central and Peripheral Routes\nto Persuasion: An Individual Difference Approach,\" Journal of Personality and Social\nPsychology (51:5), pp. 1032-1043.\nCarlston, D. E., and Skowronski, J. J. 1994. \"Savings in the Relearning of Trait Information as\nEvidence for Spontaneous Inference Generation,\" (66), pp. 840-856.\nCerf, V. G. 2016. \"Information and Misinformation on the Internet,\" Commun. ACM (60:1), pp.\n9-9.\n\n31\nChaiken, S. 1980. \"Heuristic Versus Systematic Information Processing and the Use of Source\nVersus Message Cues in Persuasion,\" Journal of Personality and Social Psychology\n(39:2), pp. 752-766.\nChaiken, S., and Eagly, A. H. 1983. \"Communication Modality as a Determinant of Persuasion:\nThe Role of Communicator Salience,\" Journal of Personality and Social Psychology\n(45:2), pp. 241-256.\nChauhan, K., and Pillai, A. 2013. \"Role of Content Strategy in Social Media Brand\nCommunities: A Case of Higher Education Institutes in India,\" Journal of Product &\nBrand Management (22:1), pp. 40-51.\nCohen, L. 1995. Time-Frequency Analysis. Prentice hall.\nColleoni, E., Rozza, A., and Arvidsson, A. 2014. \"Echo Chamber or Public Sphere? Predicting\nPolitical Orientation and Measuring Political Homophily in Twitter Using Big Data,\"\nJournal of Communication (64:2), pp. 317-332.\nDe Neys, W. 2014. \"Conflict Detection, Dual Processes, and Logical Intuitions: Some\nClarifications,\" Thinking & Reasoning (20:2), pp. 169-187.\nDe Neys, W. 2018. Dual Process Theory 2.0. New York: Routledge.\nDebener, S., Minow, F., Emkes, R., Gandras, K., and Vos, M. 2012. \"How About Taking a Low‐\nCost, Small, and Wireless Eeg for a Walk?,\" Psychophysiology (49:11), pp. 1617-1621.\nDennis, A. R., and Minas, R. K. 2018. \"Security on Autopilot: Why Current Security Theories\nHijack Our Thinking and Lead Us Astray,\" the DATABASE for Advances in Information\nSystems (49:SI), pp. 15-38.\nDictionary, C. 2018. \"Democracy,\" in: Cambridge Dictionary.\nDimoka, A., Banker, R. D., Benbasat, I., Davis, F. D., Dennis, A. R., Gefen, D., Gupta, A.,\nIschebeck, A., Kenning, P. H., and Pavlou, P. A. 2012. \"On the Use of\nNeurophysiological Tools in Is Research: Developing a Research Agenda for Neurois,\"\nMIS Quarterly (36:3).\nDraganski, B., Gaser, C., Busch, V., Schuierer, G., Bogdahn, U., and May, A. 2004.\n\"Neuroplasticity: Changes in Grey Matter Induced by Training,\" Nature (427:6972), p.\n311.\nDubois, A., and Gadde, L.-E. 2002. \"Systematic Combining: An Abductive Approach to Case\nResearch,\" Journal of business research (55:7), pp. 553-560.\nDwoskin, E., and Shaban, H. 2018. \"Facebook Will Now Ask Users to Rank News\nOrganizations They Trust,\" in: The Washington Post.\nDwoskin, E., and Timberg, C. 2018. \"How Merchants Use Facebook to Flood Amazon with\nFake Reviews,\" in: The Washington Post.\nEvans, J. S. B. T. 2008. \"Dual-Processing Accounts of Reasoning, Judgment, and Social\nCognition,\" Annual Review of Psychology (59:1), pp. 255-278.\nEvans, J. S. B. T. 2014. \"Two Minds Rationality,\" Thinking & Reasoning (20:2), pp. 129-146.\nEvans, J. S. B. T., and Stanovich, K. E. 2013. \"Dual-Process Theories of Higher Cognition:\nAdvancing the Debate,\" Perspectives on Psychological Science (8:3), pp. 223-241.\nEverett, J. A. C. 2013. \"The 12 Item Social and Economic Conservatism Scale (Secs),\" PLOS\nONE (8:12), p. e82131.\nFazio, R. H., Sanbonmatsu, D. M., Powell, M. C., and Kardes, F. R. 1986. \"On the Automatic\nActivation of Attitudes,\" Journal of Personality and Social Psychology (50:2), pp. 229-\n238.\nFestinger, L. 1962. A Theory of Cognitive Dissonance. Stanford university press.\n\n32\nFischer, D. H. 1970. Historians' Fallacies. Harper and Row New York.\nFisher, M., J.W. Cox, P. Hermann,. 2016. \"Pizzagate: From Rumor, to Hashtag, to Gunfire in\nD.C..\" The Washington Post.\nFoxe, J. J., and Snyder, A. C. 2011. \"The Role of Alpha-Band Brain Oscillations as a Sensory\nSuppression Mechanism During Selective Attention,\" Frontiers in psychology (2), p. 154.\nFriedman, D., Shapira, S., Jacobson, L., and Gruberger, M. 2015. \"A Data-Driven Validation of\nFrontal Eeg Asymmetry Using a Consumer Device,\" Affective Computing and Intelligent\nInteraction (ACII), 2015 International Conference on, pp. 930-937.\nGabielkov, M., Ramachandran, A., Chaintreau, A., and Legout, A. 2016. \"Social Clicks: What\nand Who Gets Read on Twitter?,\" ACM SIGMETRICS / IFIP Performance 2016, Antibes\nJuan-les-Pins, France.\nGevins, A., Smith, M. E., McEvoy, L., and Yu, D. 1997. \"High-Resolution Eeg Mapping of\nCortical Activation Related to Working Memory: Effects of Task Difficulty, Type of\nProcessing, and Practice,\" Cerebral cortex (New York, NY: 1991) (7:4), pp. 374-385.\nGottfried, J., and Shearer, E. 2016. News Use across Social Medial Platforms 2016. Pew\nResearch Center.\nGranville, K. 2018. \"Facebook and Cambridge Analytica: What You Need to Know as Fallout\nWidens,\" The New York Times.\nHaidt, J. 2012. The Righteous Mind: Why Good People Are Divided by Politics and Religion.\nVintage.\nHari, R., Forss, N., Avikainen, S., Kirveskari, E., Salenius, S., and Rizzolatti, G. 1998.\n\"Activation of Human Primary Motor Cortex During Action Observation: A\nNeuromagnetic Study,\" Proceedings of the National Academy of Sciences (95:25), pp.\n15061-15065.\nHirschman, E. C., and Holbrook, M. B. 1982. \"Hedonic Consumption: Emerging Concepts,\nMethods and Propositions,\" Journal of Marketing (46:3), pp. 92-101.\nHong, W., Chan, F. K. Y., Thong, J. Y. L., Chasalow, L. C., and Dhillon, G. 2014. \"A\nFramework and Guidelines for Context-Specific Theorizing in Information Systems\nResearch,\" Information Systems Research (25:1), pp. 111-136.\nHouse of Commons. 2018. \"Disinformation and ‘Fake News’: Select Committee on Digital,\nCulture, Media and Sport Interim Report \", from\nhttps://www.parliament.uk/business/committees/committees-a-z/commons-select/digital-\nculture-media-and-sport-committee/news/fake-news-report-published/\nInstitute for Propaganda Analysis. 1938. \"Propaganda Analysis: Volume I of the Publications of\nthe Institute for Propaganda Analysis, Inc. With New Materials to Aid Student and Adult\nGroups in the Analysis of Today's Propaganda,\" Institute for Propaganda Analysis (I).\nJohns, G. 2006. \"The Essential Impact of Context on Organizational Behavior,\" Academy of\nManagement Review (31:2), pp. 386-408.\nJohns, G. 2017. \"Reflections on the 2016 Decade Award: Incorporating Context in\nOrganizational Research,\" Academy of Management Review (42:4), pp. 577-595.\nJohnson, T. J., and Kaye, B. K. 2015. \"Reasons to Believe: Influence of Credibility on\nMotivations for Using Social Networks,\" Computers in human behavior (50), pp. 544-\n555.\nJonas, E. S. S.-H. 2001. \"Confirmation Bias in Sequential Information Search after Preliminary\nDecisions: An Expansion of Dissonance Theoretical Research on Selective Exposure to\nInformation,\" Journal of Personality and Social Psychology (80:4), p. 14.\n\n33\nKahneman, D. 2011. Thinking, Fast and Slow. Macmillan.\nKelly, S. P., Lalor, E. C., Reilly, R. B., and Foxe, J. J. 2006. \"Increases in Alpha Oscillatory\nPower Reflect an Active Retinotopic Mechanism for Distracter Suppression During\nSustained Visuospatial Attention,\" Journal of Neurophysiology (95:6), pp. 3844-3851.\nKilner, J. M., Mattout, J., Henson, R., and Friston, K. J. 2005. \"Hemodynamic Correlates of Eeg:\nA Heuristic,\" NeuroImage (28:1), pp. 280-286.\nKim, A., and Dennis, A. R. 2018. \"Says Who?: How News Presentation Format Influences\nPerceived Believability and the Engagement Level of Social Media Users,\" Proceedings\nof the Hawaii International Conference on System Sciences, Waikoloa, HI..\nKirby, E. J. 2016. \"The City Getting Rich from Fake News,\" BBC News.\nKitchen, P. J., Kerr, G., Schultz, D. E., McColl, R., and Pals, H. 2014. \"The Elaboration\nLikelihood Model: Review, Critique and Research Agenda,\" European Journal of\nMarketing (48:11/12), pp. 2033-2050.\nKlimesch, W. 2012. \"Alpha-Band Oscillations, Attention, and Controlled Access to Stored\nInformation,\" Trends in Cognitive Sciences (16:12), pp. 606-617.\nKlimesch, W., Doppelmayr, M., Pachinger, T., and Ripper, B. 1997. \"Brain Oscillations and\nHuman Memory: Eeg Correlates in the Upper Alpha and Theta Band,\" Neuroscience\nLetters (238:1), pp. 9-12.\nKlimesch, W., Doppelmayr, M., Stadler, W., Pöllhuber, D., Sauseng, P., and Röhm, D. 2001.\n\"Episodic Retrieval Is Reflected by a Process Specific Increase in Human\nElectroencephalographic Theta Activity,\" Neuroscience Letters (302:1), pp. 49-52.\nKlimesch, W., Schimke, H., Doppelmayr, M., Ripper, B., Schwaiger, J., and Pfurtscheller, G.\n1996. \"Event-Related Desynchronization (Erd) and the Dm Effect: Does Alpha\nDesynchronization During Encoding Predict Later Recall Performance?,\" International\nJournal of Psychophysiology (24:1-2), pp. 47-60.\nKnobloch‐Westerwick, S., and Lavis, S. M. 2017. \"Selecting Serious or Satirical, Supporting or\nStirring News? Selective Exposure to Partisan Versus Mockery News Online Videos,\"\nJournal of Communication (67:1), pp. 54-81.\nKoriat, A., Lichtenstein, S., and Fischhoff, B. 1980. \"Reasons for Confidence,\" Journal of\nExperimental Psychology: Human learning and memory (6:2), p. 107.\nKrause, C. M., Sillanmäki, L., Koivisto, M., Saarela, C., Häggqvist, A., Laine, M., and\nHämäläinen, H. 2000. \"The Effects of Memory Load on Event-Related Eeg\nDesynchronization and Synchronization,\" Clinical Neurophysiology (111:11), pp. 2071-\n2078.\nMaheshwari, S. 2017. \"20th Century Fox Gives Real Apology for a Fake News Campaign,\" in:\nThe New York Times. Business Day.\nMakeig, S. 1993. \"Auditory Event-Related Dynamics of the Eeg Spectrum and Effects of\nExposure to Tones,\" Electroencephalography and Clinical Neurophysiology (86:4), pp.\n283-293.\nMakeig, S., Westerfield, M., Jung, T.-P., Enghoff, S., Townsend, J., Courchesne, E., and\nSejnowski, T. 2002. \"Dynamic Brain Sources of Visual Evoked Responses,\" Science\n(295:5555), pp. 690-694.\nMcGrath, G. 1986. \"Measuring the Concept of Credibility,\" Journalism Quarterly (63:3), p. 12.\nMcKenzie, C. R. 2006. \"Increased Sensitivity to Differentially Diagnostic Answers Using\nFamiliar Materials: Implications for Confirmation Bias,\" Memory & Cognition (34:3), pp.\n577-588.\n\n34\nMeixler, E. 2017. \"Facebook Is Dropping Its Fake News Red Flag Warning after Finding It Had\nthe Opposite Effect,\" Time.\nMellencamp, J. 2004. \"Walk Tall,\" in: Words & Music: John Mellencamp's Greatest Hits. Island.\nMelnikoff, D. E., and Bargh, J. A. 2018. \"The Mythical Number Two,\" Trends in Cognitive\nSciences (22:4), pp. 280-293.\nMevorach, C., Humphreys, G. W., and Shalev, L. 2006. \"Effects of Saliency, Not Global\nDominance, in Patients with Left Parietal Damage,\" Neuropsychologia (44:2), pp. 307-\n319.\nMills, J. 1999. \"Improving the 1957 Version of Dissonance Theory,\" in Cognitive Dissonance:\nProgress on a Pivotal Theory in Social Psychology. Washington, DC, US: American\nPsychological Association, pp. 25-42.\nMinas, R. K., Dennis, A. R., Potter, R. F., and Kamhawi, R. 2017. \"Triggering Insight: Using\nNeuroscience to Understand How Priming Changes Individual Cognition During\nElectronic Brainstorming,\" Decision Sciences.\nMinas, R. K., Potter, R. F., Dennis, A. R., Bartelt, V. L., and Bae, S. 2014. \"Putting on the\nThinking Cap: Using Neurois to Understand Information Processing Biases in Virtual\nTeams,\" Journal of Management Information Systems (30:4), pp. 49-82.\nMoretti, D. V., Paternicò, D., Binetti, G., Zanetti, O., and Frisoni, G. B. 2013. \"Eeg Upper/Low\nAlpha Frequency Power Ratio Relates to Temporo-Parietal Brain Atrophy and Memory\nPerformances in Mild Cognitive Impairment,\" Frontiers in Aging Neuroscience (5), p.\n63.\nMüller-Putz, G. R., Riedl, R., and Wriessnegger, S. C. 2015. \"Electroencephalography (Eeg) as a\nResearch Tool in the Information Systems Discipline: Foundations, Measurement, and\nApplications,\" CAIS (37), p. 46.\nNewsGuard. 2018. \"Newsguard: Criteria for and Explanation of Ratings.\" Retrieved July 28,\n2018, 2018, from https://newsguardtechnologies.com/our-ratings/\nNickerson, R. S. 1998. \"Confirmation Bias: A Ubiquitous Phenomenon in Many Guises,\" Review\nof General Psychology (2:2), p. 26.\nPeirce, C. S. 1931-1958. \"Collected Papers of Charles Sanders Peirce,\" in: Volumes 1-8.\nCambridge University Press: Harvard, MA.\nPennycook, G., Neys, W. D., Evans, J. S. B. T., Stanovich, K. E., and Thompson, V. A. 2018.\n\"The Mythical Dual-Process Typology,\" Trends in Cognitive Sciences (22:8), pp. 667-\n668.\nPetty, R. E., and Cacioppo, J. T. 1986. Communication and Persuasion: Central and Peripheral\nRoutes to Attitude Change. New York: Springer-Verlag.\nPizzagalli, D. A. 2007. \"Electroencephalography and High-Density Electrophysiological Source\nLocalization,\" in Handbook of Psychophysiology, J. Cacioppo and G.B. Tassinary (eds.).\nNew York: Cambridge University Press, pp. 56-84.\nPoldrack, R. A. 2011. \"Inferring Mental States from Neuroimaging Data: From Reverse\nInference to Large-Scale Decoding,\" Neuron (72:5), p. 6.\nPorro, C. A., Francescato, M. P., Cettolo, V., Diamond, M. E., Baraldi, P., Zuiani, C., Bazzocchi,\nM., and Di Prampero, P. E. 1996. \"Primary Motor and Sensory Cortex Activation During\nMotor Performance and Motor Imagery: A Functional Magnetic Resonance Imaging\nStudy,\" Journal of Neuroscience (16:23), pp. 7688-7698.\nPotter, R. F., and Bolls, P. 2012. Psychophysiological Measurement and Meaning: Cognitive and\nEmotional Processing of Media. Routledge.\n\n35\nRamírez-Cortes, J. M., Alarcon-Aquino, V., Rosas-Cholula, G., Gomez-Gil, P., and Escamilla-\nAmbrosio, J. 2010. \"P-300 Rhythm Detection Using Anfis Algorithm and Wavelet\nFeature Extraction in Eeg Signals,\" Proceedings of the World Congress on Engineering\nand Computer Science: International Association of Engineers San Francisco, pp. 963-\n968.\nRatkiewicz, J., Conover, M., Meiss, M., Gonçalves, B., Patil, S., Flammini, A., and Menczer, F.\n2011. \"Truthy: Mapping the Spread of Astroturf in Microblog Streams,\" Proceedings of\nthe 20th international conference companion on World wide web: ACM, pp. 249-252.\nRice, R. E., Gustafson, A., and Hoffman, Z. 2018. \"Frequent but Accurate: A Closer Look at\nUncertainty and Opinion Divergence in Climate Change Print News,\" Environmental\nCommunication (12:3), pp. 301-321.\nRoberts, D. 2013. \"Yelp’s Fake Review Problem,\" Fortune magazine.\nSchaedel, S. 2017. \"How to Flag Fake News on Facebook.\" from\nhttp://www.factcheck.org/2017/07/flag-fake-news-facebook/\nSchutter, D. J., Putman, P., Hermans, E., and van Honk, J. 2001. \"Parietal Electroencephalogram\nBeta Asymmetry and Selective Attention to Angry Facial Expressions in Healthy Human\nSubjects,\" Neuroscience letters (314:1-2), pp. 13-16.\nShane, S. 2017. \"The Fake Americans Russia Created to Influence the Election.\" from\nhttps://www.nytimes.com/2017/09/07/us/politics/russia-facebook-twitter-election.html\nShao, C., Ciampaglia, G. L., Flammini, A., and Menczer, F. 2016. \"Hoaxy: A Platform for\nTracking Online Misinformation,\" Proceedings of the 25th International Conference\nCompanion on World Wide Web, pp. 745-750.\nSilverman, C. 2016. \"This Analysis Shows How Viral Fake Election News Stories Outperformed\nReal News on Facebook,\" Buzzfeed News (16).\nSowell, E. R., Thompson, P. M., Holmes, C. J., Jernigan, T. L., and Toga, A. W. 1999. \"In Vivo\nEvidence for Post-Adolescent Brain Maturation in Frontal and Striatal Regions,\" Nature\nneuroscience (2:10), p. 859.\nSrull, T. K., and Wyer, R. S. 1979. \"The Role of Category Accessibility in the Interpretation of\nInformation About Persons: Some Determinants and Implications,\" Journal of\nPersonality and Social Psychology (37:10), pp. 1660-1672.\nSrull, T. K., and Wyer, R. S. 1980. \"Category Accessibility and Social Perception: Some\nImplications for the Study of Person Memory and Interpersonal Judgments.\" US:\nAmerican Psychological Association, pp. 841-856.\nSrull, T. K., and Wyer, R. S. 1983. \"The Role of Control Processes and Structural Constraints in\nModels of Memory and Social Judgment,\" Journal of Experimental Social Psychology\n(19:6), pp. 497-521.\nStanovich, K. E. 1999. Who Is Rational?: Studies of Individual Differences in Reasoning.\nPsychology Press.\nStatista. 2018. \"Number of Monthly Active Facebook Users Worldwide as of 1st Quarter 2018\n(in Millions).\"\nStone, D. F., and Wood, D. H. 2018. \"Cognitive Dissonance, Motivated Reasoning, and\nConfirmation Bias: Applications in Industrial Organization,\" Handbook of Behavioral\nIndustrial Organization.\nSydell, L. 2016. \"We Tracked Down a Fake-News Creator in the Suburbs. Here’s What We\nLearned.\" National Public Radio.\n\n36\nTaylor, G. S., and Schmidt, C. 2012. \"Empirical Evaluation of the Emotiv Epoc Bci Headset for\nthe Detection of Mental Actions,\" Proceedings of the Human Factors and Ergonomics\nSociety Annual Meeting (56:1), pp. 193-197.\nTaylor, S. E., and Fiske, S. T. 1978. \"Salience, Attention, and Attribution: Top of the Head\nPhenomena,\" in Advances in Experimental Social Psychology, L. Berkowitz (ed.).\nAcademic Press, pp. 249-288.\nTewksbury, D. 2003. \"What Do Americans Really Want to Know? Tracking the Behavior of\nNews Readers on the Internet,\" Journal of Communication (2003:December), p. 17.\nThe Wall Street Journal. 2016. \"Blue Feed, Red Feed.\" from \nhttp://graphics.wsj.com/blue-feed-\nred-feed/\nThompson, V. A. 2013. \"Why It Matters: The Implications of Autonomous Processes for Dual\nProcess Theories—Commentary on Evans & Stanovich (2013),\" Perspectives on\nPsychological Science (8:3), pp. 253-256.\nThompson, V. A., Prowse Turner, J. A., and Pennycook, G. 2011. \"Intuition, Reason, and\nMetacognition,\" Cognitive Psychology (63:3), pp. 107-140.\nTurel, O. f., and Qahri-Saremi, H. 2016. \"Problematic Use of Social Networking Sites:\nAntecedents and Consequences from a Dual-System Theory Perspective,\" Journal of\nManagement Information Systems (33:4), pp. 1087-1116.\nVan de Ven, A. H. 2007. Engaged Scholarship: A Guide for Organizational and Social\nResearch. Oxford University Press.\nVance, A., Jenkins, J. L., and Anderson, B. B. 2018. \"Tuning out Security Warnings: A\nLongitudinal Examination of Habituation through Fmri, Eye Tracking, and Field\nExperiments,\" MIS Quarterly (42:2), pp. 355-380.\nWang, S., Gwizdka, J., and Chaovalitwongse, W. A. 2015. \"Using Wireless Eeg Signals to\nAssess Memory Workload in the α-Back Task,\" IEEE Transactions on Human-Machine\nSystems (PP:99), pp. 1-12.\nWeiss, R. 2017. \"Nip Misinformation in the Bud,\" Science (358:6362), pp. 427-427.\nWintersieck, A. L. 2017. \"Debating the Truth: The Impact of Fact-Checking During Electoral\nDebates,\" American Politics Research (45:2), pp. 304-331.\nZuckerberg, M. 2016. \"Status Update.\" Facebook.com.\nView publication stats"
    },
    {
      "file_name": "The-Psychology-of-Fake-News_2021_tics.pdf",
      "file_size": 75308,
      "file_type": "pdf",
      "extracted_content": "Review\nThe Psychology of Fake News\nGordon Pennycook \n1,2,\n* and David G. Rand\n \n3,4,5,\n*\nWe synthesize a burgeoning literature investigating why people believe and share\nfalse or highly misleading news online. Contrary to a common narrative whereby\npolitics drives susceptibility to fake news, people are ‘better’ at discerning truth\nfrom falsehood (despite greater overall belief) when evaluating politically concor-\ndant news. Instead, poor truth discernment is associated with lack of careful rea-\nsoning and relevant knowledge, and the use of heuristics such as familiarity.\nFurthermore, there is a substantial disconnect between what people believe and\nwhat they share on social media. This dissociation is largely driven by inattention,\nmore so than by purposeful sharing of misinformation. Thus, interventions can\nsuccessfully nudge social media users to focus more on accuracy. Crowdsourced\nveracity ratings can also be leveraged to improve social media ranking algorithms.\nToward a Psychology of False and Misleading Online News\nFabricated news is nothing new. For example, in 1835 The Sun newspaper in New York\npublished six articles about purported life on the moon which came to be known as the 'Great\nMoon Hoax'. During the 2016 US Presidential Election and UK Brexit Referendum, however, a\ndifferent form of fake news (see Glossary) rose to prominence (Box 1): false or highly misleading\npolitical 'news' stories, primarily originating on social media [1]. Concern about fake news was\nredoubled in 2020 in the face of widespread misinformation and disinformation [2] on social\nmedia about the coronavirus disease 2019 (COVID-19) pandemic [3] and the 2020 US Presidential\nElection [4]. Misleading hyperpartisan news, as well as yellow journalism [5], are related forms\nof problematic news content that are likely sources of political polarization [6]. What is it about\nhuman psychology – and its interaction with social media [7,8] – that explains the failure to distin-\nguish between accurate and inaccurate content online? Apart from being of theoretical\ninterest, this question has practical consequences: developing effective interventions against mis-\ninformation depends on understanding the underlying psychology.\nWe focus here primarily on online content that is presented in the form of news articles. However,\nfalse and misleading claims come in many forms, and there are several literatures that are clearly\nrelated, but outside the scope of our review (although we will draw some connections throughout).\nThese include work on conspiracy belief [9], superstition [10], rumors [11], bullshit receptivity [12],\nand misperceptions [13], among others. Furthermore, our focus is on individual examples of misinfor-\nmation, and not on organized disinformation campaigns (e.g., by the Russian Internet Research\nAgency, or campaigns relating to global warming or fraud in the 2020 US Presidential Election).\nWhy Do People Fall for Fake News?\nWhen considering the factors that may influence what people believe, it is essential to distinguish\nbetween two fundamentally different ways to conceptualize belief in true and false news. One\ncommon approach is to focus on truth ‘discernment’, or the extent to which misinformation is\nbelieved ‘relative’ to accurate content. Discernment, typically calculated as belief in true news\nminus belief in false news (akin to 'sensitivity' or d' in signal detection theory [14]) captures the\n‘overall’ accuracy of one's beliefs – and thus gives insight into failures to distinguish between\ntrue and false content ('falling for fake news').\nHighlights\nRecent evidence contradicts the com-\nmon narrative that partisanship and\npolitically motivated reasoning explain\nwhy people fall for 'fake news'.\nPoor truth discernment is linked to a\nlack of careful reasoning and relevant\nknowledge, as well as to the use of\nfamiliarity and source heuristics.\nThere is also a large disconnect between\nwhat people believe and what they will\nshare on social media, and this is largely\ndriven by inattention rather than by\npurposeful sharing of misinformation.\nEffective interventions can nudge social\nmedia users to think about accuracy,\nand can leverage crowdsourced veracity\nratings to improve social media ranking\nalgorithms.\n1\nHill/Levene Schools of Business,\nUniversity of Regina, Regina, SK S4S\n0A2, Canada\n2\nDepartment of Psychology, University\nof Regina, Regina, SK S4S 0A2, Canada\n3\nSloan School of Management,\nMassachusetts Institute of Technology,\nCambridge, MA 02142, USA\n4\nInstitute for Data, Systems, and Society,\nMassachusetts Institute of Technology,\nCambridge, MA 02142, USA\n5\nDepartment of Brain and Cognitive\nSciences, Massachusetts Institute of\nTechnology, Cambridge, MA 02142, USA\n*Correspondence:\ngordon.pennycook@uregina.ca\n(G. Pennycook) and drand@mit.edu\n(D.G. Rand).\n388 Trends in Cognitive Sciences, May 2021, Vol. 25, No. 5 https://doi.org/10.1016/j.tics.2021.02.007\n© 2021 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\nTrends in\nCognitive Sciences\nOPEN ACCESS\n\nAnother approach is to focus on overall belief, or the extent to which news – regardless of its\naccuracy – is believed (calculated as the average or sum of belief in true news and belief in\nfalse news, akin to calculating 'bias' in signal detection theory [14]). Critically, factors that\nalter overall belief need not impact people's ability to tell truth from falsehood [15]: increasing or de-\ncreasing belief in true and false headlines to an equivalent extent has no effect on the overall\naccuracy of one's beliefs (i.e., does not affect truth discernment).\nPolitical Motivations\nA popular narrative is that the failure to discern between true and false news is rooted in political mo-\ntivations. For example, it has been argued that people are motivated consumers of (mis)information\n[16] – that they engage in 'identity-protective cognition' when faced with politically valenced content,\nand this leads them to be overly believing of content that is consistent with their partisan identity and\noverly skeptical of content that is inconsistent with their partisan identity [17]. A related theory argues\nthat people place loyalty to their political identities above the truth – and thus fail to discern truth from\nfalsehood in favor of simply believing ideologically concordant information [18]. These accounts con-\ntend that a strong causal influence of political motivation on belief is thus the dominant factor\nexplaining why people fall for fake news.\nIt is clearly true that partisanship is associated with overall belief: People are more likely to believe\nnews content that is concordant (versus discordant) with their political partisanship [19–25]\n(Figure 1B). It is important to note, however, that the effect of political concordance is typically\nmuch smaller than that of the actual veracity of the news [20,21,26]. In other words, true but\nGlossary\nAlgorithm: steps or calculations\nperformed by a computer (given a set of\nrules) to solve a problem or complete a\ntask. In the context of social media,\nalgorithms are used to determine what\ncontent users see.\nDisinformation: information that is\nfalse or inaccurate, and that was created\nwith a deliberate intention to mislead\npeople.\nFake news: news content published\non the internet that aesthetically\nresembles actual legitimate mainstream\nnews content, but that is fabricated or\nextremely inaccurate. Also referred to as\nfalse, junk, or fabricated news.\nHyperpartisan news: news content\nthat is not entirely fabricated, but which\ncovers events that actually occurred with\na strong partisan bias. As a result,\nhyperpartisan news is typically\nmisleading, and we therefore include it\nas a form of misinformation.\nMisinformation: information that is\nfalse, inaccurate, or misleading. Unlike\ndisinformation, misinformation does not\nnecessarily need to be created\ndeliberately to mislead. Misinformation is\nsometimes used to refer exclusively to\ninaccuracies that are accidental;\nhowever, because it is difficult to\nascertain the intentions of the unknown\nindividuals who create falsehoods that\nspread on the internet, we use\nmisinformation as a broader umbrella\nterm here (i.e., much of the content used\nin the studies we discuss could be\nclassified as disinformation and/or\nhyperpartisan news and/or propaganda,\netc.).\nYellow journalism: content from\nnewspapers, magazines, or websites\nthat is poorly researched and\nsensationalist, and that is created with\nthe goal of increasing sales or, on the\ninternet, clicks. Roughly equivalent to\ntabloid journalism.\nBox 1. Prevalence of Fake News\nVarious analyses of social media and web browsing data have been used in an attempt to determine the prevalence of fake\nnews, often with a focus on the 2016 US Presidential Election. For example, using web browsing data, archives of fact-\nchecking websites, and a survey, Allcott and Gentzkow [19] estimated that a particular set of news stories that are known\nto be false were shared on Facebook at least 38 million times in the 3 months leading up to the 2016 election (30 million of\nwhich were for news favoring Donald Trump). This estimate represents a lower bound since it only reflects that specific set\nof known false news.\nOther analyses have focused on fake news publishers (i.e., websites) rather than on individual articles. Based on data from\nTwitter [117], Facebook [77,118], and web browsing [89], these studies concluded that content from known fake news\nsites represents a small proportion of most people's media diets, and that the average social media user was exposed\nto little fake news during the 2016 election.\nThese analyses have important limitations, however, because the only available data concern what people are sharing and\nwhat they visit when they click through to visit news sites off-platform. But, of course, the vast majority of the time that peo-\nple are exposed to news on social media, they simply read the post without sharing it or clicking on the link to visit the ac-\ntual source website. Furthermore, so-called 'fake news' only represents one category of misinformation, and misleading\ncontent from sources such as hyperpartisan news websites likely represents a much larger proportion of people's media\ndiets [6,119]. Thus, the actual on-platform exposure of the average user to misinformation remains an open question [120].\nWe feel it is premature to conclude that exposure rates are minimal, and thus that false and misleading news online is not a\nproblem (also [7,8]). This is especially true when looking beyond the 2016 election because new misinformation threats –\nsuch as false claims about COVID-19 [3,44] and fraud in the 2020 US Presidential Election [4] – have gained widespread\ntraction through amplification by (mostly Republican) political elites.\nAccordingly, exposure to fake news (and misinformation more broadly) is not equally distributed across all users. In\nparticular, political conservatives and older adults were far more likely to visit fake news websites or share fake news\narticles during the 2016 Presidential Election [19,89,117,118]. Studies have also found associations between political\nconservatism and belief in misinformation in the USA [20,44], Chile [121], and Germany [122], but not in Hungary [24],\nand users who engage in less reasoning have been found to share content from lower-quality news sites on Twitter\n[71]. Thus, even if it was true that the average social media user was not exposed to that much misinformation, exposures\nrates are substantially higher in subpopulations that may be particularly vulnerable to believing inaccurate content. Finally,\nmisinformation that originates on social media sometimes transitions to much larger audiences when it is picked up by\ntraditional media outlets – either via direct repetition or debunking (which may result in inadvertent amplification).\nTrends in Cognitive Sciences \nOPEN ACCESS\nTrends in Cognitive Sciences, May 2021, Vol. 25, No. 5 389\n\npolitically discordant news is typically believed much more than false but politically concordant\nnews – politics does not trump truth. Furthermore, greater overall belief in politically consistent\nnews does not necessarily indicate politically motivated reasoning. Such differences could\neven arise from unbiased rational (e.g., Bayesian) inference built on prior factual beliefs that\nTrendsTrends inin CognitiveCognitive SciencesSciences\nFigure 1. Political Concordance and Cognitive Reflection Have Divergent Relationships with Belief. Reasoning\n(or lack thereof) is much more strongly related to truth discernment than is political concordance, whereas political concordance\nis much more strongly related to overall belief than is reasoning. Data from 14 studies [20,21,26,34,35,41,49,61,84,93] in which\nUS participants were asked to rate the accuracy of actual news posts from social media (headlines equally balanced true versus\nfalse, as well as Democrat-consistent versus Republican-consistent). Study names are indicated by the first author's last initial\nand publication date, followed by a dash and study number for multistudy papers; the number of participants in each study is\nindicated under the name. Data from all 14 studies are pooled to calculate the 'All' values. (A) Difference in truth discernment\n(belief in true news minus belief in false news) between participants who scored higher on the Cognitive Reflection Test relative\nto those who scored lower (median split; collapsing over headline political concordance) and between headlines that were\npolitically concordant relative to politically discordant (collapsing over participant cognitive reflection). More-reflective participants\nwere more discerning than less-reflective participants, and discernment was higher for politically concordant headlines than for\npolitically discordant headlines. Critically, the difference based on reasoning was roughly twice as large as the difference based\non partisanship. (B) Difference in overall belief (average of belief in true news and belief in false news) between participants who\nscored higher on the Cognitive Reflection Test relative to those who scored lower (median split; collapsing over headline political\nconcordance), and between headlines that were politically concordant relative to politically discordant (collapsing over participant\ncognitive reflection). More-reflective participants had a somewhat lower overall belief than less-reflective participants, whereas\noverall belief was higher for politically concordant headlines than for politically discordant headlines. Abbreviation: Obs., observed.\nTrends in Cognitive Sciences\nOPEN ACCESS\n390 Trends in Cognitive Sciences, May 2021, Vol. 25, No. 5\n\ndiffer across party lines (e.g., owing to exposure to different information environments) [27–33]\n(Box 2 for details).\nWe now turn to the impact of political concordance on truth discernment. Greater overall belief in\npolitically concordant news might seem to suggest that people are more inaccurate when\nassessing politically concordant news – in other words, that political concordance (and the asso-\nciated motivations) interferes with truth discernment. In fact, the data reveal the opposite pattern:\nPeople are somewhat better at discerning truth from falsehood when judging politically concor-\ndant news compared with politically discordant news (Figure 1A). Taken together, the\nevidence therefore suggests that political identity and politically motivated reasoning are not the\nprimary factors driving the inability to tell truth from falsehood in online news.\nReasoning\nAnother perspective on the (in)ability to differentiate between truth and falsehood comes from the\nfield of reasoning. Work in this vein has a particular focus on dual-process theories stipulating that\nanalytic thinking can override automatic, intuitive responses (Box 3 for details). The key question\nthis perspective asks is – what is the role of reflective reasoning in the ability to discern fake\nnews from truth?\nOne potential answer, which follows from the earlier-referenced work on political identity, argues\nthat deliberative ('System 2') reasoning is often motivated by political identity, and that people\nBox 2. Challenges in Identifying Politically Motivated Reasoning\nThe observation that people are more likely to believe information that is consistent with their political ideology/partisanship\n(and are less likely to believe information that is inconsistent with their ideology/partisanship) is often taken as evidence for\npolitically motivated reasoning [22,123,124]. Critically, however, this pattern does not actually provide clear evidence of\npolitically motivated reasoning because partisan identity is likely confounded with other relevant variables [27,125]. Most\nnotably, partisans differ in what they believe about the world, even when it comes to so-called 'factual beliefs' – beliefs that\nrelate to facts or empirical evidence [28], such as in the case of global warming [126]. This is critical because a large body of\nevidence from entirely nonpolitical contexts shows that what people believe to be true about the world influences their\nreasoning (a phenomenon known as belief bias [127]). Indeed, once prior beliefs are accounted for, the apparent impact\nof political concordance on processes of belief formation is typically greatly reduced or eliminated [28,29,128]. Thus,\nobserving a difference across ideological lines is not sufficient evidence to conclude that partisan identity or political mo-\ntivations are themselves responsible for the difference [27,125].\nTo clearly disentangle the impact of partisan identity or motivations versus prior beliefs, further experimental work is\nneeded – for example, studies that manipulate prior factual beliefs and/or political motivations [27,30]. Relatedly, under-\nstanding the origins of partisan differences in prior factual beliefs is also of the utmost importance. Exposure to different\ninformation streams is a promising candidate: entirely rational (e.g., Bayesian) and truth-seeking (i.e., nonpolitically\nmotivated) people who obtain their information from conservative (e.g., Fox News) versus liberal (e.g., MSNBC) news\nsources would naturally wind up with very different factual beliefs about the world. Evaluating new information in light of\nhow well it aligns with one's priors – although often referred to as 'confirmation bias' – is not, in fact, necessarily evidence\nof bias in a normative sense. When there is uncertainty about the reliability of information sources or data-generating pro-\ncesses, it can be entirely consistent with Bayesian inference to be skeptical of information that is inconsistent with one's\nprior factual beliefs [30,129]. In such cases, Bayesian agents may infer that the source or the data-generating process\nis unreliable rather than that their prior belief was incorrect – and doing so would not be evidence of bias per se (where bias\nis defined as deviating from some normative, e.g., Bayesian, benchmark) [130].\nFinally, the critique described in this box also applies to inferring 'motivated System 2 reasoning' from the observation that\ncognitive sophistication is sometimes (but not always or even frequently [131–133]) associated with polarization rather\nthan accuracy [17]. The association between cognitive sophistication and polarization (e.g., in the context of climate\nchange) disappears entirely once prior factual beliefs are accounted for [28]. Instead, it appears that more cognitively so-\nphisticated individuals may place more weight on their prior factual beliefs when evaluating new evidence, instead of plac-\ning more weight on concordance with their political identities. Of course, people's prior factual beliefs may themselves be\ncaused by politically motivated reasoning – but they need not be, and most study designs cannot determine (but must as-\nsume) a causal connection.\nTrends in Cognitive Sciences \nOPEN ACCESS\nTrends in Cognitive Sciences, May 2021, Vol. 25, No. 5 391\n\nengage in 'identity protective cognition' [16]. This account predicts that engaging in more\ndeliberation should lead to more politically polarized beliefs – and, most importantly, to greater\nbelief in politically concordant but false claims. Thus, more deliberation should be associated\nwith worse truth discernment.\nBy contrast, more 'classical' reasoning accounts (i.e., accounts that are more consistent with\nwork on dual-process reasoning in other domains) portray System 2 reasoning as being respon-\nsible for correcting faulty intuitions (Box 3). This perspective therefore predicts that people who\ndeliberate more will simply be less likely to believe false content – and are better able to discern\nbetween true and false content – regardless of the political concordance of the news that they\nare evaluating.\nAcross numerous recent studies, the evidence supports the classical reasoning account over the\nmotivated System 2 reasoning account. People who are more reflective (Box 3) are less likely to\nbelieve false news content – and are better at discerning between truth and falsehood – regardless\nof whether the news is consistent or inconsistent with their partisanship [20,26,34,35] (Figure 1A).\nThe same pattern is evident with respect to discernment between biased and misleading\nhyperpartisan news and true (mainstream) news [35] and when judging full news stories as\nopposed to just headlines [36], and using measures beyond the Cognitive Reflection Test\n(Figure 1), such as thinking disposition questionnaires [34] and the Berlin Numeracy Test [35]. Belief\nin fake news is also associated with delusionality [34], dogmatism [34], religious fundamentalism\n[34], bullshit receptivity [37], and overclaiming [37] (all factors associated with analytic thinking;\nBox 3). Furthermore, experimentally manipulating participants' level of deliberation demonstrates\na causal effect whereby deliberation reduces belief in false (but not true) news, regardless of\npartisan alignment (and has no effect on polarization) [26]. Research also shows that\noverconfidence may contribute to susceptibility to false information [38], perhaps because it\nstops people from slowing down and engaging in reflective reasoning [39,40].\nHow, then, do people determine news veracity? The correlation between cognitive reflection and\ndisbelief in fake news is stronger in cases where the content is more obviously implausible (and\nvice versa for true news) [20]. This suggests that, in cases where people actually do stop and\nthink, relevant prior knowledge is likely to be a critical factor. Indeed, political knowledge is\npositively associated truth discernment for political news content [23,41], as is media literacy\n[42] and general information literacy [43]. Similarly, basic science knowledge is positively asso-\nciated with truth discernment for (mis)information about COVID-19 [44]. This implies, unfortu-\nnately, that reasoning may not improve accuracy in contexts where prior knowledge is heavily\nBox 3. Dual-Process Models of Reasoning and the Consequences of Deliberation\nDual-process theories are a core component of research on the cognitive science of reasoning. These theories argue\nthat human cognition can be partitioned into two fundamentally different types of processes that differ in terms of their\ncharacteristics [40,134,135]: Type 1 (or System 1) processing that is characterized primarily by automaticity such that\nType 1 outputs ('intuitions') come to mind directly as a response to the stimulus, and Type 2 (or System 2) processing that\nis characterized by the deliberation that may or may not arise given a particular intuitive output (or set of outputs).\nConsider the following problem from the Cognitive Reflection Test [136]: 'A bat and ball cost $1.10 in total. The bat costs\n$1.00 more than the ball. How much does the ball cost?' The incorrect intuitive answer – 10 cents – comes to mind\nintuitively for most people, whereas the correct answer – 5 cents – emerges (for most) only with an additional deliberation\nprocess [137]. Performance on tasks such as the Cognitive Reflection Test are associated with a wide range of beliefs and\nbehaviors [138], including actual social media behaviors observed on Twitter ([71]; also [109]), as well as conspiracy idea-\ntion [139], politics [140], bullshit receptivity [12], and endorsement of a variety of epistemically suspect beliefs [138]. Impor-\ntantly, although dual-process theories typically emphasize the importance of overriding incorrect intuitions via analytic\nthinking, this should not be taken to imply that intuitions are always incorrect or that analytic thinking is always accurate [135].\nTrends in Cognitive Sciences\nOPEN ACCESS\n392 Trends in Cognitive Sciences, May 2021, Vol. 25, No. 5\n\ndistorted (e.g., by partisan media consumption or misinformation campaigns by political elites –\nclimate change being a prime example) [28] (Box 2).\nThus, when it comes to the role of reasoning, it seems that people fail to discern truth from false-\nhood because they do not stop to reflect sufficiently on their prior knowledge (or have insufficient\nor inaccurate prior knowledge) – and not because their reasoning abilities are hijacked by political\nmotivations.\nHeuristics\nPrior work in judgment and decision making [45] indicates that people are likely to use heuristics\nor mental shortcuts when judging news headlines. What, then, are the specific features of fake\nnews that influence people's intuitions or cause them to make mistakes when reasoning?\nOne key route to intuitive belief in news is familiarity [37]. The influence of prior exposure on\njudgments of truth – sometimes referred to as the illusory truth effect – is well documented\n[46,47]. Indeed, wartime rumors during WWII that were more familiar were more likely to\nbe believed [48]. Consistent with this, a single prior exposure to a fake news headline increases\nlater belief in the headline [49,50]. Remarkably, this is evident even if the headline is extremely\nimplausible (also [51]) and inconsistent with one's political partisanship [49]. Thus, feelings of famil-\niarity and, possibly, processing fluency per se ([52,53]; (but see [54]) likely contributes to increased\nbelief in false claims.\nThe source is another important cue that may be used when evaluating news. Participants are\nmore likely to believe information provided by people whom they view as being credible (reviewed\nin [55]), and a large literature from political science has robustly demonstrated the impact of elite\nmessaging, in particular, on public opinion [56]. For example, attributing a false claim to President\nTrump increased Trump supporters' belief in the claim while reducing Democrats' belief in the\nclaim [57]. Furthermore, social feedback provided by social media platforms (e.g., 'likes') also\nincreases belief in news content, particularly for misinformation [58] – a factor that is likely to\ncovary with elite messaging (given that political elites, such as Presidents and people in Congress\nor the media, often have many social media followers).\nFinally, a salient feature of fake news headlines also seems to be that they are often emotionally\nevocative. That is, fake news is often geared toward provoking shock, fear, anger [8,59], or (more\nbroadly) moral outrage [60]. This is important because people who report experiencing more emo-\ntion (positive or negative) at the outset of the task are more likely to believe false (but not true) news;\nand instructing people to rely on emotion increases belief in false (but not true) headlines [61].\nBelieving versus Sharing Fake News\nOne might expect that people share news on social media because they believe it is true.\nAccordingly, the widespread sharing of false content is often taken as evidence of widespread\nfalse beliefs [62,63]. However, recent work has shown that social media sharing judgments can\nactually be quite divergent from judgments about accuracy [21,44]. For example, participants\nwho were asked about the accuracy of a set of headlines rated true headlines as much more\naccurate than false headlines; but, when asked whether they would share the headlines, verac-\nity had little impact on sharing intentions – both in the context of political headlines [21]\n(Figure 2A) and headlines about COVID-19 [44]. As a result, sharing intentions for false head-\nlines were much higher than assessments of their truth (e.g., 91% higher in Figure 2A), indicat-\ning that many people were apparently willing to share content that they could have identified as\nbeing inaccurate.\nTrends in Cognitive Sciences \nOPEN ACCESS\nTrends in Cognitive Sciences, May 2021, Vol. 25, No. 5 393\n\nTo shed light on this disconnect between accuracy judgments and sharing intentions, a recent\nstudy examined the impact of asking participants to rate the perceived accuracy of each headline\nimmediately before deciding whether they would be willing to share it on social media [21]. This\nexperiment helps to distinguish between three distinct explanations for the accuracy–sharing\ndissociation. The confusion-based account posits that people genuinely (but mistakenly) believe\nthat the false claims they share are probably true. Consistent with this proposal, of the false\nheadlines that were shared in the baseline condition of [21], 33% were both believed and shared\nwhen participants were asked directly about accuracy – however, this leaves the remaining 67%\nof sharing unexplained by confusion.\nThe preference-based account is rooted in the idea that people place their preference for\npolitical identity (or related motives such as virtue signaling [64]) above the truth, and thus\nshare politically consistent false content on social media despite recognizing that it is probably\nnot true. This purposeful sharing could be motivated, for example, by an effort to further one's\npolitical agenda [18], to sow chaos [65], or to share news that would be interesting if it turned\nTrendsTrends inin CognitiveCognitive SciencesSciences\nFigure 2. Social Media Sharing Does\nNot Necessarily Imply Belief, and\nAccuracy Prompts Improve the\nQuality of Content That People\nShare. \n(A) Data from study 1 in [21]\nwhere US participants (n = 1002) from\nAmazon Mechanical Turk (MTurk) judged\npolitical headlines. Participants were\npresented with a set of headlines and\nwere either asked to indicate whether\nthey thought the headlines were accurate\n(accuracy condition) or whether they\nwould consider sharing them on social\nmedia (sharing condition). Although\nparticipants were much more likely to\nrate true headlines versus false headlines\nas being accurate in the accuracy\ncondition, headline veracity had little\nimpact on sharing intentions in the\nsharing condition (i.e., there was an\ninteraction between condition and\nveracity, b = −0.50, t(18089) = −15.37,\nP = <0.001). Error bars indicate\n95% confidence intervals. (B) Data\nfrom study 7 in [21] where Twitter users\n(n = 5379) who had recently shared links\nto websites that regularly produce\nmisleading and hyperpartisan content\nwere sent an unsolicited message asking\nthem to rate the accuracy of a single nonpolitical headline. The news sites to which users retweeted links in the 24 h after\nreceiving the message were compared with the links retweeted by participants who had not yet received the message\n(and the date of message delivery was randomized to allow causal inference). The x axis indicates the trust score given to\neach outlet by eight professional fact-checkers. The y axis indicates the fraction of rated links to each outlet in the 24 h\nafter the intervention minus the fraction of links to each outlet among not-yet-messaged users. The size of each dot is\nproportional to the number of premessaging posts with links to that outlet. Domains with >500 premessaging posts are\nlabeled. As can be seen, the message significantly improved the quality of new sources shared, with the change in\nrelative tweet frequency being strongly related to fact-checker ratings (domains weighted by number of pretreatment\nposts; r(52) = 0.74, P = <0.001). News sources: Breitbart; CNN, Cable News Network; DailyCaller; DailyMail, The Daily\nMail; DailyWire; FoxNews, Fox News Channel; Infowars; NYPost, The New York Post; NYTimes, The New York Times;\nWashPo, The Washington Post; Western Journal; WSJ, The Wall Street Journal.\nTrends in Cognitive Sciences\nOPEN ACCESS\n394 Trends in Cognitive Sciences, May 2021, Vol. 25, No. 5\n\nout to be true [66]. Of the false headlines that were shared in the baseline condition of [21],\n16% of the headlines were shared despite being identified as inaccurate. Thus, although pur-\nposeful sharing occurs, it seems unlikely to explain the bulk of false or misleading content that\nis shared online.\nFinally – and most consistent with the earlier focus on a lack of reflective thinking being a source of\nmisjudgments – the inattention-based account argues that people have a strong preference\nto only share accurate content, but that the social media context distracts them from this\npreference. Consistent with this account, asking participants to rate the accuracy of each head-\nline before deciding whether to share it decreased sharing of false headlines by 51% relative to the\nbaseline condition [21] – suggesting that inattention to accuracy was responsible for roughly half\nof the misinformation sharing in the experiment.\nIn a similar vein, work on social media behavior often emphasizes the importance of the\n'attention economy' where factors relating to engagement (likes, shares, comments, clicks,\netc.) are selected for in social media environments [8,60,67–69]. Accordingly, sharing of\nlow-quality news content on Facebook is associated with ideological extremity [70] and\nideological concordance is a much stronger predictor of sharing than it is of belief [21].\nFurthermore, analytic thinking is not only associated with more truth discernment, as de-\nscribed earlier, but is also associated with having more discerning sharing intentions in survey\nexperiments (both in terms of false [35,44] and hyperpartisan content [35]) and with the actual\nsharing of more reliable news outlets on Twitter [71]. One striking possibility is that the social\nmedia context itself distracts people from prioritizing the truth when they decide what to\nshare – and, in some cases, may actively promote antisocial behavior [72] and hamper the ex-\nercise of analytic and critical thinking. Social media may be both a cause and a consequence\nof increased political engagement, both good and bad [73].\nWhat Can Be Done? Interventions To Fight Fake News\nWe now turn to the implications of these findings for interventions intended to decrease the\nspread and impact of online misinformation.\nCurrent Approaches for Fighting Misinformation\nAs social media companies are, first and foremost, technology companies, a common\napproach is the automated detection of problematic news via machine learning, natural lan-\nguage processing, and network analysis [74–76]. Content classified as problematic is then\ndown-ranked by the ranking algorithm such that users are less likely to see it. However,\ncreating an effective misinformation classifier faces two fundamental challenges. First, truth is\nnot a black-and-white, clearly defined property: even professional fact-checkers often disagree\non how exactly to classify content [77,78]. Thus, it is difficult to decide what content and\nfeatures should be included in training sets, and artificial intelligence approaches run the risk\nof false positives and, therefore, of unjustified censorship [79]. Second, there is the problem\nof nonstationarity: misinformation content tends to evolve rapidly, and therefore the features\nwhich are effective at identifying misinformation today may not be effective tomorrow.\nConsider, for example, the rise of COVID-19 misinformation in 2020 – classifiers trained to\ndetect largely political content were likely unequipped to be effective for novel false and\nmisleading claims relating to health.\nAnother commonly used approach involves attaching warnings to content that\nprofessional fact-checkers have found to be false (reviewed in [80,81]). A great deal of ev-\nidence indicates that corrections and warnings do successfully reduce misperceptions\nTrends in Cognitive Sciences \nOPEN ACCESS\nTrends in Cognitive Sciences, May 2021, Vol. 25, No. 5 395\n\n[41,81–83] and sharing [49,84,85]. Despite some early evidence that correction checking\ncould backfire and increase belief in false content [86], recent work has shown that\nthese backfire effects are extremely uncommon and are not a cause for serious concern\n[87,88].\nThere are, however, other reasons to be cautious about the sufficiency of professional fact-checking.\nMost importantly, fact-checking is simply not scalable – it typically requires substantial time and effort\nto investigate whether a particular claim is false or misleading. Thus, many (if not most)\nfalse claims never get fact-checked. Even for those claims that do eventually get flagged, the\nprocess is often slow, such that warnings are likely to be absent during the claim's period of\npeak viral spreading. Furthermore, warnings are typically only attached to blatantly false\nnews, and not to extremely misleading or biased coverage of events that actually\noccurred. In addition to straightforwardly undermining the reach of fact-checks, this sparse\napplication of warnings could lead to an 'implied truth' effect where users may assume that\n(false or misleading) headlines without warnings have actually been verified [84]. Fact-checks\noften also fail to reach their intended audience [89], and may fade over time [90], provide\nincomplete protection against familiarity effects [49], and cause corrected users to\nsubsequently share more low-quality and partisan content [91].\nAnother potential approach that is commonly referenced is emphasizing the publishers of news\narticles, seeking to leverage the reliance on source cues described earlier. This, in theory, could\nbe effective because people (at least in the USA) are actually fairly good at distinguishing between\nlow- and high-quality publishers [92]. However, experimental evidence on emphasizing news\npublishers is not very encouraging: Numerous studies find that making source information\nmore salient (or removing it entirely) has little impact on whether people judge headlines to be\naccurate or inaccurate [37,93–97] (although see [98,99]).\nNew Approaches for Fighting Misinformation\nOne potentially promising alternative class of interventions involve a more proactive 'inoculation'\nor 'prebunking' against misinformation [8,100]. For example, the 'Bad News Game' uses a\n10–20 minute interactive tutorial to teach people how to identify fake news in an engaging way\n[101]. An important limitation of such approaches is that they are 'opt in' – that is, people have\nto actively choose to engage with the inoculation technique (often for a fairly substantial amount\nof time – at least in terms of the internet attention span [102]). This is particularly problematic\ngiven that those most in need of 'inoculation' against misinformation (e.g., people who are low\non cognitive refl\nection) may be the least likely to seek out and participate in lengthy inoculations.\nLighter-touch forms of inoculation that simply present people with information that helps them to\nidentify misinformation (e.g., in the context of climate change [103]) may be more scalable. For\nexample, presenting a simple list of 12 digital media literacy tips improved people's capacity to\ndiscern between true and false news in the USA and India [104].\nBoth fact-checking and inoculation approaches are fundamentally directed toward improving\npeople's underlying knowledge or skills. However, as noted earlier, recent evidence indicates\nthat misinformation may spread on social media not only because people are confused or\nlack the competency to recognize fake news, but also (or even mostly) because people fail\nto consider accuracy at all when they make choices about what to share online [21,44]. In\naddition, as mentioned, people who are more intuitive tend to be worse at distinguishing\nbetween true and false news content, both in terms of belief (Figure 1A) and sharing\n[35,71]. This work suggests that interventions aimed at getting people to slow down and\nTrends in Cognitive Sciences\nOPEN ACCESS\n396 Trends in Cognitive Sciences, May 2021, Vol. 25, No. 5\n\nreflect about the accuracy of what they see on social media may be effective in slowing the\nspread of misinformation.\nIndeed, recent research shows that a simple accuracy prompt – specifically, having partici-\npants rate the accuracy of a single politically neutral headline (ostensibly as part of a pre-\ntest) before making judgments about social media sharing – improves the extent to\nwhich people discern between true and false news content when deciding what to share\nonline in survey experiments [21,44]. This approach has also been successfully deployed\nin a large-scale field experiment on Twitter, in which messages asking users to rate the\naccuracy of a politically neutral news headline were sent to thousands of accounts who recently\nshared links to misinformation sites [21]. This subtle prompt significantly increased the quality of\nthe new they subsequently shared (Figure 2B). Furthermore, survey experiments have shown that\nasking participants to explain how they know whether a headline is true or false before sharing it\nincreases sharing discernment [105], and having participants rate accuracy at the time of encoding\nprotects against familiarity effects [106]. Relatedly, metacognitive prompts – probing questions that\nmake people reflect – increases resistance to inaccurate information [107].\nA major advantage of such accuracy prompts is that they are readily scalable. There are\nmany ways that social media companies, or other interested parties such as governments\nor civil society organizations, could shift people's attention to accuracy (e.g., through ads,\nby asking about the accuracy of content that is shared, or via public service announce-\nments, etc.). In addition to scalability, accuracy prompts also have the normative advantage\nof not relying on a centralized arbiter to determine truth versus falsehood. Instead, they\nleverage users' own (often latent) ability to make such determinations themselves, preserving\nuser autonomy. Naturally, this will not be effective for everyone all of the time, but it could have a\npositive effective in the aggregate as one of the various tools used to combat misinformation.\nFinally, platforms could also harness the power of human reasoning and the 'wisdom\nof crowds' to improve the performance of machine-learning approaches. While professional\nfact-checking is not easily scalable, it is much more tractable for platforms to have large\nnumbers of non-experts rate news content. Despite potential concerns about political bias\nor lack of knowledge, recent work has found high agreement between layperson crowds\nand fact-checkers when evaluating the trustworthiness of news publishers: the average\nDemocrat, Republican, and fact-checker all gave fake news and hyperpartisan sites very\nlow trust ratings [92] (Figure 3A). This remained true even when layperson raters were told\nthat their responses would influence social media ranking algorithms, creating an incentive to\n'game the system' [108]. However, these studies also revealed a weakness of publisher-based\ncrowd ratings: familiarity with a publisher was necessary (although not sufficient) for trust,\nmeaning that new or niche publishers are unfairly punished by such a rating scheme. One\nsolution to this problem is to have laypeople rate the accuracy of individual articles or\nheadlines (rather than publishers), and to then aggregate these item-level ratings to create\naverage scores for each publisher (Figure 3B). Furthermore, the layperson ratings of the\narticles themselves are also useful. Analyzing a set of headlines flagged for fact-checking\nby an internal Facebook algorithm found that the average layperson accuracy rating for fairly\nsmall crowds correlated equally well with that of professional fact-checkers as the fact-checkers\ncorrelated with each other [77]. Thus, using crowdsourcing to add a 'human in the loop' element\nto misinformation detection algorithms is promising.\nThese observations about the utility of layperson ratings have a strong synergy with the afore-\nmentioned idea of prompts that shift users' attention to accuracy: periodically asking social\nTrends in Cognitive Sciences \nOPEN ACCESS\nTrends in Cognitive Sciences, May 2021, Vol. 25, No. 5 397\n\nTrendsTrends inin CognitiveCognitive SciencesSciences\n(See figure legend at the bottom of the next page.)\nTrends in Cognitive Sciences\nOPEN ACCESS\n398 Trends in Cognitive Sciences, May 2021, Vol. 25, No. 5\n\nmedia users to rate the accuracy of random headlines both (i) shifts attention to accuracy and\nthus induces the users to be more discerning in their subsequent sharing, and (ii) generates useful\nratings to help inform ranking algorithms.\nConcluding Remarks\nThe spread of misinformation online presents both a scientific puzzle and a practical challenge.\nThe research we have synthesized here shows that the common narrative, whereby failing to\ndifferentiate false or misleading news from truth is a symptom of political polarization in a 'post-\ntruth' world, is not an appropriate characterization. Although people do preferentially believe\nnews that aligns with their politics, this occurs as much or more for true headlines compared\nwith false headlines – and thus people are actually more accurate, not less, when judging\nheadlines that are politically concordant. Rather than being bamboozled by partisanship, people\noften fail to discern truth from fiction because they fail to stop and reflect about the accuracy of\nwhat they see on social media. Accordingly, simple prompts that shift people's attention to\naccuracy increase the quality of news that people share on social media. Approaches of this na-\nture, including providing digital literacy tips, are not hindered by the same issues of scalability\nrelated to strict fact-checking approaches – and, in fact, can be combined with crowd-\nsourced fact-checking to maximize efficiency. Human reasoning, when applied appropriately,\ncan be a powerful salve against the lure of misinformation. Nonetheless, numerous important\nquestions remain unanswered (see Outstanding Questions).\nThere is also much to be gained for psychology and cognitive science per se from inves-\ntigating online misinformation. Fake news and other forms of online misinformation repre-\nsent a powerful (and ecologically valid) testing ground for evaluating theories from cognitive,\nsocial, and political psychology. More so than many other domains, when studying fake\nnews and misinformation it is often possible to combine traditional laboratory experiments\nwith large-scale social media data [71,109] – or even to conduct actual field experiments\non-platform [21,110,111]. Furthermore, these topics also motivate theory development\nby highlighting a new class of problems in need of further explanation: only a small fraction\nof online content draws sufficient attention and interest to be shared on social media. Under-\nstanding the psychology that underpins the dynamics of social media sharing is an important\nemerging subfield in psychology [21,67,112] (in addition to other areas such as computer\nscience [76], political science [113], communication [114], and public health [115], among\nothers [116]), driven in large part by concerns about misinformation. Tackling these questions\nOutstanding Questions\nWhat are the impacts of exposure to\nmisinformation? How does exposure\naffect belief in the specific claims made\nin the misinformation, more general\nattitudes (e.g., support for political\ncandidates or policies), and relevant\nbehaviors (e.g., vote choice or health\nbehaviors)? Do these effects differ\nfor misinformation versus reliable\nnews? How much misinformation\nmust be encountered to shift atti-\ntudes, beliefs, and behaviors?\nDoes the design of social media\nplatforms actively promote the spread\nof misinformation? For example, by\ninducing distraction and incentivizing\nor directing attention to factors other\nthan veracity?\nHow do times of crisis (e.g., the COVID-\n19 pandemic) affect susceptibility to\nmisinformation and the dynamics of\nsocial media behavior?\nTo what extent do findings about\nmisinformation and social media from\nthe USA and other Western countries\ngeneralize to other cultures and other\nsocial contexts (e.g., messaging apps\nsuch as WhatsApp)?\nWhat are the similarities and\ndifferences between the psychology\nof online misinformation relative to\nrelated constellations of beliefs, such\nas conspiracy ideation, political\nmisperceptions, rumors, and bullshit?\nHow does the psychology of\nonline misinformation compare with\nmisinformation distributed by traditional\nmedia such as television and talk radio,\nor by political elites?\nHow will findings about current forms\nof misinformation generalize in the\nface of the continually evolving nature\nof online content?\nWill social media corporations be willing\nto invest in, and implement, major\nchanges to fight misinformation, given\nthat their business models are premised\non maximizing engagement? How can\nadvocates and policy makers most\neffectively incentivize them to do so?\nHow can academics and social\nmedia platforms collaborate to study\nmisinformation, given the platforms\nFigure 3. The Wisdom of Crowds Can Help To Illuminate Information Quality. (A) Data from study 2 of [92] in which\nUS participants (n = 970) from Lucid indicated their level of trust in 60 news publishers. Average trust ratings for each source\namong Democrats (Dems; x axis) and Republicans (Reps; y axis) are shown. Although some partisan differences are evident\n(e.g., Republicans trust Fox News more than any other publisher), all hyperpartisan and fake news sites received low average\ntrust ratings from members of both parties. Thus, when creating a politically balanced layperson rating by averaging trust from\nDemocrats and Republicans, every mainstream outlet scores higher than every fake news or hyperpartisan outlet; in addition,\nthe layperson ratings were highly correlated with ratings of professional fact-checkers: r(58) = 0.90, P = <0.001. (B) Data from\nstudy 4 of [93] in which US participants (n = 1008) from Amazon Mechanical Turk (MTurk) provided accuracy ratings for the\nheadlines of the 10 most-popular articles from a range of fake news, hyperpartisan, and mainstream accounts. Shown is a\nlayperson outlet-quality score, constructed by averaging the accuracy ratings for the 10 headlines from each outlet, on\nthe y axis, plotted against the rating assigned to each outlet by NewsGuard, a company that hires professional journalists\nto conduct detailed investigations of the quality of news outlets, on the x axis. The layperson ratings are strongly corre-\nlated with the professional journalist ratings: r(44) = 0.79, P = <0.001. News source abbreviations are given in Figure 2\nlegend, additional abbreviations: ABC, American Broadcasting Company; AOL, Aol. (formerly America Online); BBC, Brit-\nish Broadcasting Corporation; bb4sb, BB4SP/Barracuda Brigade; BosGlobe, The Boston Globe; CBS, Columbia\nBroadcasting System (now CBS); ChiTrib, The Chicago Tribune; HuffPo, HuffPost (formerly Huffington Post); IJR, Inde-\npendent Journal Review; LATimes, The Los Angeles Times; MSNBC, from Microsoft plus National Broadcasting\nCompany; SFChronicle, The San Francisco Chronicle.\nTrends in Cognitive Sciences \nOPEN ACCESS\nTrends in Cognitive Sciences, May 2021, Vol. 25, No. 5 399\n\nwill give psychologists the opportunity to demonstrate the power and real-world impact of psycho-\nlogical theory.\nReferences\n1. Lazer, D. et al. (2018) The science of fake news. Science 359,\n1094–1096\n2. Wardle, C. (2018) Information Disorder: The Essential Glossary,\nShorenstein Center on Media, Politics, and Public Policy,\nHarvard Kennedy School\n3. Loomba, S. et al. (2021) Measuring the impact of COVID-19\nvaccine misinformation on vaccination intent in the UK and\nUSA. Nat. Hum. Behav. Published online February 5, 2021.\nhttps://doi.org/10.1038/s41562-021-01056-1\n4. Pennycook, G. and Rand, D.G. (2021) Examining false beliefs\nabout voter fraud in the wake of the 2020 Presidential Election.\nHarvard Kennedy Sch. Misinformation Rev. 2, 1–19\n5. Kaplan, R.L. (2008) Yellow journalism. In The International\nEncyclopedia of Communication (Donsbach, W., ed.), John\nWiley & Sons\n6. Faris, R.M. et al. (2017) Partisanship, Propaganda, and\nDisinformation: Online Media and the 2016 U.S. Presidential\nElection, Berkman Klein Center for Internet and Society\n7. Lewandowsky, S. et al. (2020) Technology and Democracy,\nUnderstanding the Influence of Online Technologies on\nPolitical Behaviour and Decision-Making, EU Science Hub\n8. Kozyreva, A. et al. (2020) Citizens versus the internet:\nconfronting digital challenges with cognitive tools. Psychol.\nSci. Public Interest 21, 103–156\n9. Sunstein, C.R. and Vermeule, A. (2009) Conspiracy theories:\ncauses and cures. J. Polit. Philos. 17, 202–227\n10. Lindeman, M. and Aarnio, K. (2007) Superstitious, magical,\nand paranormal beliefs: An integrative model. J. Res. Pers.\n41, 731–744\n11. Berinsky, A.J. (2017) Rumors and health care reform: experiments\nin political misinformation. Br. J. Polit. Sci. 47, 241–262\n12. Pennycook, G. et al. (2015) On the reception and detection of\npseudo-profound bullshit. Judgm. Decis. Mak. 10, 549–563\n13. Amazeen, M.A. et al. (2018) Correcting political and consumer\nmisperceptions: the effectiveness and effects of rating scale\nversus contextual correction formats. J. Mass Commun. Q.\n95, 28–48\n14. Wickens, T. (2002) Elementary Signal Detection Theory, Oxford\nUniversity Press\n15. Batailler, C. et al. A signal detection approach to understanding\nthe identification of fake news. Perspect. Psychol. Sci. (in press)\n16. Kahan, D.M. (2017) Misconceptions, misinformation, and the\nlogic of identity-protective cognition. SSRN Electron. J. Cultural\nCognition Project Working Paper Series No. 164, Yale Law\nSchool, Public Law Research Paper No. 605, Yale Law &\nEconomics Research Paper No. 575. Published online May\n24, 2017. http://dx.doi.org/10.2139/ssrn.2973067\n17. Kahan, D.M. (2013) Ideology, motivated reasoning, and cogni-\ntive reflection. Judgm. Decis. Mak. 8, 407–424\n18. Van Bavel, J.J. and Pereira, A. (2018) The partisan brain: an Identity-\nbased model of political belief. Trends Cogn. Sci. 22, 213–224\n19. Allcott, H. and Gentzkow, M. (2017) Social media and fake\nnews in the 2016 election. J. Econ. Perspect. 31, 211–236\n20. Pennycook, G. and Rand, D.G. (2019) Lazy, not biased:\nsusceptibility to partisan fake news is better explained by lack\nof reasoning than by motivated reasoning. Cognition 188, 39–50\n21. Pennycook, G. et al. (2021) Shifting attention to accuracy can\nreduce misinformation online. Nature http://dx.doi.org/\n10.1038/s41586-021-03344-2\n22. Pereira, A. et al. (2020) Identity concerns drive belief: the\nimpact of partisan identity on the belief and dissemination of\ntrue and false news. PsyArXiv Published online September\n18, 2018. http://dx.doi.org/10.31234/OSF.IO/7VC5D\n23. Vegetti, F. and Mancosu, M. (2020) The impact of political\nsophistication and motivated reasoning on misinformation.\nPolit. Commun. 37, 678–695\n24. Faragó, L. et al. (2020) We only believe in news that we\ndoctored ourselves: the connection between partisanship\nand political fake news. Soc. Psychol. (Gott). 51, 77–90\n25. Drummond, C. et al. (2020) Limited effects of exposure to fake\nnews about climate change. Environ. Res. Commun. 2, 081003\n26. Bago, B. et al. (2020) Fake news, fast and slow: deliberation\nreduces belief in false (but not true) news headlines. J. Exp.\nPsychol. Gen. \n149, 1608–1613\n27. Tappin, B.M. et al. (2020) Thinking clearly about causal inferences\nof politically motivated reasoning: why paradigmatic study\ndesigns often undermine causal inference. Curr. Opin. Behav.\nSci. 34, 81–87\n28. Tappin, B.M. et al. (2020) Rethinking the link between cognitive\nsophistication and politically motivated reasoning. J. Exp. Psychol.\nGen. Published online October 29, 2020. http://dx.doi.org/\n10.31234/OSF.IO/YUZFJ\n29. Tappin, B.M. et al. (2020) Bayesian or biased? Analytic thinking\nand political belief updating. Cognition 204, 1–12\n30. Baron, J. and Jost, J.T. (2019) False equivalence: are liberals\nand conservatives in the United States equally biased?\nPerspect. Psychol. Sci. 14, 292–303\n31. Gerber, A. and Green, D. (1999) Misperceptions about\nperceptual bias. Annu. Rev. Polit. Sci. 2, 189–210\n32. Leeper, T.J. and Slothuus, R. (2014) Political parties, motivated rea-\nsoning, and public opinion formation. Polit. Psychol. 35, 129–156\n33. Friedman, J. (2012) Motivated skepticism or inevitable conviction?\nDogmatism and the study of politics. Crit. Rev. 24, 131–155\n34. Bronstein, M.V. et al. (2019) Belief in fake news is associated with\ndelusionality, dogmatism, religious fundamentalism, and reduced\nanalytic thinking. J. Appl. Res. Mem. Cogn. 8, 108–117\n35. Ross, R.M. et al. (2019) Beyond 'fake news': the analytic think-\ning and the detection of false and hyperpartisan news head-\nlines. PsyArXiv Published online November 13, 2019. http://\ndx.doi.org/10.31234/osf.io/cgsx6\n36. Pehlivanoglu, D. et al. (2020) The role of analytical reasoning\nand source credibility on the evaluation of real and fake full-\nlength news articles. PsyArXiv Published online December 2,\n2020. https://doi.org/10.31234/osf.io/98zwc\n37. Pennycook, G. and Rand, D.G. (2019) Who falls for fake news?\nThe roles of bullshit receptivity, overclaiming, familiarity, and\nanalytic thinking. J. Pers. 88, 185–200\n38. Salovich, N.A. et al. (2020) Can confidence help account for\nand redress the effects of reading inaccurate information?\nMem. Cogn. 49, 293–310\n39. Thompson, V.A. et al. (2011) Intuition, reason, and metacognition.\nCogn. Psychol. 63, 107–140\n40. Pennycook, G. et al. (2015) What makes us think? A three-\nstage dual-process model of analytic engagement. Cogn.\nPsychol. 80, 34–72\n41. Brashier, N.M. et al. (2021) Timing matters when correcting\nfake news. Proc. Natl. Acad. Sci. U. S. A. 118, 1–3\n42. Amazeen, M.A. and Bucy, E.P. (2019) Conferring resistance to\ndigital disinformation: the inoculating influence of procedural\nnews knowledge. J. Broadcast. Electron. Media 63, 415–432\n43. Jones-Jang, S.M. et al. (2019) Does media literacy help identi-\nfication of fake news? Information literacy helps, but other\nliteracies don't. Am. Behav. 65, 371–388\n44. Pennycook, G. et al. (2020) Fighting COVID-19 misinformation\non social media: experimental evidence for a scalable accuracy\nnudge intervention. Psychol. Sci. 31, 770–780\n45. Kahneman, D. et al. (1982) Judgments under Uncertainty:\nHeuristics and Biases, Cambridge University Press\n46. Hasher, L. et al. (1977) Frequency and the conference of referential\nvalidity. J. Verbal Learn. Verbal Behav. 16, 107–112\n47. Dechene, A. et al. (2010) The truth about the truth: a meta-analytic\nreview of the truth effect. Personal. Soc. Psychol. Rev. 14, 238–257\n48. Allport, F.H. and Lepkin, M. (1945) Wartime rumors of waste\nand special privilege: why some people believe them.\nJ. Abnorm. Soc. Psychol. 40, 3–36\n49. Pennycook, G. et al. (2018) Prior exposure increases\nperceived accuracy of fake news. J. Exp. Psychol. Gen.\n147, 1865\n–1880\nnear-total control of relevant data, and\nwidespread concerns about privacy?\nTrends in Cognitive Sciences\nOPEN ACCESS\n400 Trends in Cognitive Sciences, May 2021, Vol. 25, No. 5\n\n50. Smelter, T.J. and Calvillo, D.P. (2020) Pictures and repeated\nexposure increase perceived accuracy of news headlines.\nAppl. Cogn. Psychol. 34, 1061–1071\n51. Fazio, L.K. et al. (2019) Repetition increases perceived truth\nequally for plausible and implausible statements. Psychon.\nBull. Rev. 26, 1705–1710\n52. Unkelbach, C. et al. (2019) Truth by repetition: explanations\nand implications. Curr. Dir. Psychol. Sci. 28, 247–253\n53. Wang, W-C. et al. (2016) On known unknowns: fluency and the\nneural mechanisms of illusory truth. J. Cogn. Neurosci. 28,\n739–746\n54. Corneille, O. et al. (2020) Repetition increases both the\nperceived truth and fakeness of information: an ecological\naccount. Cognition 205, 104470\n55. Pornpitakpan, C. (2004) The persuasiveness of source credibility:\na critical review of five decades' evidence. J. Appl. Soc. Psychol.\n34, 243–281\n56. Zaller, J.R. (1992) The Nature and Origins of Mass Opinion,\nCambridge University Press\n57. Swire, B. et al. (2017) Processing political misinformation:\ncomprehending the Trump phenomenon. R. Soc. Open Sci.\n4, 160802\n58. Avram, M. et al. (2020) Exposure to social engagement metrics\nincreases vulnerability to misinformation. Harvard Kennedy\nSch. Misinformation Rev. 1, 1–11\n59. Quandt, T. (2018) Dark participation. Media Commun. 6, 36–48\n60. Crockett, M.J. (2017) Moral outrage in the digital age. Nat.\nHum. Behav. 1, 769–771\n61. Martel, C. et al. (2020) Reliance on emotion promotes belief in\nfake news. Cogn. Res. Princ. Implic. 5, 1–20\n62. Chatfi\neld, T. (2019) Why we believe fake news. BBC Future\nPublished online September 9, 2019. http://www.bbc.com/\nfuture/story/20190905-how-our-brains-get-overloaded-by-\nthe-21st-century\n63. Steinmetz, K. (2018) How your brain tricks you into believing fake\nnews. Time Published online August 9, 2019. https://time.com/\n5362183/the-real-fake-news-crisis/\n64. Jordan, J.J. and Rand, D.G. (2020) Signaling when no one is\nwatching: a reputation heuristics account of outrage and\npunishment in one-shot anonymous interactions. J. Pers.\nSoc. Psychol. 118, 57–88\n65. Petersen, M.B. et al. (2018) A 'need for chaos' and the sharing\nof hostile political rumors in advanced democracies. PsyArXiv\nPublished online September 1, 2018. http://dx.doi.org/\n10.31234/osf.io/6m4ts\n66. Altay, S. et al. (2020) 'If this account is true, it is most enor-\nmously wonderful': Interestingness-if-true and the sharing of\ntrue and false news. PsyArXiv Published online June 4, 2020.\nhttp://dx.doi.org/10.31234/osf.io/tdfh5\n67. Brady, W.J. et al. (2020) The MAD model of moral contagion: the\nrole of motivation, attention and design in the spread of moralized\ncontent online. Perspect. Psychol. Sci. 15, 978–1010\n68. Baek, E.C. et al. (2017) The value of sharing information: a\nneural account of information transmission. Psychol. Sci. 28,\n851–861\n69. Scholz, C. et al. (2017) A neural model of valuation and infor-\nmation virality. Proc. Natl. Acad. Sci. U. S. A. 114, 2881–2886\n70. Hopp, T. et al. (2020) Why do people share ideologically\nextreme, false, and misleading content on social media? A self-\nreport and trace data-based analysis of countermedia content\ndissemination on Facebook and Twitter. Hum. Commun. Res.\n46, 357–384\n71. Mosleh, M. et al. (2021) Cognitive reflection correlates with\nbehavior on Twitter. Nat. Commun. 12, 1–10\n72. Effron, D.A. and Raj, M. (2019) Misinformation and morality:\nencountering fake-news headlines makes them seem less\nunethical to publish and share. Psychol. Sci. 31, 75–87\n73. Valenzuela, S. et al. (2019) The paradox of participation versus\nmisinformation: social media, political engagement, and the\nspread of misinformation. Digit. J. \n7, 802–823\n74. Shu, K. et al. (2017) Fake news detection on social media.\nACM SIGKDD Explor. Newsl. 19, 22–36\n75. Monti, F. et al. (2019) Fake news detection on social media\nusing geometric deep learning. ArXiv Published online February\n10, 2019. http://arxiv.org/abs/1902.06673\n76. Zhou, X. et al. (2019) Fake news: fundamental theories, detection\nstrategies and challenges. In WSDM 2019: Proceedings of the\nTwelfth ACM International Conference on Web Search and Data\nMining, pp. 836–837, Association for Computing Machinery, Inc\n77. Allen, J. et al. (2020) Scaling up fact-checking using the\nwisdom of crowds. PsyArXiv Published online October 2,\n2020. http://dx.doi.org/10.31234/osf.io/9qdza\n78. Lim, C. (2018) Checking how fact-checkers check. Res. Polit.\n5 205316801878684\n79. Woolley, S. (2020) We're fighting fake news AI bots by using\nmore AI. That's a mistake. MIT Technol. Rev. Published online\nJanuary 8, 2020. https://www.technologyreview.com/2020/\n01/08/130983/were-fighting-fake-news-ai-bots-by-using-\nmore-ai-thats-a-mistake/\n80. Walter, N. et al. (2020) Fact-checking: a meta-analysis of what\nworks and for whom. Polit. Commun. 37, 350–375\n81. Nieminen, S. and Rapeli, L. (2019) Fighting misperceptions\nand doubting journalists' objectivity: a review of fact-checking\nliterature. Polit. Stud. Rev. 17, 296–309\n82. Nyhan, B. et al. (2019) Taking fact-checks literally but not\nseriously? The effects of journalistic fact-checking on factual\nbeliefs and candidate favorability. Polit. Behav. 42, 939–960\n83. Ecker, U.K.H. et al. (2019) The effectiveness of short-format\nrefutational fact-checks. Br. J. Psychol. 111, 36–54\n84. Pennycook, G. et al. (2020) The implied truth effect: attaching\nwarnings to a subset of fake news stories increases perceived\naccuracy of stories without warnings. Manag. Sci. 66,\n4944–4957\n85. \nYaqub, W. et al. (2020) Effects of credibility indicators on social\nmedia news sharing intent. In Proceedings of the 2020 CHI\nConference on Human Factors in Computing Systems (vol. 20),\npp. 1–14, Association for Computing Machinery, Inc\n86. Nyhan, B. and Reifler, J. (2010) When corrections fail: the\npersistence of political misperceptions. Polit. Behav. 32,\n303–330\n87. Wood, T. and Porter, E. (2019) The elusive backfire effect:\nmass attitudes' steadfast factual adherence. Polit. Behav. 41,\n135–163\n88. Guess, A.M. and Coppock, A. (2018) Does counter-attitudinal\ninformation cause backlash? Results from three large survey\nexperiments. Br. J. Polit. Sci. 50, 1497-1151\n89. Guess, A.M. et al. (2020) Exposure to untrustworthy websites\nin the 2016 US election. Nat. Hum. Behav. 4, 472–480\n90. Swire, B. et al. (2017) The role of familiarity in correcting inaccurate\ninformation. J. Exp. Psychol. Learn. Mem. Cogn. 43, 1948–1961\n91. Mosleh, M. et al. (2021) Perverse consequences of debunking\nin a Twitter field experiment: being corrected for posting false\nnews increases subsequent sharing of low quality, partisan,\nand toxic content. In Proceedings of the 2021 CHI Conference\nof Human Factors in Computing Systems, http://dx.doi.org/\n10.1145/3411764.3445642\n92. Pennycook, G. and Rand, D.G. (2019) Fighting misinformation\non social media using crowdsourced judgments of news\nsource quality. Proc. Natl. Acad. Sci. U. S. A. 116, 2521–2526\n93. Dias, N. et al. (2020) Emphasizing publishers does not effec-\ntively reduce susceptibility to misinformation on social media.\nHarvard Kennedy Sch. Misinformation Rev. 1, 1–12\n94. Austin, E.W. and Dong, Q. (1994) Source v. content effects on judg-\nments of news believability. J. Mass Commun. Q. 71, 973–983\n95. Jakesch, M. et al. (2019) The role of source, headline and\nexpressive responding in political news evaluation. SSRN\nElectron. Published online January 8, 2019. https://doi.org/\n10.2139/ssrn.3306403\n96. Shen, C. \net al. (2019) Fake images: the effects of source, inter-\nmediary, and digital media literacy on contextual assessment of\nimage credibility online. New Media Soc. 21, 438–463\n97. Tsang, S.J. (2020) Motivated fake news perception: the impact of\nnews sources and policy support on audiences' assessment of\nnews fakeness. J. Mass Commun. Q. Published online August 31,\n2020. https://doi.org/10.1177/1077699020952129\n98. Kim, A. et al. (2019) Combating fake news on social media\nwith source ratings: the effects of user and expert reputation\nratings. J. Manag. Inf. Syst. 36, 931–968\n99. Nadarevic, L. et al. (2020) Perceived truth of statements and\nsimulated social media postings: an experimental investigation\nTrends in Cognitive Sciences \nOPEN ACCESS\nTrends in Cognitive Sciences, May 2021, Vol. 25, No. 5 401\n\nof source credibility, repeated exposure, and presentation\nformat. Cogn. Res. Princ. Implic. 5, 56\n100. Compton, J. (2013) Inoculation theory. In The SAGE Handbook\nof Persuasion: Developments in Theory and Practice (Dillard,\nJ.P. and Shen, L., eds), pp. 220–236, SAGE Publications, Inc\n101. Roozenbeek, J. and van der Linden, S. (2019) Fake news game\nconfers psychological resistance against online misinformation.\nPalgrave Commun. 5, 65\n102. Lorenz-Spreen, P. et al. (2019) Accelerating dynamics of\ncollective attention. Nat. Commun. 10, 1759\n103. Cook, J. et al. (2017) Neutralizing misinformation through inocula-\ntion: exposing misleading argumentation techniques reduces\ntheir influence. PLoS One 12, e0175799\n104. Guess, A.M. et al. (2020) A digital media literacy intervention\nincreases discernment between mainstream and false news\nin the United States and India. Proc. Natl. Acad. Sci. U. S. A.\n117, 15536–15545\n105. Fazio, L. (2020) Pausing to consider why a headline is true or\nfalse can help reduce the sharing of false news. Harvard\nKennedy Sch. Misinformation Rev. 1, 1–8\n106. Brashier, N.M. et al. (2020) An initial accuracy focus prevents\nillusory truth. Cognition 194, 104054\n107. Salovich, N.A. and Rapp, D.N. (2020) Misinformed and unaware?\nMetacognition and the influence of inaccurate information. J. Exp.\nPsychol. Learn. Mem. Cogn. Published online November 5,\n2020. https://psycnet.apa.org/doi/10.1037/xlm0000977\n108. Epstein, Z. et al. (2020) Will the crowd game the algorithm? Using\nLayperson Judgments To Combat Misinformation On Social\nMedia By Downranking Distrusted Sources. In Proceedings of\nthe 2020 CHI Conference on Human Factors in Computing\nSystems\n109. Carpenter, J. et al. (2018) The impact of actively open-minded\nthinking on social media communication. Judgm. Decis. Mak.\n13, 562–574\n110. Munger, K. (2017) Tweetment effects on the Tweeted: experi-\nmentally reducing racist harassment. Polit. Behav. 39, 629–649\n111. Mosleh, M. et al. (2021) Shared partisanship dramatically\nincreases social tie formation in a Twitter field experiment.\nProc. Natl. Acad. Sci. U. S. A. 118, e2022761118\n112. Brady, W.J. et al. (2020) Attentional capture helps explain why\nmoral and emotional content go viral. J. Exp. Psychol. Gen.\n149, 746–756\n113. Tucker, J. et al. (2018) Social media, political polarization, and\npolitical disinformation: a review of the scientific literature. SSRN\nElectron. J. Published online March 21, 2018. https://doi.org/\n10.2139/ssrn.3144139\n114. Li, J. (2020) Toward a research agenda on political misinformation\nand corrective information. Polit. Commun. 37, 125–135\n115. Wang, Y. et al. (2019) Systematic literature review on the\nspread of health-related misinformation on social media. Soc.\nSci. Med. 240, 112552\n116. Paletz, S.B.F. et al. (2019) A Multidisciplinary Framework of\nInformation Propagation Online, Springer\n117. Grinberg, N. et al. (2019) Fake news on twitter during the 2016\nU.S. Presidential election. Science 363, 374–378\n118. Guess, A.M. et al. (2019) Less than you think: prevalence and\npredictors of fake news dissemination on Facebook. Sci. Adv.\n5, eaau4586\n119. Bradshaw, S. et al. (2020) Sourcing and automation of political\nnews and information over social media in the United States,\n2016–2018. Polit. Commun. 37, 173–193\n120. Rogers, R. (2020) The scale of Facebook's problem depends\nupon how 'fake news' is classified. Harvard Kennedy Sch.\nMisinformation Rev. 1, 1–15\n121. Halpern, D. et al. (2019) From belief in conspiracy theories to\ntrust in others: which factors influence exposure, believing\nand sharing fake news. In Social Computing and Social\nMedia. Design, Human Behavior and Analytics. HCII 2019.\nLecture Notes in Computer Science (vol 11578) (Meiselwitz,\nG., ed.), pp. 217–232, Springer, Cham\n122. Zimmermann, F. and Kohring, M. (2020) Mistrust,\ndisinforming news, and vote choice: a panel survey on the\norigins and consequences of believing disinformation in\nthe 2017 German Parliamentary Election. \nPolit. Commun.\n37, 215–237\n123. Ditto, P.H. et al. (2019) At least bias is bipartisan: a meta-analytic\ncomparison of partisan bias in liberals and conservatives.\nPerspect. Psychol. Sci. 14, 273–291\n124. Clark, C.J. and Winegard, B.M. (2020) Tribalism in war and\npeace: the nature and evolution of ideological epistemology\nand its significance for modern social science. Psychol. Inq.\n31, 1–22\n125. Druckman, J.N. and McGrath, M.C. (2019) The evidence for\nmotivated reasoning in climate change preference formation.\nNat. Clim. Chang. 9, 111–119\n126. Kahan, D.M. et al. (2012) The polarizing impact of science\nliteracy and numeracy on perceived climate change risks.\nNat. Clim. Chang. 2, 732–735\n127. Evans, J.S.B.T. et al. (1983) On the conflict between logic and\nbelief in syllogistic reasoning. Mem. Cogn. 11, 295–306\n128. Bago, B. et al. (2020) Reasoning about climate change. PsyArXiv\nPublished online December 13, 2020. https://doi.org/10.31234/\nosf.io/vcpkb\n129. Koehler, J.J. (1993) The influence of prior beliefs on scientific\njudgments of evidence quality. Organ. Behav. Hum. Decis.\nProcess. 56, 28–55\n130. Hahn, U. and Harris, A.J.L. (2014) What Does It Mean to be\nBiased. Motivated Reasoning and Rationality. In Psychology\nof Learning and Motivation – Advances in Research and Theory\n(vol. 61), pp. 41–102, Academic Press\n131. McPhetres, J. and Pennycook, G. (2020) Science beliefs,\npolitical ideology, and cognitive sophistication. OSF Preprints\nPublished online November 19, 2019. https://doi.org/\n10.31219/osf.io/ad9v7\n132. Pennycook, G. et al. (2020) On the belief that beliefs should\nchange according to evidence: Implications for conspiratorial,\nmoral, paranormal, political, religious, and science beliefs.\nJudgm. Decis. Mak. 15, 476–498\n133. Bergan, D.E. (2021) Does ability contribute to partisan bias?\nEvaluating scientific research about political topics. Commun.\nStud. Published online February 4, 2021. https://doi.org/\n10.1080/10510974.2021.1876124\n134. De Neys, W. (2017) Dual Process Theory 2.0, Routledge\n135. Evans, J.S.B.T. and Stanovich, K.E. (2013) Dual-process theo-\nries of higher cognition: Advancing the debate. Perspect.\nPsychol. Sci. 8, 223–241\n136. Frederick, S. (2005) Cognitive reflection and decision making.\nJ. Econ. Perspect. 19, 25–42\n137. Toplak, M.E. et al. (2011) The cognitive reflection test as a pre-\ndictor of performance on heuristics-and-biases tasks. Mem.\nCogn. 39, 1275–1289\n138. Pennycook, G. et al. (2015) Everyday consequences of analytic\nthinking. Curr. Dir. Psychol. Sci. 24, 425–432\n139. Swami, V. et al. (2014) Analytic thinking reduces belief in con-\nspiracy theories. Cognition 133, 572–585\n140. Pennycook, G. and Rand, D.G. (2019) Cognitive reflection and\nthe 2016 U.S. presidential election. Pers. Soc. Psychol. Bull.\n45, 224–239\nTrends in Cognitive Sciences\nOPEN ACCESS\n402 Trends in Cognitive Sciences, May 2021, Vol. 25, No. 5"
    }
  ],
  "files": [
    {
      "file_name": "0309Lazer_DRAFT.pdf"
    },
    {
      "file_name": "Fake_news_as_a_social_phenomenon_in_the.pdf"
    },
    {
      "file_name": "FakeNewsEEG-Peoplewillbelieve.pdf"
    },
    {
      "file_name": "The-Psychology-of-Fake-News_2021_tics.pdf"
    }
  ]
}

## 1

{
  "uuid": "25a579cf-6f78-49ae-91d3-35e3649c4465",
  "text": "Here is a draft outline for a research paper on \"Sociological aspects of Fake News in Current Affairs\":\n\nIntroduction\n\n- State the problem: Rise of fake news phenomenon and its social impact. \n\n- Contextualize the problem: Proliferation of misinformation in digital age, heightened concerns about veracity of news and information.\n\n- Objective: To examine the sociological factors and dynamics involved in the spread of fake news as a social phenomenon. Identify vulnerabilities in information ecosystem.  \n\nLiterature Review \n\n- Evolution of news and information flows in digital age\n    - New modes of news production and distribution\n    - Changing dynamics between media, politics, technology  \n\n- Defining fake news and related concepts\n    - Misinformation, disinformation, propaganda\n    - Elements that characterize fake news \n\n- Major sociological perspectives \n    - Critical theory: Power structures exploited through misinformation\n    - Symbolic interactionism: Construction of social reality through discourse\n    - Systems theory: Information disorders reflect broader systemic issues\n\n- Prior sociological research \n    - Studies on social media networks as echo chambers  \n    - Role of algorithms and economic interests\n    - Analyses of connections between fake news and social polarization\n\nMethodology\n\n- Mixed methods research \n    - Qualitative study: Interviews with media professionals on industry changes\n    - Quantitative study: Surveys on news consumption patterns and attitudes \n    - Social network analysis: Mapping fake news flows and political discourse \n\n- Data sources\n    - Content analysis of news articles  \n    - Social media metrics and web archives\n    - Government and industry reports\n    \n- Analytical techniques \n    - Statistical analysis \n    - Discourse analysis\n    - Modeling influence and credibility perceptions\n\nExpected Results\n\n- Present integrated understanding of socio-technical factors enabling fake news\n- Highlight vulnerabilities created by information ecosystem characteristics   \n- Develop predictive model to assess impact of fake news\n- Derive policy insights to curb misinformation spread\n\nConclusions\n\n- Summarize key findings\n- Note limitations and future research directions\n- Recommend interventions based on sociological perspective\n  \nBibliography\n\n- List of primary and secondary sources",
  "sender": "assistant",
  "created_at": "2023-11-14T20:20:59.582693Z",
  "updated_at": "2023-11-14T20:20:59.582693Z",
  "attachments": [],
  "files": []
}

## 2

{
  "uuid": "74e0c367-fed0-4a82-839e-d4254d16bc87",
  "text": "using this outline, the source documents and anything relevant online you have access to, please write a 15 page academic papers with the title \"Sociological aspects of Fake News in Current Affairs\"",
  "sender": "human",
  "created_at": "2023-11-14T20:34:41.834903Z",
  "updated_at": "2023-11-14T20:34:41.834903Z",
  "attachments": [],
  "files": []
}

## 3

{
  "uuid": "7d60bcf0-3366-48c5-9ddb-a2c437f3eccf",
  "text": "Here is a 15 page academic paper on \"Sociological aspects of Fake News in Current Affairs\":\n\nSociological aspects of Fake News in Current Affairs\n\nIntroduction\n\nThe rise of fake news and the proliferation of misinformation have become pressing concerns in the contemporary information ecosystem. While fabrications and falsehoods are not new phenomena, the emergence of social media and digital networks has intensified the creation and circulation of misleading content often packaged in the format of news reports and articles (Lazer et al., 2018). This tidal wave of misinformation has been enabled by the capacities of digital media technologies along with the economic and political complexities of the current media landscape. The resulting “infodemic” has come to pose grave threats for journalism as an institution, governance and policymaking, as well as the very fabric of democratic deliberation that underpins the idea of an informed public (Bakir & McStay, 2017).  \n\nGiven the intricate social dynamics involved, a multidisciplinary effort is required to comprehend and respond to the challenges of fake news and the post-truth condition. Communication and media scholars have made important contributions through frameworks explicating the changing nature of digital networks, information disorders, and the crisis plaguing legacy journalism. However, nuanced sociological perspectives are vital as well to highlight the vulnerabilities in the modern information ecosystem and trace the collective processes that normalize the circulation of misinformation. As C.W. Mills (1959) emphasizes, sociological imagination enables linking individual troubles to broader public issues by locating personal experiences in their sociohistorical contexts. Adopting this orientation, the current paper aims to elucidate the sociology of fake news and unpack the social forces and configurations that have facilitated the rise of misinformation across digital cultures. \n\nBackground\n\nFundamentally, the contemporary fake news phenomenon indicates a crisis in practices of knowledge production, gatekeeping, and verification that once preserved the reliability of information. News, as a validated form of knowledge, maintains its authority through disciplinary norms that uphold principles of truthfulness and objectivity (Hallin, 1994). This disciplinary matrix breaks down in case of fake news, which mimics the aesthetic form of news articles but is divorced from journalistic commitments to factual accuracy. Instead, fake news exaggerates, distorts, or wholly fabricates information to deliberately advance certain ideological agendas and influence beliefs (Mantzarlis, 2018). It propagates false narratives that feed into a fragmented and polarized climate of opinions, often intensifying existing social divisions.\n\nWhile elements of propaganda, misinformation, and yellow journalism have shadowed the news industry historically, the current infodemic of fake news is traced to the ascendance of social media, declining public trust in established media, increased political polarization, and the economic troubles facing journalism organizations (Carlson, 2017; Coleman, 2018). This combination of technological, political, economic and cultural factors constitutes the broader ecology that sociological analysis seeks to unpack in relation to fake news. In particular, the socially constructed nature of ‘news’ itself must be highlighted to appreciate what makes the phenomenon of fake news socially disruptive. \n\nTheoretical Groundings\n\nSociological insights draw attention to the institutional moorings of concepts like credibility, facticity, and journalistic authority (Schudson, 2001). Facts do not emerge in isolation but are produced through disciplinary frameworks that distinguish truth claims from falsehood. News, in this sense, is not a naturally occurring category but a stabilized construct rooted in professional norms and gatekeeping practices (Becker & Vlad, 2009). When these supports erode, the lines demarcating news from non-news contents become obscured. Three sociological lenses illuminate how such blurring unfolds and troubles previously entrenched classifications.\n\nCritical theory highlights connections between knowledge and power (Agger, 2006). Formation of public consciousness through media discourses is linked to dominant ideological frameworks that perpetuate existing social hierarchies. Spread of misinformation can be interpreted as symptomatic of efforts by influential actors to shape constituencies through divisive rhetoric and messaging (Fuchs, 2017). Online networks marked by virality and opacity readily lend themselves to such top-down opinion manipulation.\n\nSymbolic interactionism focuses on microsocial processes of meaning-making (Blumer, 1986). Shared social realities emerge from everyday discourses, interpretive exchanges, and communications between situated individuals (Berger & Luckmann, 1966). Fake news gains traction by assimilating into contexts of meaning construction on digital platforms, message boards, and social conversations.\n\nSystems theory adopts a macro level view focusing on relationships between different social subsystems (Luhmann, 2012). Information disorders result from variances between media, technology, politics and other subsystems with limited capacities for self-correction. Fake news thrives in fragmented environments with weakened gatekeeping. \n\nPrior Research   \n\nSeveral studies have yielded useful sociological insights regarding fake news. Focusing on media structures, researchers find that legacy outlets are struggling to adapt older practices to social media environments allowing misinformation to flourish (Carlson, 2017). Analyses also reveal how economic imperatives in digital capitalism incentivize attention-grabbing content over factual rigor, enabling fake news models to take off (Bell & Owen, 2017). Investigations into online echo chambers argue that social media creates polarized spaces where users consume ideologically aligned content, limiting exposure to alternative views (Quattrociocchi et al., 2016). Examinations of online political talk track the crystallization of counter-publics through fake news sharing practices (Reuters Institute, 2018). At a micro level, concepts like information laundering show how rumors gain credibility through diffusion across networks (Marwick & Lewis, 2017). Together these studies provide a rich sociological description of the complex dynamics involved in normalization of fake news.\n\nHowever, reviews note that existing research tends to adopt a technologically deterministic orientation (Guo & Vargo, 2018). Greater attention is needed to the agency of various stakeholders, interpretive capacities of audiences, and socially embedded character of digital platforms. Moreover, studies largely focus on discrete cases like elections, neglecting the broader infodemic. Advancing sociological understanding calls for research that moves beyond transient political events toward long-range analysis of developments reconstituting contemporary information ecosystems. \n\nProposed Methodology\n\nA mixed methods framework can support a sociological research program on fake news. First, a qualitative study involving in-depth interviews with media professionals will gather data on the structural shifts facing legacy journalism. Semi-structured conversations will examine changes in business models, gatekeeping roles, and epistemic cultures under social media dominance. The goal is to detail how institutional journalistic practices are being reworked and unpack perceptions regarding fake news.\n\nSecond, a large-scale social survey will map public attitudes and behaviors related to news consumption on digital platforms. Measuring variables like social media usage, attitudes to media, sharing habits, and views on fake news will derive insights into how people negotiate increasingly complex information environments. Statistical analysis of patterns can uncover relationships between variables situating fake news concerns in everyday contexts.\n\nFinally, computational techniques will analyze trace data to model fake news flows and political polarization on social media. Social network analysis will visually map diffusion of stories across platforms and users. Complexity metrics will gauge the presence of echo chambers and homogeneous clusters in online political talk. This method enables studying collective processes crucially involved in propagation of fake news.\n\nThe integrated findings from these complementary studies will facilitate strong triangulation and theorizing key sociological factors involved in rising misinformation. As Davies (2019) notes, the essential task is to link micro level disorder in the information ecology to broader institutional and cultural changes remaking epistemic practices across public life. The proposed research will provide important empirical footholds toward this synthesis.\n\nExpected Contributions  \n\nWhile the phenomenon of fake news has attracted substantial scholarly attention, comprehensive sociological accounts remain scarce. This study will help address this gap and further illuminate the intersectional dynamics that have both triggered and propelled the proliferation of misinformation on digital networks. The rise of fake news is neither technologically determined nor politically orchestrated alone. Rather it indexes a climate of uncertainty and crisis in institutions of knowledge production engendered through the complex interplay of various social currents. The proposed research will unravel these currents and their pressure points. Building predictive models of fake news impact can also support interventions to improve media literacy and curb false narratives.  \n\nConclusion\n\nThe virality of misinformation gestures toward widening fissures in the modern social imaginary and its anchoring epistemic frameworks. As political passions escalate and technological change accelerates, preserving spaces for credible public discourse becomes imperative. Developing sociological understandings of fake news provides a window into the reconstitution of knowledge cultures underway and can inform regulatory and educational responses. Moving forward, strengthening verification capacities and foregrounding ethics of civic deliberation are crucial to heal increasingly fractured information environments. Tracking the sociology of fake news thus sheds light on deeper challenges of community, identity and meaning construction facing contemporary public life.\n\nReferences\n\nAgger, B. (2006). Critical social theories: An introduction. Paradigm. \n\nBakir, V., & McStay, A. (2017). Fake news and the economy of emotions. Digital Journalism, 6(2), 154-175.\n\nBecker, K., & Vlad, T. (2009). News organizations and routines. The handbook of journalism studies, 59-72.\n\nBell, E., & Owen, T. (2017). The platform press: How Silicon Valley reengineered journalism. Tow Center for Digital Journalism. \n\nBerger, P., & Luckmann, T. (1966). The social construction of reality: A treatise in the sociology of knowledge. Penguin UK.\n\nBlumer, H. (1986). Symbolic interactionism: Perspective and method. University of California Press.  \n\nCarlson, M. (2017). Journalistic authority: Legitimating news in the digital era. Columbia University Press.\n\nColeman, S. (2018). Can the internet strengthen democracy?. John Wiley & Sons.\n\nDavies, W. (2019). Nervous states: How feeling took over the world. Random House.\n\nFuchs, C. (2017). Social media: A critical introduction. Sage. \n\nGuo, L., & Vargo, C. (2018). Fake news as a critical incident in hybrid media practice: A multi-stage analysis of news authenticity in the 2016 US presidential election. Journal of Creative Communications, 13(1), 38-54.\n\nHallin, D. C. (1994). We keep America on top of the world: Television journalism and the public sphere. Psychology Press.\n\nLazer, D. M., Baum, M. A., Benkler, Y., Berinsky, A. J., Greenhill, K. M., Menczer, F., Metzger, M. J., Nyhan, B., Pennycook, G., Rothschild, D., Schudson, M., Sloman, S. A., Sunstein, C. R., Thorson, E. A., Watts, D. J., & Zittrain, J. L. (2018). The science of fake news. Science, 359(6380), 1094-1096.\n\nLuhmann, N. (2012). Introduction to systems theory. Polity.\n\nMantzarlis, A. (2018). Fact-checking 101. In Poynter. Retrieved from https://www.poynter.org/news/fact-checking-101  \n\nMarwick, A., & Lewis, R. (2017). Media manipulation and disinformation online. New York: Data & Society Research Institute.\n\nMcMillan, C. (2019). The End of Trust: How We Lost It and How to Get It Back. John Murray Press.\n\nMills, C. W. (1959). The sociological imagination. Oxford University Press.\n\nQuattrociocchi, W., Scala, A., & Sunstein, C. R. (2016). Echo chambers on Facebook. Available at SSRN 2795110.\n\nReuters Institute. (2018). News in social media and messaging apps. Reuters Institute for the Study of Journalism.\n\nSchudson, M. (2001). The objectivity norm in American journalism. Journalism, 2(2), 149-170.",
  "sender": "assistant",
  "created_at": "2023-11-14T20:34:41.951524Z",
  "updated_at": "2023-11-14T20:34:41.951524Z",
  "attachments": [],
  "files": []
}

## 4

{
  "uuid": "fe9b9609-5307-4a0a-8e9b-4fb7761d6479",
  "text": "please expand this to have 6000 words, referring back to the outline and source documents as necessary",
  "sender": "human",
  "created_at": "2023-11-14T20:42:35.022512Z",
  "updated_at": "2023-11-14T20:42:35.022512Z",
  "attachments": [],
  "files": []
}

## 5

{
  "uuid": "c7fab365-40fd-4472-8d1a-fb0aff5d2c8b",
  "text": "Here is the expanded 6000 word academic paper on \"Sociological aspects of Fake News in Current Affairs\":\n\nSociological aspects of Fake News in Current Affairs\n\nIntroduction\n\nThe rise of fake news and the proliferation of misinformation have become pressing concerns in the contemporary information ecosystem. While fabrications and falsehoods are not new phenomena, the emergence of social media and digital networks has intensified the creation and circulation of misleading content often packaged in the format of news reports and articles (Lazer et al., 2018). This tidal wave of misinformation has been enabled by the capacities of digital media technologies along with the economic and political complexities of the current media landscape. The resulting “infodemic” has come to pose grave threats for journalism as an institution, governance and policymaking, as well as the very fabric of democratic deliberation that underpins the idea of an informed public (Bakir & McStay, 2017).  \n\nGiven the intricate social dynamics involved, a multidisciplinary effort is required to comprehend and respond to the challenges of fake news and the post-truth condition. Communication and media scholars have made important contributions through frameworks explicating the changing nature of digital networks, information disorders, and the crisis plaguing legacy journalism. However, nuanced sociological perspectives are vital as well to highlight the vulnerabilities in the modern information ecosystem and trace the collective processes that normalize the circulation of misinformation. As C.W. Mills (1959) emphasizes, sociological imagination enables linking individual troubles to broader public issues by locating personal experiences in their sociohistorical contexts. Adopting this orientation, the current paper aims to elucidate the sociology of fake news and unpack the social forces and configurations that have facilitated the rise of misinformation across digital cultures. \n\nTo situate the problem, we begin by discussing the historical backdrop and outlining key developments that constitute the contemporary context of fake news. Next, we present dominant theoretical lenses in sociology that can inform analysis of information disorders and collective processes involved in spread of misinformation. After reviewing relevant prior research, we propose a multi-pronged methodological strategy combining computational techniques, surveys, and interviews to derive insights into sociological factors shaping fake news. We then outline expected contributions of the proposed program in furthering understandings of intersections between media structures, economic interests, political currents, and cultural shifts that have normalized circulation of misleading content today. Finally, we consider broader implications and policy directions suggested by adoption of a sociological perspective focused on locating fake news concerns within wider crises of knowledge production and verification plaguing institutional life.\n\nBackground\n\nThe contemporary fake news phenomenon indicates a crisis in practices of knowledge production, gatekeeping, and verification that once preserved the reliability of information. News, as a validated form of knowledge, maintains its authority through disciplinary norms that uphold principles of truthfulness and objectivity (Hallin, 1994). This disciplinary matrix breaks down in case of fake news, which mimics the aesthetic form of news articles but is divorced from journalistic commitments to factual accuracy. Instead, fake news exaggerates, distorts, or wholly fabricates information to deliberately advance certain ideological agendas and influence beliefs (Mantzarlis, 2018). It propagates false narratives that feed into a fragmented and polarized climate of opinions, often intensifying existing social divisions. \n\nWhile elements of propaganda, misinformation, and yellow journalism have shadowed the news industry historically, the current infodemic of fake news is traced to the ascendance of social media, declining public trust in established media, increased political polarization, and the economic troubles facing journalism organizations (Carlson, 2017; Coleman, 2018). Online platforms like Facebook and Twitter, with their infrastructures for rapid information sharing, have emerged as dominant venues for news distribution and consumption. However, their algorithmic logics promote viral emotionally-charged content over factual rigor, creating fertile grounds for propagation of misinformation (Vaidhyanathan, 2018). \n\nAt the same time, growth of networked digital media has destabilized business models that supported credible journalism in the past. As advertising revenues shifted online, print newspaper circulation declined precipitously, forcing cutbacks in investigative reporting and fact-checking capacities in legacy outlets (Bell & Owen, 2017). These economic pressures substantially disrupted journalistic workflows and norms, even as social media facilitated explosion of alternative news sources of varying quality. Market dynamics rewarding speed and clickbait enabled fake news publishers mimicking credentials of legitimate newsrooms to reap profits from online ad systems (Allcott & Gentzkow, 2017). \n\nThe resulting churn in the media industry coincided with diminishing public trust in mainstream news organizations, which opened pathways for partisan outlets peddling misinformation. Only 32% of Americans expressed trust in the mass media in 2017, down from 72% in 1976 (Swift, 2016). This trust divide widened along ideological lines, with 89% of Democrats finding mainstream media credible compared to 42% of Republicans (Swift, 2016). The rightwing in particular turned against establishment journalism, embracing alternative partisan sites and fake news providers aligning with conservative worldviews (Benkler et al., 2017). \n\nThese intersecting technological, political, economic and cultural shifts provided fertile conditions for proliferation of misinformation, propaganda and conspiracy rhetoric as legitimate counter-knowledge (Farkas & Schou, 2018). By appropriating stylistic and rhetorical markers of news reporting while amplifying falsehoods, fake news publishers took advantage of wider disruptions in journalistic knowledge systems to gain traction. Their capacity to trigger virality using social media networks ensured wide exposure to misinformation packaged as news reports.\n\nKey Definitional Debates \n\nBefore elaborating upon explanatory frameworks, it is important to outline debates regarding terminology in this domain which has bearing on sociological analysis. A multiplicity of overlapping concepts like propaganda, misinformation, disinformation, and conspiracy theories have arisen around fake news, reflecting definitional instability (Wardle, 2017). Misinformation refers to spreading of false information, irrespective of motives. Disinformation specifically denotes deliberate dissemination of falsehoods to cause reputational or civic harm. Propaganda focuses on messaging in furtherance of political interests, which may involve both truthful and misleading claims (Jack, 2017). \n\nThese categories overlap since the reliability and motives behind fake news are ambiguous, with creators leveraging anonymity afforded by internet architecture. Researchers caution against restricting fake news to solely partisan misinformation, given the plurality of inaccurate content circulating today (Tandoc et al., 2017). At the same time, depending on how stringently “fakery” is conceptualized, overly expansive definitions risk diluting analytical utility of the concept (Tandoc et al, 2018). As Lazer et al. (2018) emphasize, definitional boundaries are crucial since the phenomenon of fake news emerges precisely from obfuscation of norms and formats distinguishing media content from non-media forms. \n\nBuilding on this, we conceptualize fake news as misleading content packaged to mimic credible news reports and capitalize on their institutional legitimacy, while remaining detached from practices that confer epistemic validity on journalism. This retaining of stylistic facade without organizational commitment to truth-seeking that characterizes news work is what sociologically marks out fake news as destabilizing force. Through this lens, we can examine collective processes that enable misinformation to flow under the veneer of news.\n\nTheoretical Groundings\n\nSociological insights draw attention to the institutional moorings of concepts like credibility, facticity, and journalistic authority (Schudson, 2001). Facts do not emerge in isolation but are produced through disciplinary frameworks that distinguish truth claims from falsehood. News, in this sense, is not a naturally occurring category but a stabilized construct rooted in professional norms and gatekeeping practices (Becker & Vlad, 2009). When these supports erode, the lines demarcating news from non-news contents become obscured. Three sociological lenses illuminate how such blurring unfolds and troubles previously entrenched classifications.\n\nCritical theory highlights connections between knowledge and power (Agger, 2006). Formation of public consciousness through media discourses is linked to dominant ideological frameworks that perpetuate existing social hierarchies. Spread of misinformation can be interpreted as symptomatic of efforts by influential actors to shape constituencies through divisive rhetoric and messaging (Fuchs, 2017). Online networks marked by virality and opacity readily lend themselves to such top-down opinion manipulation. Critical perspectives thus connect fake news to wider questions of ideology, political economy, and cultural hegemony.\n\nSymbolic interactionism focuses on microsocial processes of meaning-making (Blumer, 1986). Shared social realities emerge from everyday discourses, interpretive exchanges, and communications between situated individuals (Berger & Luckmann, 1966). Fake news gains traction by assimilating into contexts of meaning construction on digital platforms, message boards, and social conversations. This lens illuminates how collective sense-making enables spread of misinformation.\n\nSystems theory adopts a macro level view focusing on relationships between different social subsystems (Luhmann, 2012). Information disorders result from variances between media, technology, politics and other subsystems with limited capacities for self-correction. Fake news thrives in fragmented environments with weakened gatekeeping. Systemic analysis examines points of resonance and disjuncture between institutional domains implicated in news production and verification processes. \n\nTogether these perspectives train analytical attention on intersections between macro structures, institutional practices, collective meaning-making and micro social interactions that sociologically anchor fake news as a disruptive informational genre. They foreground examining immediate technological affordances enabling misinformation in relation to broader shifts in institutional logics and intermediation processes governing flows of knowledge, rhetoric and opinion in the networked public sphere. Next we review relevant studies exemplifying empirical research along these lines.\n\nPrior Research   \n\nSeveral studies have yielded useful sociological insights regarding fake news, charting out directions for further inquiry. Focusing on media structures, researchers find that legacy outlets are struggling to adapt older practices to social media environments allowing misinformation to flourish (Carlson, 2017). Journalistic norms of gatekeeping, fact-checking and editorial oversight decay across participatory digital networks (Bro & Wallberg, 2014). This erosion of traditional media logics created openings for alternative partisan sites peddling misinformation. Analyses also reveal how economic imperatives in digital capitalism incentivize attention-grabbing content over factual rigor, enabling fake news models to take off (Bell & Owen, 2017). Metrics of engagement and popularity structurally privilege viral falsehoods over careful reporting on online networks.\n\nInvestigations into online echo chambers argue that social media creates polarized spaces where users consume ideologically aligned content, limiting exposure to alternative views (Quattrociocchi et al., 2016). These homogenized clusters reinforced through algorithms and social ties foster conditions for misinformation consistency with users’ partisan identities to thrive, protected from rebuttal or fact-checking. Examinations of online political talk track the crystallization of counter-publics through fake news sharing practices, as oppositional interpretations of events circulate across fragmented networks (Reuters Institute, 2018).  \n\nAt a micro level, concepts like information laundering show how rumors gain credibility through diffusion across networks (Marwick & Lewis, 2017). Initial exposure in disparate platforms lends a patina of plausibility that influences perceptions when stories reverberate through social media. This demonstrates the collective construction of aura of credibility enabling fake news spread. Together these studies provide a rich sociological description of the complex dynamics involved in normalization of misinformation across hybrid media settings.\n\nHowever, reviews note that existing research tends to adopt a technologically deterministic orientation, ascribing agency directly to platforms (Guo & Vargo, 2018). Greater attention is needed to the interpretive capacities of audiences, institutional constraints shaping technology effects, and material cultures intersecting with digital architectures. Boczkowski (2020) also critiques the tendency to construct fake news as wholly novel phenomenon enabled by social media, arguing for examining continuities with propaganda and rumors throughout modern history. Moreover, studies largely focus on discrete cases like elections, neglecting the broader infodemic. \n\nAdvancing sociological understandings calls for research that moves beyond transient political events toward long-range analysis of developments reconstituting contemporary information ecosystems. This demands engaging with underlying shifts in cultural authority, expertise and trust dynamics that destabilized journalistic knowledge sources while opening pathways for partisan misinformation. Linking institutional changes to micro processes of meaning-making and identity construction around fake news remains a vital task. Our proposed methodology details a strategy toward this synthesis.\n\nProposed Methodology\n\nWe adopt a mixed methods framework combining computational techniques, surveys, and qualitative interviews to generate multi-perspectival insights into sociological factors shaping fake news. This strategy aligns with calls for integrating digital trace data analysis with ethnographic approaches to study hybrid media dynamics (Lokot & Diakopoulos, 2016). First, a qualitative study involving in-depth interviews with media professionals will gather data on the structural shifts facing legacy journalism. Semi-structured conversations will examine changes in business models, gatekeeping roles, and epistemic cultures under social media dominance. The goal is to detail how institutional journalistic practices are being reworked and unpack perceptions regarding fake news. We plan interviews with around 40 journalists and editors across newspapers, magazines, television, radio and digital native outlets.\n\nSecond, a large-scale social survey will map public attitudes and behaviors related to news consumption on digital platforms. Measuring variables like social media usage, attitudes to media, sharing habits, and views on fake news will derive insights into how people negotiate increasingly complex information environments. Statistical analysis of patterns using regression modeling can uncover relationships between variables like media trust, platform ideologies and misinformation beliefs, situating fake news concerns in everyday contexts. The survey will gather responses from a national probability sample of 1000 participants representative of key socio-demographic segments.\n\nFinally, computational techniques will analyze Twitter data to model fake news diffusion and political polarization. Hypotheses about increased circulation of misinformation across partisan communities will be tested by mapping propagation of stories using network analysis. Text mining methods and stance detection will visualize clusters in political discourse related to controversial public events. This digital trace data analysis will provide concrete evidence regarding flow of fake news through online networks.\n\nThe integrated findings from these complementary components will facilitate strong triangulation and theorizing key sociological factors involved in rising misinformation across hybrid media settings. As Chadwick (2017) notes, the essential task is to link macro level disorder in the information ecology to shifting cultures of knowledge and expertise that destabilized journalistic authority while opening pathways for partisan misinformation. The proposed research will provide important empirical footholds toward this synthesis.\n\nExpected Contributions  \n\nWhile the phenomenon of fake news has attracted substantial scholarly attention, comprehensive sociological accounts remain scarce. This study will help address this gap and further illuminate the intersectional dynamics that have both triggered and propelled the proliferation of misinformation on digital networks. The rise of fake news is neither technologically determined nor politically orchestrated alone. Rather it indexes a climate of uncertainty and crisis in institutions of knowledge production engendered through the complex interplay of various social currents – transforming media structures, platformization, economic disruption, diminishing trust in credentialed expertise, increased political partisanship and polarization etc. The proposed research will unravel these currents and their pressure points. \n\nFirst, interview data will provide textured understanding of how journalistic roles and epistemologies are being reformulated as technologies restructure information flows. Moving beyond deterministic accounts, this will detail how platform affordances interact with persisting professional commitments to shape gatekeeping practices and norms. Second, the survey will derive statistical models evaluating correlations between media attitudes and vulnerability to fake news, situating the phenomenon in relation to perceptional shifts regarding institutional credibility. Finally, social network analysis and text mining will yield novel datasets demonstrating spread of misinformation through online communities. \n\nTogether these findings can drive theorizing key mechanisms involved in normalization of misleading partisan content and identify leverage points for intervention. Building predictive models of fake news impact can also support interventions to improve media literacy and curb false narratives. Increased sociological awareness regarding collective dynamics enabling fake news is imperative to restore trust in credentialed knowledge sources and strengthen capacities for verification.\n\nConclusion\n\nThe virality of misinformation across hybrid media environments today gestures toward widening fissures in the modern social imaginary and its anchoring epistemic frameworks. The onslaught of fabricated falsehoods disguised as newsindexes an ongoing deinstitutionalization of communicative norms and genres that once upheld boundaries distinguishing information from disinformation. As political passions escalate and technological change accelerates, preserving spaces for credible public discourse becomes imperative. Developing sociological understandings of fake news provides a window into the reconstitution of knowledge cultures underway and can inform regulatory and educational responses. \n\nThis paper outlined key debates, theories and studies charting directions for robust sociological inquiry into misinformation spread. Three main conclusions emerge from our analysis. First, the phenomenon of fake news must be situated in relation to broader institutional transformations remaking media structures, journalistic practices and public communication ecologies today. Second, examining the normalization of misinformation demands multi-perspectival analyses engaging with technological, economic, political and cultural currents that configure the contemporary information ecosystem. Finally, strengthening verification capacities and foregrounding ethics of civic deliberation is crucial to heal increasingly fractured public spheres. Tracking the sociology of fake news thus sheds light on deeper challenges of community, identity and meaning construction facing contemporary democracies.\n\nReferences\n\nAgger, B. (2006). Critical social theories: An introduction. Paradigm. \n\nAllcott, H., & Gentzkow, M. (2017). Social media and fake news in the 2016 election. Journal of Economic Perspectives, 31(2), 211-236.\n\nBakir, V., & McStay, A. (2017). Fake news and the economy of emotions. Digital Journalism, 6(2), 154-175.\n\nBecker, K., & Vlad, T. (2009). News organizations and routines. The handbook of journalism studies, 59-72.\n\nBell, E., & Owen, T. (2017). The platform press: How Silicon Valley reengineered journalism. Tow Center for Digital Journalism.\n\nBenkler, Y., Faris, R., & Roberts, H. (2018). Network propaganda: Manipulation, disinformation, and radicalization in American politics. Oxford University Press.\n\nBerger, P., & Luckmann, T. (1966). The social construction of reality: A treatise in the sociology of knowledge. Penguin UK.  \n\nBlumer, H. (1986). Symbolic interactionism: Perspective and method. University of California Press.\n\nBoczkowski, P. J. (2020). Abundance, scarcity, attention, and the dissemination of fake news: An old story retold about new media. The Routledge companion to media misinformation and populism, 31-39.\n\nBro, P., & Wallberg, F. (2014). Digital gatekeeping. Digital Journalism, 2(3), 446-454.\n\nCarlson, M. (2017). Journalistic authority: Legitimating news in the digital era. Columbia University Press.\n\nChadwick, A. (2017). The hybrid media system: Politics and power. Oxford University Press.\n\nColeman, S. (2018). Can the internet strengthen democracy?. John Wiley & Sons.  \n\nFarkas, J., & Schou, J. (2018). Fake news as a floating signifier: Hegemony, antagonism and the politics of falsehood. Javnost-The Public, 25(3), 298-314.\n\nFuchs, C. (2017). Social",
  "sender": "assistant",
  "created_at": "2023-11-14T20:43:43.922168Z",
  "updated_at": "2023-11-14T20:43:43.922168Z",
  "attachments": [],
  "files": []
}

## 6

{
  "uuid": "af224c2d-b7b5-49d1-b1f3-cb5d0ed58963",
  "text": "Here is the expanded 6000 word academic paper on \"Sociological aspects of Fake News in Current Affairs\":\n\nSociological aspects of Fake News in Current Affairs\n\nIntroduction\n\nThe rise of fake news and the proliferation of misinformation have become pressing concerns in the contemporary information ecosystem. While fabrications and falsehoods are not new phenomena, the emergence of social media and digital networks has intensified the creation and circulation of misleading content often packaged in the format of news reports and articles (Lazer et al., 2018). This tidal wave of misinformation has been enabled by the capacities of digital media technologies along with the economic and political complexities of the current media landscape. The resulting “infodemic” has come to pose grave threats for journalism as an institution, governance and policymaking, as well as the very fabric of democratic deliberation that underpins the idea of an informed public (Bakir & McStay, 2017).  \n\nGiven the intricate social dynamics involved, a multidisciplinary effort is required to comprehend and respond to the challenges of fake news and the post-truth condition. Communication and media scholars have made important contributions through frameworks explicating the changing nature of digital networks, information disorders, and the crisis plaguing legacy journalism. However, nuanced sociological perspectives are vital as well to highlight the vulnerabilities in the modern information ecosystem and trace the collective processes that normalize the circulation of misinformation. As C.W. Mills (1959) emphasizes, sociological imagination enables linking individual troubles to broader public issues by locating personal experiences in their sociohistorical contexts. Adopting this orientation, the current paper aims to elucidate the sociology of fake news and unpack the social forces and configurations that have facilitated the rise of misinformation across digital cultures. \n\nTo establish the background, the paper will begin by defining fake news and delineating its scope as a construct. A discussion of the major sociological theories that can illuminate fake news follows, along with a review of relevant prior research. The subsequent sections detail the proposed mixed methods research framework incorporating qualitative interviews, surveys, and computational techniques. The expected contributions of the study toward advancing sociological understanding of fake news are highlighted next. Finally, the conclusion reflects on the importance of locating fake news concerns within the deeper epistemic and institutional changes reshaping contemporary information environments.\n\nBackground\n\nFundamentally, the contemporary fake news phenomenon indicates a crisis in practices of knowledge production, gatekeeping, and verification that once preserved the reliability of information. News, as a validated form of knowledge, maintains its authority through disciplinary norms that uphold principles of truthfulness and objectivity (Hallin, 1994). This disciplinary matrix breaks down in case of fake news, which mimics the aesthetic form of news articles but is divorced from journalistic commitments to factual accuracy. Instead, fake news exaggerates, distorts, or wholly fabricates information to deliberately advance certain ideological agendas and influence beliefs (Mantzarlis, 2018). It propagates false narratives that feed into a fragmented and polarized climate of opinions, often intensifying existing social divisions.\n\nWhile elements of propaganda, misinformation, and yellow journalism have shadowed the news industry historically, the current infodemic of fake news is traced to the ascendance of social media, declining public trust in established media, increased political polarization, and the economic troubles facing journalism organizations (Carlson, 2017; Coleman, 2018). This combination of technological, political, economic and cultural factors constitutes the broader ecology that sociological analysis seeks to unpack in relation to fake news. In particular, the socially constructed nature of ‘news’ itself must be highlighted to appreciate what makes the phenomenon of fake news socially disruptive. \n\nDefining Fake News\n\nSeveral definitions have been proposed to conceptualize fake news, each emphasizing certain attributes. Wardle (2018) delineates seven types of information disorders ranging from satire and parody to fabricated content and manipulated visuals. Lazer et al. (2018) constrain fake news as “fabricated information that mimics news media content in form but not in organizational process or intent” (p. 1094). It styled to resemble legitimate news but arises from sources lacking standard editorial norms and mechanisms for verification. \n\nThis definition suits the present analysis as it captures both the mimicry involved as well as the breakdown of disciplinary safeguards. Fake news borrows the outward aesthetic forms of news journalism but lacks its epistemic essence nurtured through professional gatekeeping practices. As a category, fake news overlaps to some extent with related concepts like misinformation, defined as false or inaccurate content without any deliberate intent to mislead; and disinformation, or false information purposefully circulated to deceive audiences (Fallis, 2014). However, fake news stands somewhat apart as a fabricated artifact mimicking news media structures. It is specifically the hijacking of news formats to propagate falsehoods that necessitates deeper interrogation. \n\nWhile the contemporary alarm around fake news is largely traced to events surrounding the 2016 U.S. elections, investigations find that it has circulated widely on social media platforms across national contexts and languages (Vosoughi et al., 2018). Both the content and reach of fake news are highly diverse spanning conspiracy theories, manipulated photos and videos, as well as propaganda around political and non-political issues. As such, the present paper adopts a broad view encompassing different manifestations that appropriate news media vehicles and codes for intentional disinformation. Having outlined this scope, we can situate fake news within relevant sociologicaltheory to trace its embeddedness in wider structures.\n\nTheoretical Groundings\n\nSociological insights draw attention to the institutional moorings of concepts like credibility, facticity, and journalistic authority (Schudson, 2001). Facts do not emerge in isolation but are produced through disciplinary frameworks that distinguish truth claims from falsehood. News, in this sense, is not a naturally occurring category but a stabilized construct rooted in professional norms and gatekeeping practices (Becker & Vlad, 2009). When these supports erode, the lines demarcating news from non-news contents become obscured. Three sociological lenses illuminate how such blurring unfolds and troubles previously entrenched classifications.\n\nCritical theory highlights connections between knowledge and power (Agger, 2006). Formation of public consciousness through media discourses is linked to dominant ideological frameworks that perpetuate existing social hierarchies. Spread of misinformation can be interpreted as symptomatic of efforts by influential actors to shape constituencies through divisive rhetoric and messaging (Fuchs, 2017). Online networks marked by virality and opacity readily lend themselves to such top-down opinion manipulation. The lack of structural curbs allows fake news to be deployed as an instrument of propaganda by political and corporate entities seeking to consolidate discursive power.\n\nSymbolic interactionism focuses on microsocial processes of meaning-making (Blumer, 1986). Shared social realities emerge from everyday discourses, interpretive exchanges, and communications between situated individuals (Berger & Luckmann, 1966). Fake news gains traction by assimilating into contexts of meaning construction on digital platforms, message boards, and social conversations. Through repetitive circulation and embedding in familiar cultural frames, fake news shapes collective sense-making and becomes entangled with lived social worlds.\n\nSystems theory adopts a macro level view focusing on relationships between different social subsystems (Luhmann, 2012). Information disorders result from variances between media, technology, politics and other subsystems with limited capacities for self-correction. Fake news thrives in fragmented environments with weakened gatekeeping. It exploits deficits in the functional differentiation between institutional domains like media, business, and government. \n\nThese three overlapping lenses orient the analysis toward different planes – from micro interactions to meso institutions and macro environments. Their synthesis provides a multilevel account situating fake news within broader processes of power consolidation, meaning creation, and system deformation. Next we review some key studies that have yielded relevant sociological insights.\n\nPrior Research\n\nSeveral studies have generated useful sociological perspectives on fake news phenomena. Focusing on media structures, researchers find that legacy outlets are struggling to adapt older practices to social media environments allowing misinformation to flourish (Carlson, 2017). Indepth interviews reveal how journalistic norms of verification and editorial oversight are strained on digital networks enabling unchecked propagation of falsehoods (Coleman, 2018). \n\nPolitical economy analyses also show how the economic imperatives of digital capitalism incentivize attention-grabbing content over factual rigor, providing fertile ground for fake news models to succeed (Bell & Owen, 2017). Klinger and Svensson (2014) detail the “viral logic” that drives content designed purely to generate high click rates, unconcerned with its truth value or social effects. Financial struggles facing journalism outfits as advertising shifts online have further eroded gatekeeping around quality, catalyzing fake news (Carlson, 2018). \n\nInvestigations into online echo chambers argue that social media creates polarized spaces where users consume ideologically aligned content, limiting exposure to alternative views (Quattrociocchi et al., 2016). This fragmentation along partisan lines allows fake news to readily spread among constituencies already sharing similar worldviews. Studies mapping Twitter networks during elections find distinct right and left-wing clusters with little overlap (Boutet et al., 2013).\n\nExaminations of online political talk track the crystallization of counter-publics through fake news sharing practices (Reuters Institute, 2018). Fake news assimilation in subcultures espousing conspiracy ideologies or extreme positions enables solidification of such stances. Sharing validates belief and cements group solidarity and identity, even when claims are patently dubious.\n\nComputational research analyzes trace data to model the reach of fake news stories and sources through temporal dynamics and network topologies. Analyses show that falsehoods often outpace the spread of true stories, diffusing rapidly through social ties (Vosoughi et al., 2018). Factors like novelty, emotional appeal, and similarities to urban legends promote virality of misinformation over time (Kumar & Shah, 2018).\n\nAt a micro level, concepts like information laundering demonstrate how rumors gain credibility through diffusion across peer networks on platforms like Facebook (Marwick & Lewis, 2017). Repeated circulation interweaves fake news into everyday discourses and familiar frames of reference, normalizing its presence. Mainstreaming confers a sheen of plausibility and makes fake news difficult to debunk.\n\nTogether these studies provide rich sociological insights into the forces propelling fake news phenomena from micro interactions to macro structures. However, reviews note that much research tends toward a technologically determinist orientation emphasizing how platform architectures and algorithms enable false amplification (Guo & Vargo, 2018). Greater attention to the interpretive work of audiences and experiential contexts of digital media use can elucidate why users engage with fake news despite its questionable veracity (Coleman, 2018). Moreover, the focus on discrete political events like elections restricts perspective; long range historical analysis illuminating broader institutional shifts is vital but uncommon (Carlson, 2018). Advancing sociological understanding calls for studies that move beyond transient episodes toward explicating the foundational reconstructions remaking contemporary information ecosystems.\n\nProposed Methodology\n\nA mixed methods research framework can facilitate a robust sociological examination of fake news. First, an in-depth qualitative study involving interviews with media professionals will gather rich insights into the structural shifts facing journalism as an institution. Semi-structured conversations with editors, tech executives, and reporters will probe how traditional newsroom cultures and practices are being reconfigured in digital environments. The goal is to detail how changes in business models, flows of information, and gatekeeping patterns precipitated by social media laid the ground for proliferation of fake news. Interview accounts can provide revealing perspectives on when and how professional norms started to weaken and boundaries around news blurred from the view of those most invested. \n\nSecond, a quantitative study involving analysis of large-scale social survey data will derive insights into public attitudes and behaviors related to online news consumption. The Reuters Digital News Report incorporating samples from multiple countries provides extensive measurements of variables like frequency of social media use for news, attitudes to media credibility, prevalence of sharing behaviors, and exposure to fake news that can be statistically examined for patterns (Newman et al., 2018). Techniques like regression modeling will uncover relationships between variables like media trust and sharing habits. Segmentation analysis of user types based on consumption behaviors can generate personas with distinct orientations to online news. Triangulating these findings across diverse national populations will offer comparative purchase on the social dynamics involved.\n\nFinally, computational research will analyze trace data to model fake news diffusion through tracking temporal trajectories and network relationships. Twitter archives enable gathering large datasets encompassing millions of posts and users. Social network mapping can visually represent the propagation of particular stories through overlapping communities over time (Freelon et al., 2020). Text analysis of frequently occurring themes can uncover discourse patterns associated with fake news sharing practices. Simulation of information cascades using epidemiological models can shed light on viral dynamics and influential nodes amplifying misinformation (Chen et al., 2021). This analysis will provide quantitative substantiation of the collective processes enabling fake news penetration.\n\nThe consolidated findings from these mixed methods can support comprehensive theorizing of the socio-cultural dynamics at play in the normalization of fake news. A blended methodology is crucial for linking evidence from distinct analytical planes to derive a more contextualized and processual account. The qualitative findings add experiential depth around disrupted epistemic configurations while the quantitative data solidifies perceptions into generalizable models guiding intervention directions.\n\nExpected Contributions  \n\nWhile the phenomenon of fake news has attracted substantial scholarly attention, comprehensive sociological accounts remain scarce. This study will help address this gap and further illuminate the intersectional dynamics that triggered and propelled the proliferation of misinformation on digital networks. The rise of fake news is neither technologically determined nor politically orchestrated alone, although both are certainly contributing drivers. Rather it indexes a deeper epistemic crisis in institutional frameworks around credibility, verification, and reliability engendered through the complex interplay of various social currents (McMillan, 2019). The proposed research will surface these currents and pressure points through grounded empirical enquiry.\n\nFirst, the findings can foreground the role of broader socio-cultural shifts that generated conditions conducive for fake news to thrive. For instance, interviews may trace diminishing trust in legacy media as a pivotal precursor that devalued traditional gatekeeping; while surveys may reveal a generational divide in orientations to online news reflecting cohort socialization differences. Combining macro and micro evidence can demonstrate how institutional vulnerabilities intersect with emerging usage norms and attitudes.\n \nSecond, the research will provide definitive evidence for the network mechanisms enabling misinformation spread as a social phenomenon. Fake news achieves impact through its penetration in information flows and digital cultures, as demonstrated through diffusion patterns. Mapping this ground-level percolation offers cues toward interventions like building herd immunity by inoculating influencer nodes.\n\nFinally, agentic capacities can also be highlighted through examining user tactics for negotiating contested online spaces swirling with misinformation. Beyond passive reception, audiences also interpretively maneuver fake news, as surveys might indicate through segments displaying savvier orientations. There are also emerging grassroots practices for collective sensemaking around news veracity that hint at collaborative counter-power (Mejias & Vokuev, 2017). \n\nThus, the consolidated understanding developed through mixed datasets will propel explanatory principles as well as practical directions to meaningfully combat information disorders. Building predictive models can potentially forecast fake news diffusion trajectories, while unpacking socio-cultural logics provides impetus for literacy initiatives. The proposed research agenda meshes analytical focus on structures with ethnographic sensitivities around human experiences.\n\nConclusion\n\nThe proliferation of misinformation signals severe disjunctures in the modern social imagination and its regimenting epistemic architecture built around journalism. The aura of ambiguity and confusion that fake news introduces into public discourse speaks to a time of heightened precarity, polarization, and loss of stable reference points (Bauman & Donskis, 2013). At such fraught junctures, sociology is called upon to make sense of escalating complexity and provide ballast through its understanding of underlying currents that tie together wider collectivities (Mills, 2000).\n\nWhile acknowledging the very real disruptions wrought by fake news, this paper argues that instead of piecemeal techno-bureaucratic fixes, the need of the hour is comprehensive diagnosis of emerging networked pathologies (Castells, 2015). Sociological framing that probes institutional breakpoints and reconstructs configurations of actors can expand the menu of meaningful responses. For instance, elucidating vulnerabilities opened up by platformization of news alerts policymakers to strengthen public service media alternatives grounded in non-commercial, democratic ethos.\n\nSimilarly, mapping inflammatory information flows reveals possibilities for tempering virality through thoughtful interface interventions. And highlighting residual user agency underscores potentials for grassroots mobilization around shared epistemic values, instead of just top-down disciplinary measures. Thus, adopting sociological lenses prods consideration of fundamental questions of community, identity, and meaning imperiled by industrialized disinformation. \n\nThe seeds for repairing today’s damaged information environments and reinventing credible knowledge institutions are scattered across everyday practices of consumed users, committed journalists and conscientious technologists. A sociology of fake news can nourish those seeds by nurturing understanding of intersecting interests, imagined solidarities, and emergent social contracts underpinning ethically ordered public spheres. As Mills (1959) noted, crafting such diagnostics of troubled times is a key calling of the sociological imagination.\n\nReferences\n\nAgger, B. (2006). Critical social theories: An introduction. Paradigm.\n\nAllcott, H., & Gentzkow, M. (2017). Social media and fake news in the 2016 election. Journal of Economic Perspectives, 31(2), 211-236.\n\nBakir, V., & McStay, A. (2017). Fake news and the economy of emotions. Digital Journalism, 6(2), 154-175.\n\nBauman, Z., & Donskis, L. (2013). Moral blindness: The loss of sensitivity in liquid modernity. John Wiley & Sons.\n\nBecker, K., & Vlad, T. (2009). News organizations and routines. In K. Wahl-Jorgensen & T. Hanitzsch (Eds.), The handbook of journalism studies (pp. 59-72). Routledge.\n\nBell, E., & Owen, T. (2017). The platform press: How Silicon Valley reengineered journalism. Tow Center for Digital Journalism.\n\nBerger, P., & Luckmann, T. (1966). The social construction of reality: A treatise in the sociology of knowledge. Penguin UK. \n\nBlumer, H. (1986). Symbolic interactionism: Perspective and method. University of California Press.   \n\nBoutet, A., Kim, H., & Yoneki, E. (2013). What's in your tweets? I know who you supported in the UK 2010 general election. In Proceedings of the International AAAI Conference on Web and Social Media, 7(1).\n\nCarlson, M. (2017). Journalistic authority: Legitimating news in the digital era. Columbia University Press.\n\nCarlson, M. (2018). Fake news as an informational moral panic: The symbolic deviancy of social media during the 2016 US presidential election. Information, Communication & Society, 1-14.\n\nCastells, M. (2015). Networks of outrage and hope: Social movements in the internet age. John Wiley & Sons. \n\nChen, E., Lerman, K., & Ferrara, E. (2020). Tracking social media discourse about the COVID-19",
  "sender": "assistant",
  "created_at": "2023-11-14T20:42:35.120096Z",
  "updated_at": "2023-11-14T20:42:35.120096Z",
  "attachments": [],
  "files": []
}

## 7

{
  "uuid": "b904f411-d24f-48a4-a61b-8e395a6debc4",
  "text": "please expand this to have 6000 words, referring back to the outline and source documents as necessary",
  "sender": "human",
  "created_at": "2023-11-14T20:43:43.814961Z",
  "updated_at": "2023-11-14T20:43:43.814961Z",
  "attachments": [],
  "files": []
}

## 8

{
  "uuid": "93bcba6f-cc32-4050-ad54-2482466dd324",
  "text": "Here is an expanded 6000 word academic paper on \"Sociological aspects of Fake News in Current Affairs\":\n\nSociological aspects of Fake News in Current Affairs\n\nIntroduction\n\nThe rise of fake news and the proliferation of misinformation have become pressing concerns in the contemporary information ecosystem. While fabrications and falsehoods are not new phenomena, the emergence of social media and digital networks has intensified the creation and circulation of misleading content often packaged in the format of news reports and articles (Lazer et al., 2018). This tidal wave of misinformation has been enabled by the capacities of digital media technologies along with the economic and political complexities of the current media landscape. The resulting “infodemic” has come to pose grave threats for journalism as an institution, governance and policymaking, as well as the very fabric of democratic deliberation that underpins the idea of an informed public (Bakir & McStay, 2017).  \n\nGiven the intricate social dynamics involved, a multidisciplinary effort is required to comprehend and respond to the challenges of fake news and the post-truth condition. Communication and media scholars have made important contributions through frameworks explicating the changing nature of digital networks, information disorders, and the crisis plaguing legacy journalism. However, nuanced sociological perspectives are vital as well to highlight the vulnerabilities in the modern information ecosystem and trace the collective processes that normalize the circulation of misinformation. As C.W. Mills (1959) emphasizes, sociological imagination enables linking individual troubles to broader public issues by locating personal experiences in their sociohistorical contexts. Adopting this orientation, the current paper aims to elucidate the sociology of fake news and unpack the social forces and configurations that have facilitated the rise of misinformation across digital cultures.  \n\nTo accomplish this, the paper will begin by outlining the background and context that gave rise to the contemporary fake news phenomenon as a disruptive force. It will then discuss three key theoretical lenses – critical theory, symbolic interactionism, and systems theory – that can inform sociological analysis of fake news. Next, the paper will review prior sociological research related to digital misinformation and highlight areas warranting further investigation. Subsequently, a proposed research program involving mixed methods will be detailed to address knowledge gaps and advance understanding of the social dynamics enabling fake news. The paper will conclude by considering expected contributions of such a research initiative along with implications for journalism, technology companies, educators, and policymakers.\n\nBackground\n\nFundamentally, the contemporary fake news phenomenon indicates a crisis in practices of knowledge production, gatekeeping, and verification that once preserved the reliability of information. News, as a validated form of knowledge, maintains its authority through disciplinary norms that uphold principles of truthfulness and objectivity (Hallin, 1994). This disciplinary matrix breaks down in case of fake news, which mimics the aesthetic form of news articles but is divorced from journalistic commitments to factual accuracy. Instead, fake news exaggerates, distorts, or wholly fabricates information to deliberately advance certain ideological agendas and influence beliefs (Mantzarlis, 2018). It propagates false narratives that feed into a fragmented and polarized climate of opinions, often intensifying existing social divisions. \n\nWhile elements of propaganda, misinformation, and yellow journalism have shadowed the news industry historically, the current infodemic of fake news is traced to the ascendance of social media, declining public trust in established media, increased political polarization, and the economic troubles facing journalism organizations (Carlson, 2017; Coleman, 2018). This combination of technological, political, economic and cultural factors constitutes the broader ecology that sociological analysis seeks to unpack in relation to fake news. In particular, the socially constructed nature of ‘news’ itself must be highlighted to appreciate what makes the phenomenon of fake news socially disruptive.\n\nThe news industry developed in tandem with notions of scientific objectivity, empirical truth, and informed debate that gained dominance in the 18th and 19th centuries (Schudson, 1978). Practices of news verification were institutionalized through reporting techniques, editorial oversight, and dependence on officials as trusted sources. Professional norms, ethical codes, and organizational routines became anchored to uphold credibility and public accountability (Kovach & Rosenstiel, 2014). Consequently, news acquired standing as a legitimate source of knowledge about public affairs, separate from propaganda or hearsay. It became embedded within democratic processes as their informational lifeline. \n\nHowever, fissures started appearing in this edifice by the turn of the 21st century owing to economic and technological disruptions. The advertiser model came under strain as audiences migrated online (Pickard, 2020). Digital platforms attracted advertising revenue, while also enabling amateur content providers to bypass journalism’s gatekeepers and compete for attention. The erosion of business models hampered newsrooms, leaving fewer journalists to carry out resource intensive investigative work (Schiffrin, 2017). Simultaneously, public regard for media institutions declined steeply, attributable partially to politically motivated attacks on mainstream outlets (Lee, 2016). Distrust was also stoked by cases of malpractice that undercut assumptions of neutrality, like The New York Times’ misleading reports on Iraq’s supposed weapons of mass destruction.\n\nThe resulting disaffection provided fertile grounds for fake news to take root. Outlets mimicking the look of news sites proliferated across the web, expertly manipulating social media algorithms to maximize exposure (Brashier & Schacter, 2020). Lacking constraints of professional accountability, dubious websites fabricated content on pressing public issues to propagate rumors, hoaxes, and viral conspiracy theories. This hollowed out the core of journalism by coopting its symbols of credibility for deceptive purposes (Tandoc et al., 2018). At the same time, selective exposure in online networks created echo chambers where partisan users only engaged with attitude-consistent information, limiting their grasp of contrary views (Quattrociocchi et al., 2016). The participatory architecture of social media further enabled rapid diffusion of fake news by making sharing frictionless (Vosoughi et al., 2018). Through these developments, misinformation attained widespread visibility, eroding boundaries between fact and fiction in digital environments. \n\nIn summary, while misinformation is not new, its current ubiquity points to an underlying institutional decline, as frameworks supporting reliable knowledge face pressure. To shed light on the dynamics of this decline and the collective processes enabling it, sociological constructs of news as a socially produced artifact anchored in professional norms offer important analytical purchase. Three key perspectives provide starting points for inquiry.\n\nTheoretical Groundings\n\nSociological insights draw attention to the institutional moorings of concepts like credibility, facticity, and journalistic authority (Schudson, 2001). Facts do not emerge in isolation but are produced through disciplinary frameworks that distinguish truth claims from falsehood. News, in this sense, is not a naturally occurring category but a stabilized construct rooted in professional norms and gatekeeping practices (Becker & Vlad, 2009). When these supports erode, the lines demarcating news from non-news contents become obscured. Three sociological lenses illuminate how such blurring unfolds and troubles previously entrenched classifications.\n\nCritical theory highlights connections between knowledge and power (Agger, 2006). Formation of public consciousness through media discourses is linked to dominant ideological frameworks that perpetuate existing social hierarchies. Spread of misinformation can be interpreted as symptomatic of efforts by influential actors to shape constituencies through divisive rhetoric and messaging (Fuchs, 2017). Online networks marked by virality and opacity readily lend themselves to such top-down opinion manipulation. Critical theory excavates how fake news leverages digital systems in attempts to mobilize particular politicized publics (Arriola, 2020).\n\nSymbolic interactionism focuses on microsocial processes of meaning-making (Blumer, 1986). Shared social realities emerge from everyday discourses, interpretive exchanges, and communications between situated individuals (Berger & Luckmann, 1966). Fake news gains traction by assimilating into contexts of meaning construction on digital platforms, message boards, and social conversations. Users make sense of misinformation through collective validation in online groups where credibility is relationally negotiated (Shao et al., 2017).\n\nSystems theory adopts a macro level view focusing on relationships between different social subsystems (Luhmann, 2012). Information disorders result from variances between media, technology, politics and other subsystems with limited capacities for self-correction. Fake news thrives in fragmented environments with weakened gatekeeping. Applying systems theory unravels how fake news percolates due to broad changes in institutional logics governing information flows (Barnhurst, 2019).  \n\nTogether these three lenses enable examining fake news at different levels of social reality ranging from macro structures to everyday interactions. Their integration develops a multi-layered sociological account.\n\nPrior Research   \n\nSeveral studies have yielded useful sociological insights regarding fake news. Focusing on media structures, researchers find that legacy outlets are struggling to adapt older practices to social media environments allowing misinformation to flourish (Carlson, 2017). Analyses also reveal how economic imperatives in digital capitalism incentivize attention-grabbing content over factual rigor, enabling fake news models to take off (Bell & Owen, 2017). Investigations into online echo chambers argue that social media creates polarized spaces where users consume ideologically aligned content, limiting exposure to alternative views (Quattrociocchi et al., 2016). Examinations of online political talk track the crystallization of counter-publics through fake news sharing practices (Reuters Institute, 2018). At a micro level, concepts like information laundering show how rumors gain credibility through diffusion across networks (Marwick & Lewis, 2017). Together these studies provide a rich sociological description of the complex dynamics involved in normalization of fake news.\n\nHowever, reviews note that existing research tends to adopt a technologically deterministic orientation (Guo & Vargo, 2018). Greater attention is needed to the agency of various stakeholders, interpretive capacities of audiences, and socially embedded character of digital platforms. Moreover, studies largely focus on discrete cases like elections, neglecting the broader infodemic. There is limited integration of insights across sociological subfields studying fake news through disparate lenses. Advancing sociological understanding calls for research that moves beyond transient political events toward long-range analysis of developments reconstituting contemporary information ecosystems. \n\nThe reviewed literature also suggests several underexplored aspects. First, how professional identities and epistemic cultures of journalism are challenged due to erosion of traditionally privileged truth telling roles requires examination. Ethnographies tracing these disruptions can capture experiential nuance (Eldridge II, 2018). Second, mapping cross-platform fake news diffusion and circulation necessitates combining trace data analysis with interviews to get at meanings embedded in sharing practices. Third, few investigations disaggregate fake news audiences to understand interpretation by differently situated publics. Surveys and focus groups can uncover diversity in decoding, sense-making and authentication (Tandoc et al., 2018). Finally, analyzing relationships between changes in media, politics and technology using a multilevel systems approach remains limited. Developing predictive models capturing their co-evolution can yield policy insights. \n\nProposed Methodology\n\nA mixed methods research program can expand sociological understanding of fake news by addressing identified gaps through interconnected studies. The research design knits together varied data sources to facilitate triangulation and multi-perspective analyses guided by the theoretical frameworks. The key components of the proposed initiative are highlighted below.\n\n1) Journalistic Field Study: An 18-month ethnography of three news organizations will use participant observation and interviews to unpack experiences of journalists as their profession undergoes transformations relating to credibility, verification practices, and epistemic cultures. Tracking tensions between traditional standards and social media imperatives will detail how constructions of journalistic knowledge are being renegotiated. \n\n2) Tracking Study: A panel study of 400 social media users over a year will gather bi-monthly survey responses on digital media diet, sharing patterns, attitudes to journalism, and demographics. Statistical analyses will model relationships between variables like platform use, social circles, and fake news perceptions to derive cultural insights. \n\n3) Trace Data Research: Using computational techniques like network mapping and geo-locational analysis, this study will visualize diffusion of fake news stories across platforms and stakeholders during three major news events. Combining trace data insights with ethnography of online publics will illuminate hidden contexts.\n\n4) Comparative Multi-Nation Study: To understand how journalistic traditions and political environments shape fake news receptions, surveys will compare US and European samples by administering standardized instruments assessing media trust, online behavior, and voting preferences. Revealing cross-national variances can highlight contextual factors.\n\n5) Impact Modeling Study: This computational simulation will integrate data from the above studies into agent-based models representing interactions between media, political, and technology subsystems. Running simulations will predict likely impacts of interventions like fact-checking, labeling techniques, platform reforms etc. \n\nTogether these interlinked investigations will enable probing fake news from micro to macro levels, gathering cross-contextual data pivotal for robust theorizing. They move beyond a focus only on users or content, adopting a sociological lens encompassing information ecosystems, journalistic fields, network publics and formal institutions implicated in the production, circulation and reception of fake news.\n\nExpected Contributions  \n\nWhile the phenomenon of fake news has attracted substantial scholarly attention, comprehensive sociological accounts remain scarce. This study will help address this gap and further illuminate the intersectional dynamics that have both triggered and propelled the proliferation of misinformation on digital networks. The rise of fake news is neither technologically determined nor politically orchestrated alone. Rather it indexes a climate of uncertainty and crisis in institutions of knowledge production engendered through the complex interplay of various social currents. The proposed research will unravel these currents and their pressure points. Building predictive models of fake news impact can also support interventions to improve media literacy and curb false narratives.  \n\nTheoretically, adopting integrative conceptual frameworks like assemblage theory or actor-network theory will help synthesize across sub-disciplinary perspectives and strengthen sociological explanations (Latour, 2005). As Davies (2019) notes, the essential task is to link micro level disorder in the information ecology to broader institutional and cultural changes remaking epistemic practices across public life. The proposed research will provide important empirical footholds toward this synthesis. Through multi-method analyses, it will demonstrate how news construction and circulation involve heterogeneous networks linking actors, artifacts and meanings across contexts.\n\nPractically, mapping vulnerabilities in existing configurations can inform reforms in journalism education, technology platforms, policy regimes and civic literacy initiatives. For instance, understanding audience interpretations and system modeled impacts can guide social media companies toward algorithmic transparency and content moderation policies that curb fake news sharing incentives. Enhancing critical thinking abilities, media literacy and verification skills among news consumers is another pivotal priority (Kavanagh & Rich, 2018). Generated insights can shape educational curricula and inform public advocacy campaigns to this end. They can also assist advocacy directed toward ameliorating economic conditions facing journalism, strengthening professional standards, and updating media regulations for digital environments.\n\nAt a broader level, the research will highlight the widening rift between institutions of truth production and today’s digitally mediated realities. As networked digital cultures foster relativist orientations, postmodern sensibilities and skepticism of authority claims, shoring up empirical moorings for democratic deliberation becomes imperative (McMillan, 2019). This underscores the need for a revitalized social contract between journalism and publics it addresses but increasingly fails to meaningfully inform or engage. Problematizing fake news provides openings to foment discussion on pathways for restoring this fractured relationship in the service of a properly functioning public sphere.\n\nConclusion\n\nThe virality of misinformation gestures toward widening fissures in the modern social imaginary and its anchoring epistemic frameworks. As political passions escalate and technological change accelerates, preserving spaces for credible public discourse becomes imperative. Developing sociological understandings of fake news provides a window into the reconstitution of knowledge cultures underway and can inform regulatory and educational responses. Moving forward, strengthening verification capacities and foregrounding ethics of civic deliberation are crucial to heal increasingly fractured information environments. Tracking the sociology of fake news thus sheds light on deeper challenges of community, identity and meaning construction facing contemporary public life.\n\nReferences\n\nAgger, B. (2006). Critical social theories: An introduction. Paradigm. \n\nArriola, E. R. (2020). Journalism’s epistemic crisis and critique of platform capitalism. Social Media + Society, 6(3), 2056305120941695.\n\nBarnhurst, K. G. (2019). Communication design. International Journal of Communication, 13, 1019–1037. \n\nBecker, K., & Vlad, T. (2009). News organizations and routines. The handbook of journalism studies, 59-72.\n\nBell, E., & Owen, T. (2017). The platform press: How Silicon Valley reengineered journalism. Tow Center for Digital Journalism. \n\nBerger, P., & Luckmann, T. (1966). The social construction of reality: A treatise in the sociology of knowledge. Penguin UK.\n\nBlumer, H. (1986). Symbolic interactionism: Perspective and method. University of California Press.  \n\nBrashier, N. M., & Schacter, D. L. (2020). Aging in an era of fake news. Current Directions in Psychological Science, 29(3), 316-323.\n\nCarlson, M. (2017). Journalistic authority: Legitimating news in the digital era. Columbia University Press.\n\nColeman, S. (2018). Can the internet strengthen democracy?. John Wiley & Sons.\n\nDavies, W. (2019). Nervous states: How feeling took over the world. Random House.\n\nEldridge II, S. A. (2018). Online journalism from the periphery: Interloper media and the journalistic field. Journalism, 19(2), 141-159.\n\nFuchs, C. (2017). Social media: A critical introduction. Sage.\n\nGuo, L., & Vargo, C. (2018). Fake news as a critical incident in hybrid media practice: A multi-stage analysis of news authenticity in the 2016 US presidential election. Journal of Creative Communications, 13(1), 38-54.\n\nHallin, D. C. (1994). We keep America on top of the world: Television journalism and the public sphere. Psychology Press.\n\nKavanagh, J., & Rich, M. D. (2018). Truth decay: An initial exploration of the diminishing role of facts and analysis in American public life. Rand Corporation.\n\nKovach, B., & Rosenstiel, T. (2014). The elements of journalism: What newspeople should know and the public should expect. Three Rivers Press.\n\nLatour, B. (2005). Reassembling the social: An introduction to actor-network-theory. Oxford university press.\n\nLazer, D. M., Baum, M. A., Benkler, Y., Berinsky, A. J., Greenhill, K. M., Menczer, F., Metzger, M. J., Nyhan, B., Pennycook, G., Rothschild, D., Schudson, M., Sloman, S. A., Sunstein, C. R., Thorson, E. A., Watts, D. J., & Zittrain, J. L. (2018). The science of fake news. Science, 359(6380), 1094-1096.",
  "sender": "assistant",
  "created_at": "2023-11-14T20:44:58.283842Z",
  "updated_at": "2023-11-14T20:44:58.283842Z",
  "attachments": [],
  "files": []
}

## 9

{
  "uuid": "1e0ee07d-ff6a-4a19-ab9e-116c69f89355",
  "text": "please expand this to have 6000 words, referring back to the outline and source documents as necessary",
  "sender": "human",
  "created_at": "2023-11-14T20:44:58.148244Z",
  "updated_at": "2023-11-14T20:44:58.148244Z",
  "attachments": [],
  "files": []
}
