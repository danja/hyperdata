# Untitled

## 0

{
  "uuid": "d075b076-6907-4905-a6b8-4900c50b76e0",
  "text": "Q3",
  "sender": "human",
  "created_at": "2024-09-27T02:23:52.568333Z",
  "updated_at": "2024-09-27T02:23:52.568333Z",
  "attachments": [],
  "files": []
}

## 1

{
  "uuid": "bc05052f-8cbe-4f2c-8910-a1938c72f492",
  "text": " To implement a system for automatically updating and improving prompts based on successful outputs in Transmissions, we could follow these steps:\n\n1. Logging successful outputs:\n   ```javascript\n   class SuccessLogger {\n     constructor(db) {\n       this.db = db;\n     }\n\n     async logSuccess(serviceId, input, output, metrics) {\n       await this.db.insert('successful_outputs', {\n         serviceId,\n         input,\n         output,\n         metrics,\n         timestamp: new Date()\n       });\n     }\n   }\n   ```\n\n2. Retrieving relevant examples:\n   ```javascript\n   class ExampleRetriever {\n     constructor(db) {\n       this.db = db;\n     }\n\n     async getRelevantExamples(serviceId, input, limit = 5) {\n       // Implement similarity search here\n       // This is a placeholder implementation\n       return await this.db.query('successful_outputs')\n         .where('serviceId', serviceId)\n         .orderBy('timestamp', 'desc')\n         .limit(limit);\n     }\n   }\n   ```\n\n3. Updating prompts:\n   ```javascript\n   class PromptUpdater {\n     constructor(exampleRetriever) {\n       this.exampleRetriever = exampleRetriever;\n     }\n\n     async updatePrompt(serviceId, currentPrompt, input) {\n       const relevantExamples = await this.exampleRetriever.getRelevantExamples(serviceId, input);\n       \n       // Combine current prompt with relevant examples\n       const updatedPrompt = this.incorporateExamples(currentPrompt, relevantExamples);\n       \n       return updatedPrompt;\n     }\n\n     incorporateExamples(prompt, examples) {\n       // Logic to incorporate examples into the prompt\n       // This is a simple implementation and might need to be more sophisticated\n       const exampleText = examples.map(ex => `Input: ${ex.input}\\nOutput: ${ex.output}`).join('\\n\\n');\n       return `${prompt}\\n\\nHere are some successful examples:\\n\\n${exampleText}`;\n     }\n   }\n   ```\n\n4. Integration into service execution:\n   ```javascript\n   class TransmissionService {\n     constructor(llm, promptUpdater, successLogger) {\n       this.llm = llm;\n       this.promptUpdater = promptUpdater;\n       this.successLogger = successLogger;\n     }\n\n     async process(input) {\n       const updatedPrompt = await this.promptUpdater.updatePrompt(this.id, this.basePrompt, input);\n       const output = await this.llm.generate(updatedPrompt, input);\n       \n       const metrics = this.evaluateOutput(output);\n       if (this.isSuccessful(metrics)) {\n         await this.successLogger.logSuccess(this.id, input, output, metrics);\n       }\n\n       return output;\n     }\n\n     evaluateOutput(output) {\n       // Implement output evaluation logic\n     }\n\n     isSuccessful(metrics) {\n       // Define success criteria based on metrics\n     }\n   }\n   ```\n\nThis system continuously improves by incorporating successful examples into future prompts. It's flexible and can be adapted to different services within Transmissions.\n\nQ1: How might we implement the similarity search in the ExampleRetriever to find truly relevant examples?\nQ2: What strategies could we use to prevent prompt explosion as more examples are added over time?\nQ3: How could we incorporate human feedback into this automated system to ensure the quality of improvements?",
  "sender": "assistant",
  "created_at": "2024-09-27T02:23:52.568333Z",
  "updated_at": "2024-09-27T02:23:52.568333Z",
  "attachments": [],
  "files": []
}

## 2

{
  "uuid": "508ebbfe-0030-47d6-80eb-2c831f27eff5",
  "text": " The concept of data flywheels for LLM applications could be highly relevant and useful for the Transmissions project. Here's how it might relate and be applied:\n\n1. Evaluation Metrics: \n   - Define specific success metrics for each service in the Transmissions pipeline.\n   - Implement both code-based and LLM-based validators to assess these metrics.\n\n2. Monitoring:\n   - Set up a system to continuously monitor the performance of each service.\n   - Regularly reassess if the chosen metrics still align with project goals.\n\n3. Continual Improvement:\n   - Use insights from monitoring to iteratively improve prompts and pipeline configurations.\n   - Implement an automated system to update few-shot examples based on successful outputs.\n\n4. Multi-step Pipeline Validation:\n   - Treat each service in Transmissions as a node in an LLM graph.\n   - Implement validators at critical points in the pipeline to catch and correct errors early.\n\n5. Input Validation:\n   - Add input validators to ensure only appropriate data enters the pipeline.\n\n6. Dynamic Few-shot Learning:\n   - Implement a system to dynamically retrieve relevant examples for each service based on input similarity.\n\n7. Logging and Analysis:\n   - Set up comprehensive logging of inputs, outputs, and metric scores for each service.\n   - Use this data to identify patterns and areas for improvement.\n\n8. Automated Improvement:\n   - Develop a system to automatically \"fix\" low-scoring outputs and use these fixes in future runs.\n\nImplementing these concepts could significantly enhance the robustness and performance of the Transmissions project, allowing it to continually improve based on real-world usage.\n\nQ1: How might we implement dynamic few-shot example retrieval in the Transmissions project?\nQ2: What specific metrics would be most relevant for evaluating the performance of Transmissions services?\nQ3: How could we design a system to automatically update and improve prompts based on successful outputs in Transmissions?",
  "sender": "assistant",
  "created_at": "2024-09-27T02:20:33.481508Z",
  "updated_at": "2024-09-27T02:20:33.481508Z",
  "attachments": [],
  "files": []
}

## 3

{
  "uuid": "7a386dad-25d2-49ae-8ccf-1ca86782393c",
  "text": "Please read the following, then tell me how flywheels might relate and be useful to the Trasmissions project: Shreya Shankar\nHome Papers Blog\nJul 1, 2024\n|\n20 min read\nData Flywheels for LLM Applications\nTable of Contents\nA Framework for Creating a Flywheel\n1. Evaluation: Defining Success Metrics\na) Identify metrics that matter for your specific use case.\nb) Implement these metrics.\nc) Validating multi-step pipelines (i.e., “graphs” of LLM calls).\nd) Validate inputs as well as outputs.\n2. Monitoring: Operationalizing Metric Implementations\na) Automatically reassess if your chosen metrics still align with your goals.\nb) Keep metric implementations aligned with your evaluation criteria.\n3. Continual Improvement: Closing the Loop\na) Manually iterate on prompts or pipelines to enhance performance on your defined metrics.\nb) Automatically improve the pipeline in response to metrics.\nLLM Applications: A New Set of Challenges\nApplications of LLM Uncertainty Quantification to Aligning Validators\nData Flywheels for “Graphs” of LLM Calls\nDatabase-Driven LLM Pipeline Validation\nWrapping Up\nFootnotes\nOver the past few months, I have been thinking a lot about workflows to automatically and dynamically improve LLM applications using production data. This stems from our research on validating data quality in LLM pipelines and applications—which is starting to be productionized in both vertical AI applications and LLMOps companies. (I am always very thankful to the teams in industry who find my work useful and are open to collaborating.)\n\nMy ideas for data flywheels are grounded in several observations:\n\nHumans need to be in the loop for evaluation regularly, as human preferences on LLM outputs change over time.\nFine-tuning models come with significant overhead, and many teams prefer LLM APIs for rapid iteration and simple deployment. LLM APIs do not always listen to instructions in the prompt, especially over large batches of inputs, and there thus needs to be validators on LLM outputs.\nLLM-as-a-judge is increasingly popular. LLMs are getting cheaper, better, and faster.\nEmpirical evidence shows that few-shot examples are most effective for improving prompt performance. As such, LLM-as-a-judge can be aligned with human preferences—given the right few-shot examples.\nIn this post, I’ll outline a three-part framework for approaching this problem. Before we dive into the details of each part, let’s take a look at an overview of the entire LLM application pipeline:\n\nData Flywheels\n\nFigure 1: Continually-Evolving LLM Application Pipeline, adapted from the source of Figure 1 in this paper.\n\nThis diagram illustrates my (idealized) architecture of an LLM pipeline, from input processing through evaluation and logging. It showcases ideas I’ll discuss throughout the blog post, such as input validation, dynamic few-shot example retrieval, and the use of both code-based and LLM-based metrics for evaluation.\n\nNow, let’s break down this process into the main parts: Evaluation, Monitoring, and Continual Improvement.\n\nA Framework for Creating a Flywheel\n1. Evaluation: Defining Success Metrics\nThere are several great resources specifically on evaluation. Here’s one I really like from my friend Hamel, with concrete examples of what metrics to evaluate and tools to use. I’ll focus on a more high-level overview of the metric process: first, establish clear criteria for success, then, try to find the right implementations of these metrics. This process is more nuanced than it might initially appear:\n\na) Identify metrics that matter for your specific use case.\nFor a customer service chatbot, you might care about “response conciseness” or “empathy scores.” However, it’s important to note that you can’t determine effective criteria purely through theorizing about possible failure modes for your task. You need to examine actual data—many LLM outputs—to get a true sense of what criteria you want to validate for LLM outputs for your specific task.\n\nLooking at data to inform your evaluation metrics is necessary because some metrics correspond to LLM idiosyncrasies or even quirks specific to your task. For example, you might notice that words like “delve” and “crucial” commonly give off a “GPT smell” that you want to avoid. In this case, you might implement a metric that checks for the absence of such words. These kinds of insights only emerge from careful examination of real outputs.\n\nOnce you’ve identified the right metrics for your use case, the next step is implementing them effectively:\n\nb) Implement these metrics.\nAs depicted in the “Binary Metrics” section of Figure 1, we can use both code-based metrics and LLM-based metrics (via prompts) for evaluation:\n\nSimple code functions to assess heuristics: For instance, character counting for conciseness.\n”LLM-as-a-judge”: Querying an LLM to evaluate a specific metric that you define. This approach is particularly useful for more subjective or complex criteria.\nWhen using LLM-as-a-judge, aligning the judge’s outputs with your own judgment can be challenging, especially for custom or bespoke tasks. Providing some examples of what you think are good and bad LLM outputs (with respect to your metric) in the judge’s prompt can significantly improve alignment.\n\nIt’s worth noting that binary metrics (True/False) are much easier to align and reason about from a UX standpoint. Many people reflexively turn to Likert scales or more fine-grained metrics for LLM judges, but it’s often better to start simple. Binary metrics are easier to align (you only need to agree on what constitutes True and False) and easier for humans to consistently judge, which is important for maintaining the quality of your evaluation data over time.\n\nc) Validating multi-step pipelines (i.e., “graphs” of LLM calls).\nIn complex LLM applications with interconnected “graphs” of LLM calls, we can think of validators as probes in software observability. Just as probes are strategically placed to monitor software behavior, validators should be placed in LLM graphs to assess the quality and correctness of intermediate and final outputs. For each type of node in an LLM graph, we want to probe its behavior differently. My friend Han proposed three node types that resonated with my experience. I found his categorization insightful and decided to expand upon it with my own observations from the field:\n\nLLM Graph Nodes\n\nFigure 2: LLM-based nodes in a customer support chatbot pipeline. Diamond nodes represent classifiers, hexagonal nodes represent codegen nodes, and the rectangular nodes represent LLM-as-writers. This diagram omits tool use (e.g., executing the generated SQL statement against a database; looking up keys in a knowledge base to find documents that might answer the user’s query) for clarity.\n\nLLM as Classifiers (State Transition Nodes; Diamond Nodes in Figure 2):\nExample: In a more complex customer support chatbot, a state transition node could use an LLM to classify user intents (e.g., “billing issue,” “technical support,” “general inquiry”) and route the conversation to the appropriate subgraph based on the predicted intent.\nFocus on assessing the correctness of decision-making—using metrics like accuracy, precision, recall, and F1 score.\nApply rule-based validation (e.g., if the user mentions “bill” or “payment,” the expected intent should be “billing issue”, and if the LLM’s predicted intent does not match the expected intent, flag the conversation for human review and potentially update few-shot examples and training data).\nLLM as Writers (Generative Nodes; Rectangular Nodes in Figure 2):\nThese metrics will be the most bespoke, as they are highly user-specific and task-specific.\nE.g., assess quality, coherence, and appropriateness of generated content. Can also metrics like perplexity, diversity, and adherence to task-specific constraints—depending on how actionable these are for the business.\nIn the customer support chatbot example, one could write an LLM-based validator to check adherence to the brand’s tone and voice guidelines (based on examples of the company branding).\nLLM as Compilers/Code Generators (Codegen Nodes; Hexagonal Nodes in Figure 2):\nExample: In the customer support chatbot, an LLM-as-codegen node could generate SQL queries to retrieve relevant information from a knowledge base based on the user’s intent and context.\nUse static code analysis, linters, and test suites to validate generated code.\nEmploy dynamic analysis techniques when applicable, such as automatically executing generated SQL statements against a database in a text-to-SQL LLM pipeline and verifying the output.\nPotentially use an LLM-based validator to determine if the output reasonably answer’s the user’s query.\nWhen implementing validators at every node in the graph of LLM calls, you can easily end up with many validators, and it’s not always clear which ones are most critical. By labeling validator outputs and computing per-step accuracy, you can trace how errors compound across the pipeline, providing valuable insights for targeted improvements. I think that comprehensive solutions for validating multi-step LLM pipelines are still an open research question, which I’ll discuss further later.\n\nd) Validate inputs as well as outputs.\nInput validation, illustrated on the left side of Figure 1, ensures that only appropriate queries are processed by the LLM. While the majority of literature focuses on validating LLM outputs, validating input data is also important for building robust LLM applications. This ensures that your pipeline handles only inputs it was designed for, rejecting or flagging out-of-distribution inputs.\n\nWhile data validation is well-explored for traditional ML, adapting these techniques for bespoke LLM pipelines with free-text inputs often requires equally bespoke validation approaches, in addition to general validation (e.g., language detection).\n\nMy general philosophy here mirrors Brendan Dolan-Gavitt’s, who says: “the best design principle seems to be Postel’s Law: be strict in what you send to the LLM, and liberal in what you accept as a valid response.” Let’s consider a customer support chatbot example to illustrate some input validation metrics:\n\nUser Authentication:\nMetric: Query relates only to the authenticated customer’s data\nImplementation: Compare entities mentioned in the query (e.g., order numbers, account details) against the customer’s record in your database\nTopic Relevance:\nMetric: Query relates to predefined support topics\nImplementation: a) Keyword matching: Check for presence of key terms related to your support areas b) Semantic similarity: Calculate cosine similarity between query embedding and embeddings of your support document set, setting a minimum threshold (e.g., 0.7)\nQuery Complexity:\nMetric: Query length and structure fall within expected ranges\nImplementation: Set acceptable ranges for token count, sentence count, and presence of specific syntactic structures\nLanguage Detection:\nMetric: Query is in a supported language, like English\nImplementation: Use a language detection library to ensure the query is in one of your supported languages\nSensitive Information Detection:\nMetric: Query does not contain unexpected sensitive information\nImplementation: Use regex patterns or named entity recognition to flag potential presence of sensitive data (e.g., credit card numbers, social security numbers)\nAdversarial Input Detection:\nMetric: Query does not attempt to manipulate the system\nImplementation: Check for known adversarial patterns, such as repeated attempts to access other users’ data or prompt injection attempts\nAnomaly Detection:\nMetric: Query is similar to recent queries\nImplementation: Compare the query’s embedding to embeddings of recent queries. If the similarity falls below a threshold, flag it for human review\nOne can also use LLM-based validators to validate inputs—which also raises the question of how to align the LLM with human judgment. I’ll discuss this in the next subsection.\n\n2. Monitoring: Operationalizing Metric Implementations\nOnce you’ve defined your metrics, the next challenge is ensuring they keep up with production data. This process should be as automated as possible, given the dynamic nature of LLM applications.\n\na) Automatically reassess if your chosen metrics still align with your goals.\nThe metric set that you come up with from the previous section is not meant to be static. When you deploy your application, you will learn of new failure modes, and you may want to update your metric set. Moreover, LLM APIs are constantly changing under the hood, and your ideal system behavior will evolve over time.\n\nOne idea to semi-automate the evolution of your metric set is to consider deploying an LLM “agent” that regularly analyzes a sample of labeled production data to determine if your metric set needs updating. This agent could:\n\nPropose new metrics (e.g., if customers consistently dislike certain phrasings)\nSuggest changes to metric definitions (e.g., realizing that conciseness is more subjective than a simple character count)\nRecommend removing metrics that don’t correlate with key performance indicators like accuracy or click-through rates\nIn the prompt, the LLM agent only needs samples of the production data and the current metric set. While this LLM agent can automatically run regularly, it’s probably important that an AI engineer actually verifies new metric sets before implementation.1\n\nWhile an LLM agent can help evolve your metric set over time, it’s equally important to ensure that your metric implementations also evolve.\n\nb) Keep metric implementations aligned with your evaluation criteria.\nAlignment has to be continually reassessed as production data drifts over time. This is particularly important for LLM-based metric evaluators. A more detailed workflow might look like this:\n\nFor each metric, regularly sample and label a set of responses from production data.\nStore these labeled examples in a database, preferably with timestamps for recency tracking. I know of some people who also store embedding-based indexes of their labeled examples.\nFor code-based metric implementations, schedule periodic manual reviews to ensure the code still captures the essence of the metric given recent data.\nFor LLM-based evaluations, keep the validator prompt dynamic:\nRetrieve examples from your database that are relevant to the instance being validated. The dynamic few-shot examples, highlighted in Figure 1, are retrieved based on input similarity.\nConsider an active learning-inspired approach: prioritize retrieving examples where human labels differed from what an LLM would predict. This helps focus on edge cases and areas of potential misalignment. I know of 2 people who do this (TBD on how much lift it provides over uniformly randomly sampling examples.)\nWeight similarity scores by label recency. Recent production data should have a higher likelihood of being selected as few-shot demonstrations compared to older data. Take this specific recency-weighting idea with a grain of salt–I don’t know of anyone who is doing this right now, but it seems important. (Interestingly, I know of one person who retrieves the most recent few-shot examples without looking at semantic similarity—mostly because there is low data volume, so semantic similarity may not help.)\nOne significant challenge in this process is ensuring regular human labeling of data for each metric. It’s important but can be burdensome. LangChain is exploring an interesting approach to this: using an LLM to label data by default, with humans able to edit these labels as they choose. While this definitely alleviates the labeling burden, it’s unclear how well the LLM labeler will align with human judgment over time–especially if engineers or team members are lazy and don’t want to verify labels. However, even if not perfect, this approach ensures a constant stream of recent, relevant examples for few-shot demonstrations2, which is valuable in and of itself.\n\n3. Continual Improvement: Closing the Loop\nWith robust evaluation and monitoring in place, the focus shifts to systematically improving your application. This process involves both (manual) human insight and automated techniques to enhance performance based on real-world usage. As illustrated by the “Logs” component in Figure 1, a prerequisite here is to maintain a comprehensive log of outputs and their associated metric scores; many LLMOps tools (e.g., LangSmith) can help with this.\n\na) Manually iterate on prompts or pipelines to enhance performance on your defined metrics.\nThe key here is leveraging the insights gained from your evaluation and monitoring processes. With well-defined metrics and continually evolving implementations, you can identify which production traces performed well and which didn’t, and learn from mistakes to improve prompts or other application components.\n\nSome strategies to consider:\n\nRegularly review the distribution of metric scores across your production data. Look for patterns or clusters of low-performing instances.\nDo analytics on inputs or query data distribution to observe patterns and changes in user behavior.\nTry A/B testing different prompt structures or pipeline configurations to empirically determine which performs better on your metrics.\nb) Automatically improve the pipeline in response to metrics.\nWhile human analysis described above is valuable, automating parts of the improvement process might improve efficiency. Here’s what I imagine could be a basic improvement strategy:\n\nRegularly review and “fix” low-scoring outputs. This can be done on a daily or weekly cadence, depending on your application’s scale and resources. The “fixing” process involves:\nIdentifying what went wrong in the original output\nRewriting the output to be correct\nDocumenting the changes made and the reasoning behind them (for other team members to learn from)\nImplement an active learning-inspired approach for continuous improvement:\nKeep a database of production traces along with their metric scores and any human-provided fixes.\nFor traces with low scores, prioritize them for manual review and fixing.\nDuring LLM inference at application runtime, retrieve the most similar “fixed” traces to the current query and include these as few-shot demonstrations in the prompt.3\nYou can experiment with different retrieval strategies, such as:\nIncluding both the most similar trace (regardless of its score or whether it was fixed) and the most similar fixed trace\nWeighting the retrieval by recency to favor more recent examples\nIncorporating a diversity measure to ensure a range of example types in your few-shot demonstrations\nIt’s worth noting that decomposing “good” outputs into multiple dimensions, rather than just having humans label traces holistically, offers several key benefits:\n\nBreaking down quality into specific metrics enables more accurate and consistent human labeling. Instead of making a single, high-level judgment, raters can focus on well-defined aspects like conciseness or politeness. This granularity promotes alignment and reduces subjectivity.\nAligning LLM validators becomes easier when they only need to check a single metric at a time. Trying to capture the full notion of “goodness” in one fell swoop is challenging, but validating individual dimensions is more tractable.\nHaving scores for each metric provides a more nuanced view of performance. If most outputs are failing due to a specific metric, it pinpoints the area needing improvement.\nSo while involving human labeling regardless, using multiple metrics brings rigor, consistency, and actionable insights to the continual improvement process. It allows for a more systematic and effective approach to refining LLM applications over time.\n\nLLM Applications: A New Set of Challenges\nThe framework I’ve outlined provides a solid foundation for building self-improving LLM applications. However, as we push the boundaries of what’s possible with these systems, new challenges emerge. Here are a couple of problems I’ve been thinking about in the LLMOps lifecycle, more from a research perspective:\n\nApplications of LLM Uncertainty Quantification to Aligning Validators\nOne key challenge is uncertainty quantification for LLM APIs, especially when dealing with custom tasks. Active learning, a powerful tool for improving ML models, becomes challenging with LLM APIs, where we lack meaningful probability estimates. Next-token probabilities from instruction-tuned models are uncalibrated and unhelpful when analyzing outputs at scale. Some ML research papers explore asking LLMs to simply output confidence scores after their outputs, but this doesn’t work all that well either. Of course, a straightforward solution is to fine-tune LLMs instead of using LLM APIs, but this ignores the premise—the simplicity of LLM APIs.\n\nRobust uncertainty estimates could significantly enhance our ability to align metric implementations, sample informative few-shot examples, and prioritize data for human labeling. While this is an open research question, advances in this area could directly strengthen the Evaluation and Continual Improvement components of our framework.\n\nData Flywheels for “Graphs” of LLM Calls\nAs LLM applications grow in complexity, we often deal with interconnected networks or “graphs” of LLM calls, representing multi-step reasoning processes. Some applications even put LLMs in loops! Implementing data flywheels for such systems presents unique challenges:\n\nErrors from one node can amplify through subsequent nodes, necessitating early detection and correction.\nWe need to assess the quality of both intermediate and final outputs. This can be a lot for a human to manually grade.\nGraphs are dynamic: LLM-based graphs might change structure based on inputs or intermediate results, complicating evaluation and improvement processes.\nTo address these challenges, key research directions include:\n\na) What does it mean to implement graph-aware evaluation metrics? Do we employ some form of hierarchical validation—where we validate individual LLM calls or nodes, walks or subgraphs, and then entire graphs? These validation probes should probably be different for different types of nodes, but what are the types and how should validation differ?\n\nb) How can we design systems that dynamically insert validation probes at critical junctures in the graph? This approach could involve developing algorithms that identify high-risk nodes or transitions based on historical performance data or uncertainty measurements—or causal analysis techniques. Ideally, we can catch and correct errors early, before they propagate through the system.\n\nc) Can we extend the concept of dynamic few-shot learning to graph structures? This might involve developing techniques to retrieve relevant subgraph traces based on the current graph configuration and task. We could explore using graph-aware embeddings or subgraph matching algorithms to find similar patterns in historical data.\n\nDatabase-Driven LLM Pipeline Validation\nI am curious about integrating validators directly into database systems. Currently, it’s a lot of work for application developers to implement logging and validation of all traces. It’s challenging to implement few-shot retrieval, ensure all validators (especially LLM-powered validators) complete successfully, write the results to the DB, etc., in real-time or even in the application background—especially given that this can require significant computational resources or take a long time to run. Ideally, the database can do this for us.\n\nKey questions I’ve been thinking about include:\n\na) Implementing validators as database triggers or stored procedures: This allows for automatic execution of both code-based and LLM-based validators whenever new data is inserted or updated. How can we design these triggers to be efficient and not significantly impact database performance?\n\nb) Using database views for flexible metric computation: By implementing metrics as views rather than computing all metrics (which could be costly, especially if LLMs are evaluating metrics), we can potentially save resources. What are the trade-offs between on-demand computation and materialized views for different types of metrics?\n\nc) Incremental maintenance of semantic indexes of logs: For efficient retrieval of few-shot examples, we need to maintain up-to-date semantic similarity indexes for LLM pipeline traces. Are there simple incremental update strategies to keep these indexes current without full rebuilds?\n\nWrapping Up\nBuilding self-improving LLM applications is about applying a systematic approach, focusing on what truly matters for your use case, and iterating based on real-world performance. The framework I outlined is intentionally simple: it doesn’t rely on sophisticated tools, ML platforms, or enterprise-grade monitoring systems. Instead, it emphasizes thoughtful metric selection, consistent monitoring, and data-driven improvements.\n\nAt the end of the day, people (myself included) won’t be happy until all production data is maximally leveraged for future pipeline runs—and this hopefully-simple approach makes a big leap in turning every production trace into an opportunity for refinement and enhancement. Of course, there is still work and research to do—e.g., LLM-as-a-judge is not perfect, and LLM applications can be pretty complex, possibly requiring different validation techniques.\n\nZooming out a bit—I think it is such an exciting time to be building intelligent software applications. Self-improving applications are actually possible to build right now, and there are still many questions to answer. But I am most excited because the path forward will be collaborative and interdisciplinary—uniting industry practitioners, researchers, and experts across various fields and subareas of computer science—as we chart the future of engineering intelligent software.\n\nThanks to my friend Han Lee for his valuable feedback and insights on an earlier draft of this post. I also thank my advisor Aditya Parameswaran for suggesting the idea of a detailed blog post on our recent LLMOps research, and my colleague Parth Asawa for inspiring the narrative around the data flywheel concept.\n\nIf you are interested in collaborating on any of these research projects, please feel free to reach out to me at shreyashankar@berkeley.edu. Include your CV and specify the project you are interested in, along with any initial thoughts or ideas you might have.\n\nFootnotes\nWhile I have not seen any agent that does exactly what I have outlined above, Alta, a startup that I work with, employs an agent to automatically suggest changes to metric definitions. I think Alta’s approach is awesome and allows them to be ultra-personalized (i.e., have a different metric set per end-user), and I am excited to see how it evolves over time. ↩\n\nHan raised a good point here while reviewing the blog post: this approach might not be kosher for some industries, e.g., what if the legal team wants to “sign off” on prompts before they go into production? Or, what’s the risk of prompt poisoning here—can someone adversarially write queries that get included in the prompt as few-shot examples? ↩\n\nI really like this blog post (authored by my former colleague Devin Stein), which champions dynamically fetching examples for the prompt at runtime. At least 5 people that I know are doing something similar. Interestingly, this architecture shifts the prompt engineering problem to a retrieval problem—how do you retrieve examples that are (1) most informative for the LLM, and (2) most correlated with what the end-user might want?",
  "sender": "human",
  "created_at": "2024-09-27T02:20:33.481508Z",
  "updated_at": "2024-09-27T02:20:33.481508Z",
  "attachments": [],
  "files": []
}

## 4

{
  "uuid": "ee4482ed-e9e5-4e2d-b896-76dbf7e8f1f1",
  "text": "Please read the following, then tell me how flywheels might relate or be useful to the Transmissions project: Shreya Shankar\nHome Papers Blog\nJul 1, 2024\n|\n20 min read\nData Flywheels for LLM Applications\nTable of Contents\nA Framework for Creating a Flywheel\n1. Evaluation: Defining Success Metrics\na) Identify metrics that matter for your specific use case.\nb) Implement these metrics.\nc) Validating multi-step pipelines (i.e., “graphs” of LLM calls).\nd) Validate inputs as well as outputs.\n2. Monitoring: Operationalizing Metric Implementations\na) Automatically reassess if your chosen metrics still align with your goals.\nb) Keep metric implementations aligned with your evaluation criteria.\n3. Continual Improvement: Closing the Loop\na) Manually iterate on prompts or pipelines to enhance performance on your defined metrics.\nb) Automatically improve the pipeline in response to metrics.\nLLM Applications: A New Set of Challenges\nApplications of LLM Uncertainty Quantification to Aligning Validators\nData Flywheels for “Graphs” of LLM Calls\nDatabase-Driven LLM Pipeline Validation\nWrapping Up\nFootnotes\nOver the past few months, I have been thinking a lot about workflows to automatically and dynamically improve LLM applications using production data. This stems from our research on validating data quality in LLM pipelines and applications—which is starting to be productionized in both vertical AI applications and LLMOps companies. (I am always very thankful to the teams in industry who find my work useful and are open to collaborating.)\n\nMy ideas for data flywheels are grounded in several observations:\n\nHumans need to be in the loop for evaluation regularly, as human preferences on LLM outputs change over time.\nFine-tuning models come with significant overhead, and many teams prefer LLM APIs for rapid iteration and simple deployment. LLM APIs do not always listen to instructions in the prompt, especially over large batches of inputs, and there thus needs to be validators on LLM outputs.\nLLM-as-a-judge is increasingly popular. LLMs are getting cheaper, better, and faster.\nEmpirical evidence shows that few-shot examples are most effective for improving prompt performance. As such, LLM-as-a-judge can be aligned with human preferences—given the right few-shot examples.\nIn this post, I’ll outline a three-part framework for approaching this problem. Before we dive into the details of each part, let’s take a look at an overview of the entire LLM application pipeline:\n\nData Flywheels\n\nFigure 1: Continually-Evolving LLM Application Pipeline, adapted from the source of Figure 1 in this paper.\n\nThis diagram illustrates my (idealized) architecture of an LLM pipeline, from input processing through evaluation and logging. It showcases ideas I’ll discuss throughout the blog post, such as input validation, dynamic few-shot example retrieval, and the use of both code-based and LLM-based metrics for evaluation.\n\nNow, let’s break down this process into the main parts: Evaluation, Monitoring, and Continual Improvement.\n\nA Framework for Creating a Flywheel\n1. Evaluation: Defining Success Metrics\nThere are several great resources specifically on evaluation. Here’s one I really like from my friend Hamel, with concrete examples of what metrics to evaluate and tools to use. I’ll focus on a more high-level overview of the metric process: first, establish clear criteria for success, then, try to find the right implementations of these metrics. This process is more nuanced than it might initially appear:\n\na) Identify metrics that matter for your specific use case.\nFor a customer service chatbot, you might care about “response conciseness” or “empathy scores.” However, it’s important to note that you can’t determine effective criteria purely through theorizing about possible failure modes for your task. You need to examine actual data—many LLM outputs—to get a true sense of what criteria you want to validate for LLM outputs for your specific task.\n\nLooking at data to inform your evaluation metrics is necessary because some metrics correspond to LLM idiosyncrasies or even quirks specific to your task. For example, you might notice that words like “delve” and “crucial” commonly give off a “GPT smell” that you want to avoid. In this case, you might implement a metric that checks for the absence of such words. These kinds of insights only emerge from careful examination of real outputs.\n\nOnce you’ve identified the right metrics for your use case, the next step is implementing them effectively:\n\nb) Implement these metrics.\nAs depicted in the “Binary Metrics” section of Figure 1, we can use both code-based metrics and LLM-based metrics (via prompts) for evaluation:\n\nSimple code functions to assess heuristics: For instance, character counting for conciseness.\n”LLM-as-a-judge”: Querying an LLM to evaluate a specific metric that you define. This approach is particularly useful for more subjective or complex criteria.\nWhen using LLM-as-a-judge, aligning the judge’s outputs with your own judgment can be challenging, especially for custom or bespoke tasks. Providing some examples of what you think are good and bad LLM outputs (with respect to your metric) in the judge’s prompt can significantly improve alignment.\n\nIt’s worth noting that binary metrics (True/False) are much easier to align and reason about from a UX standpoint. Many people reflexively turn to Likert scales or more fine-grained metrics for LLM judges, but it’s often better to start simple. Binary metrics are easier to align (you only need to agree on what constitutes True and False) and easier for humans to consistently judge, which is important for maintaining the quality of your evaluation data over time.\n\nc) Validating multi-step pipelines (i.e., “graphs” of LLM calls).\nIn complex LLM applications with interconnected “graphs” of LLM calls, we can think of validators as probes in software observability. Just as probes are strategically placed to monitor software behavior, validators should be placed in LLM graphs to assess the quality and correctness of intermediate and final outputs. For each type of node in an LLM graph, we want to probe its behavior differently. My friend Han proposed three node types that resonated with my experience. I found his categorization insightful and decided to expand upon it with my own observations from the field:\n\nLLM Graph Nodes\n\nFigure 2: LLM-based nodes in a customer support chatbot pipeline. Diamond nodes represent classifiers, hexagonal nodes represent codegen nodes, and the rectangular nodes represent LLM-as-writers. This diagram omits tool use (e.g., executing the generated SQL statement against a database; looking up keys in a knowledge base to find documents that might answer the user’s query) for clarity.\n\nLLM as Classifiers (State Transition Nodes; Diamond Nodes in Figure 2):\nExample: In a more complex customer support chatbot, a state transition node could use an LLM to classify user intents (e.g., “billing issue,” “technical support,” “general inquiry”) and route the conversation to the appropriate subgraph based on the predicted intent.\nFocus on assessing the correctness of decision-making—using metrics like accuracy, precision, recall, and F1 score.\nApply rule-based validation (e.g., if the user mentions “bill” or “payment,” the expected intent should be “billing issue”, and if the LLM’s predicted intent does not match the expected intent, flag the conversation for human review and potentially update few-shot examples and training data).\nLLM as Writers (Generative Nodes; Rectangular Nodes in Figure 2):\nThese metrics will be the most bespoke, as they are highly user-specific and task-specific.\nE.g., assess quality, coherence, and appropriateness of generated content. Can also metrics like perplexity, diversity, and adherence to task-specific constraints—depending on how actionable these are for the business.\nIn the customer support chatbot example, one could write an LLM-based validator to check adherence to the brand’s tone and voice guidelines (based on examples of the company branding).\nLLM as Compilers/Code Generators (Codegen Nodes; Hexagonal Nodes in Figure 2):\nExample: In the customer support chatbot, an LLM-as-codegen node could generate SQL queries to retrieve relevant information from a knowledge base based on the user’s intent and context.\nUse static code analysis, linters, and test suites to validate generated code.\nEmploy dynamic analysis techniques when applicable, such as automatically executing generated SQL statements against a database in a text-to-SQL LLM pipeline and verifying the output.\nPotentially use an LLM-based validator to determine if the output reasonably answer’s the user’s query.\nWhen implementing validators at every node in the graph of LLM calls, you can easily end up with many validators, and it’s not always clear which ones are most critical. By labeling validator outputs and computing per-step accuracy, you can trace how errors compound across the pipeline, providing valuable insights for targeted improvements. I think that comprehensive solutions for validating multi-step LLM pipelines are still an open research question, which I’ll discuss further later.\n\nd) Validate inputs as well as outputs.\nInput validation, illustrated on the left side of Figure 1, ensures that only appropriate queries are processed by the LLM. While the majority of literature focuses on validating LLM outputs, validating input data is also important for building robust LLM applications. This ensures that your pipeline handles only inputs it was designed for, rejecting or flagging out-of-distribution inputs.\n\nWhile data validation is well-explored for traditional ML, adapting these techniques for bespoke LLM pipelines with free-text inputs often requires equally bespoke validation approaches, in addition to general validation (e.g., language detection).\n\nMy general philosophy here mirrors Brendan Dolan-Gavitt’s, who says: “the best design principle seems to be Postel’s Law: be strict in what you send to the LLM, and liberal in what you accept as a valid response.” Let’s consider a customer support chatbot example to illustrate some input validation metrics:\n\nUser Authentication:\nMetric: Query relates only to the authenticated customer’s data\nImplementation: Compare entities mentioned in the query (e.g., order numbers, account details) against the customer’s record in your database\nTopic Relevance:\nMetric: Query relates to predefined support topics\nImplementation: a) Keyword matching: Check for presence of key terms related to your support areas b) Semantic similarity: Calculate cosine similarity between query embedding and embeddings of your support document set, setting a minimum threshold (e.g., 0.7)\nQuery Complexity:\nMetric: Query length and structure fall within expected ranges\nImplementation: Set acceptable ranges for token count, sentence count, and presence of specific syntactic structures\nLanguage Detection:\nMetric: Query is in a supported language, like English\nImplementation: Use a language detection library to ensure the query is in one of your supported languages\nSensitive Information Detection:\nMetric: Query does not contain unexpected sensitive information\nImplementation: Use regex patterns or named entity recognition to flag potential presence of sensitive data (e.g., credit card numbers, social security numbers)\nAdversarial Input Detection:\nMetric: Query does not attempt to manipulate the system\nImplementation: Check for known adversarial patterns, such as repeated attempts to access other users’ data or prompt injection attempts\nAnomaly Detection:\nMetric: Query is similar to recent queries\nImplementation: Compare the query’s embedding to embeddings of recent queries. If the similarity falls below a threshold, flag it for human review\nOne can also use LLM-based validators to validate inputs—which also raises the question of how to align the LLM with human judgment. I’ll discuss this in the next subsection.\n\n2. Monitoring: Operationalizing Metric Implementations\nOnce you’ve defined your metrics, the next challenge is ensuring they keep up with production data. This process should be as automated as possible, given the dynamic nature of LLM applications.\n\na) Automatically reassess if your chosen metrics still align with your goals.\nThe metric set that you come up with from the previous section is not meant to be static. When you deploy your application, you will learn of new failure modes, and you may want to update your metric set. Moreover, LLM APIs are constantly changing under the hood, and your ideal system behavior will evolve over time.\n\nOne idea to semi-automate the evolution of your metric set is to consider deploying an LLM “agent” that regularly analyzes a sample of labeled production data to determine if your metric set needs updating. This agent could:\n\nPropose new metrics (e.g., if customers consistently dislike certain phrasings)\nSuggest changes to metric definitions (e.g., realizing that conciseness is more subjective than a simple character count)\nRecommend removing metrics that don’t correlate with key performance indicators like accuracy or click-through rates\nIn the prompt, the LLM agent only needs samples of the production data and the current metric set. While this LLM agent can automatically run regularly, it’s probably important that an AI engineer actually verifies new metric sets before implementation.1\n\nWhile an LLM agent can help evolve your metric set over time, it’s equally important to ensure that your metric implementations also evolve.\n\nb) Keep metric implementations aligned with your evaluation criteria.\nAlignment has to be continually reassessed as production data drifts over time. This is particularly important for LLM-based metric evaluators. A more detailed workflow might look like this:\n\nFor each metric, regularly sample and label a set of responses from production data.\nStore these labeled examples in a database, preferably with timestamps for recency tracking. I know of some people who also store embedding-based indexes of their labeled examples.\nFor code-based metric implementations, schedule periodic manual reviews to ensure the code still captures the essence of the metric given recent data.\nFor LLM-based evaluations, keep the validator prompt dynamic:\nRetrieve examples from your database that are relevant to the instance being validated. The dynamic few-shot examples, highlighted in Figure 1, are retrieved based on input similarity.\nConsider an active learning-inspired approach: prioritize retrieving examples where human labels differed from what an LLM would predict. This helps focus on edge cases and areas of potential misalignment. I know of 2 people who do this (TBD on how much lift it provides over uniformly randomly sampling examples.)\nWeight similarity scores by label recency. Recent production data should have a higher likelihood of being selected as few-shot demonstrations compared to older data. Take this specific recency-weighting idea with a grain of salt–I don’t know of anyone who is doing this right now, but it seems important. (Interestingly, I know of one person who retrieves the most recent few-shot examples without looking at semantic similarity—mostly because there is low data volume, so semantic similarity may not help.)\nOne significant challenge in this process is ensuring regular human labeling of data for each metric. It’s important but can be burdensome. LangChain is exploring an interesting approach to this: using an LLM to label data by default, with humans able to edit these labels as they choose. While this definitely alleviates the labeling burden, it’s unclear how well the LLM labeler will align with human judgment over time–especially if engineers or team members are lazy and don’t want to verify labels. However, even if not perfect, this approach ensures a constant stream of recent, relevant examples for few-shot demonstrations2, which is valuable in and of itself.\n\n3. Continual Improvement: Closing the Loop\nWith robust evaluation and monitoring in place, the focus shifts to systematically improving your application. This process involves both (manual) human insight and automated techniques to enhance performance based on real-world usage. As illustrated by the “Logs” component in Figure 1, a prerequisite here is to maintain a comprehensive log of outputs and their associated metric scores; many LLMOps tools (e.g., LangSmith) can help with this.\n\na) Manually iterate on prompts or pipelines to enhance performance on your defined metrics.\nThe key here is leveraging the insights gained from your evaluation and monitoring processes. With well-defined metrics and continually evolving implementations, you can identify which production traces performed well and which didn’t, and learn from mistakes to improve prompts or other application components.\n\nSome strategies to consider:\n\nRegularly review the distribution of metric scores across your production data. Look for patterns or clusters of low-performing instances.\nDo analytics on inputs or query data distribution to observe patterns and changes in user behavior.\nTry A/B testing different prompt structures or pipeline configurations to empirically determine which performs better on your metrics.\nb) Automatically improve the pipeline in response to metrics.\nWhile human analysis described above is valuable, automating parts of the improvement process might improve efficiency. Here’s what I imagine could be a basic improvement strategy:\n\nRegularly review and “fix” low-scoring outputs. This can be done on a daily or weekly cadence, depending on your application’s scale and resources. The “fixing” process involves:\nIdentifying what went wrong in the original output\nRewriting the output to be correct\nDocumenting the changes made and the reasoning behind them (for other team members to learn from)\nImplement an active learning-inspired approach for continuous improvement:\nKeep a database of production traces along with their metric scores and any human-provided fixes.\nFor traces with low scores, prioritize them for manual review and fixing.\nDuring LLM inference at application runtime, retrieve the most similar “fixed” traces to the current query and include these as few-shot demonstrations in the prompt.3\nYou can experiment with different retrieval strategies, such as:\nIncluding both the most similar trace (regardless of its score or whether it was fixed) and the most similar fixed trace\nWeighting the retrieval by recency to favor more recent examples\nIncorporating a diversity measure to ensure a range of example types in your few-shot demonstrations\nIt’s worth noting that decomposing “good” outputs into multiple dimensions, rather than just having humans label traces holistically, offers several key benefits:\n\nBreaking down quality into specific metrics enables more accurate and consistent human labeling. Instead of making a single, high-level judgment, raters can focus on well-defined aspects like conciseness or politeness. This granularity promotes alignment and reduces subjectivity.\nAligning LLM validators becomes easier when they only need to check a single metric at a time. Trying to capture the full notion of “goodness” in one fell swoop is challenging, but validating individual dimensions is more tractable.\nHaving scores for each metric provides a more nuanced view of performance. If most outputs are failing due to a specific metric, it pinpoints the area needing improvement.\nSo while involving human labeling regardless, using multiple metrics brings rigor, consistency, and actionable insights to the continual improvement process. It allows for a more systematic and effective approach to refining LLM applications over time.\n\nLLM Applications: A New Set of Challenges\nThe framework I’ve outlined provides a solid foundation for building self-improving LLM applications. However, as we push the boundaries of what’s possible with these systems, new challenges emerge. Here are a couple of problems I’ve been thinking about in the LLMOps lifecycle, more from a research perspective:\n\nApplications of LLM Uncertainty Quantification to Aligning Validators\nOne key challenge is uncertainty quantification for LLM APIs, especially when dealing with custom tasks. Active learning, a powerful tool for improving ML models, becomes challenging with LLM APIs, where we lack meaningful probability estimates. Next-token probabilities from instruction-tuned models are uncalibrated and unhelpful when analyzing outputs at scale. Some ML research papers explore asking LLMs to simply output confidence scores after their outputs, but this doesn’t work all that well either. Of course, a straightforward solution is to fine-tune LLMs instead of using LLM APIs, but this ignores the premise—the simplicity of LLM APIs.\n\nRobust uncertainty estimates could significantly enhance our ability to align metric implementations, sample informative few-shot examples, and prioritize data for human labeling. While this is an open research question, advances in this area could directly strengthen the Evaluation and Continual Improvement components of our framework.\n\nData Flywheels for “Graphs” of LLM Calls\nAs LLM applications grow in complexity, we often deal with interconnected networks or “graphs” of LLM calls, representing multi-step reasoning processes. Some applications even put LLMs in loops! Implementing data flywheels for such systems presents unique challenges:\n\nErrors from one node can amplify through subsequent nodes, necessitating early detection and correction.\nWe need to assess the quality of both intermediate and final outputs. This can be a lot for a human to manually grade.\nGraphs are dynamic: LLM-based graphs might change structure based on inputs or intermediate results, complicating evaluation and improvement processes.\nTo address these challenges, key research directions include:\n\na) What does it mean to implement graph-aware evaluation metrics? Do we employ some form of hierarchical validation—where we validate individual LLM calls or nodes, walks or subgraphs, and then entire graphs? These validation probes should probably be different for different types of nodes, but what are the types and how should validation differ?\n\nb) How can we design systems that dynamically insert validation probes at critical junctures in the graph? This approach could involve developing algorithms that identify high-risk nodes or transitions based on historical performance data or uncertainty measurements—or causal analysis techniques. Ideally, we can catch and correct errors early, before they propagate through the system.\n\nc) Can we extend the concept of dynamic few-shot learning to graph structures? This might involve developing techniques to retrieve relevant subgraph traces based on the current graph configuration and task. We could explore using graph-aware embeddings or subgraph matching algorithms to find similar patterns in historical data.\n\nDatabase-Driven LLM Pipeline Validation\nI am curious about integrating validators directly into database systems. Currently, it’s a lot of work for application developers to implement logging and validation of all traces. It’s challenging to implement few-shot retrieval, ensure all validators (especially LLM-powered validators) complete successfully, write the results to the DB, etc., in real-time or even in the application background—especially given that this can require significant computational resources or take a long time to run. Ideally, the database can do this for us.\n\nKey questions I’ve been thinking about include:\n\na) Implementing validators as database triggers or stored procedures: This allows for automatic execution of both code-based and LLM-based validators whenever new data is inserted or updated. How can we design these triggers to be efficient and not significantly impact database performance?\n\nb) Using database views for flexible metric computation: By implementing metrics as views rather than computing all metrics (which could be costly, especially if LLMs are evaluating metrics), we can potentially save resources. What are the trade-offs between on-demand computation and materialized views for different types of metrics?\n\nc) Incremental maintenance of semantic indexes of logs: For efficient retrieval of few-shot examples, we need to maintain up-to-date semantic similarity indexes for LLM pipeline traces. Are there simple incremental update strategies to keep these indexes current without full rebuilds?\n\nWrapping Up\nBuilding self-improving LLM applications is about applying a systematic approach, focusing on what truly matters for your use case, and iterating based on real-world performance. The framework I outlined is intentionally simple: it doesn’t rely on sophisticated tools, ML platforms, or enterprise-grade monitoring systems. Instead, it emphasizes thoughtful metric selection, consistent monitoring, and data-driven improvements.\n\nAt the end of the day, people (myself included) won’t be happy until all production data is maximally leveraged for future pipeline runs—and this hopefully-simple approach makes a big leap in turning every production trace into an opportunity for refinement and enhancement. Of course, there is still work and research to do—e.g., LLM-as-a-judge is not perfect, and LLM applications can be pretty complex, possibly requiring different validation techniques.\n\nZooming out a bit—I think it is such an exciting time to be building intelligent software applications. Self-improving applications are actually possible to build right now, and there are still many questions to answer. But I am most excited because the path forward will be collaborative and interdisciplinary—uniting industry practitioners, researchers, and experts across various fields and subareas of computer science—as we chart the future of engineering intelligent software.\n\nThanks to my friend Han Lee for his valuable feedback and insights on an earlier draft of this post. I also thank my advisor Aditya Parameswaran for suggesting the idea of a detailed blog post on our recent LLMOps research, and my colleague Parth Asawa for inspiring the narrative around the data flywheel concept.\n\nIf you are interested in collaborating on any of these research projects, please feel free to reach out to me at shreyashankar@berkeley.edu. Include your CV and specify the project you are interested in, along with any initial thoughts or ideas you might have.\n\nFootnotes\nWhile I have not seen any agent that does exactly what I have outlined above, Alta, a startup that I work with, employs an agent to automatically suggest changes to metric definitions. I think Alta’s approach is awesome and allows them to be ultra-personalized (i.e., have a different metric set per end-user), and I am excited to see how it evolves over time. ↩\n\nHan raised a good point here while reviewing the blog post: this approach might not be kosher for some industries, e.g., what if the legal team wants to “sign off” on prompts before they go into production? Or, what’s the risk of prompt poisoning here—can someone adversarially write queries that get included in the prompt as few-shot examples? ↩\n\nI really like this blog post (authored by my former colleague Devin Stein), which champions dynamically fetching examples for the prompt at runtime. At least 5 people that I know are doing something similar. Interestingly, this architecture shifts the prompt engineering problem to a retrieval problem—how do you retrieve examples that are (1) most informative for the LLM, and (2) most correlated with what the end-user might want?",
  "sender": "human",
  "created_at": "2024-09-27T02:22:20.532778Z",
  "updated_at": "2024-09-27T02:22:20.532778Z",
  "attachments": [],
  "files": []
}

## 5

{
  "uuid": "58519ab6-2907-4437-8d83-04e3673271e5",
  "text": " The concept of data flywheels for LLM applications could be highly relevant and beneficial for the Transmissions project. Here's how it might relate and be applied:\n\n1. Evaluation Metrics:\n   - Define specific success metrics for each service in the Transmissions pipeline.\n   - Implement both code-based and LLM-based validators to assess these metrics.\n   - Use binary metrics (True/False) for easier alignment and reasoning.\n\n2. Multi-step Pipeline Validation:\n   - Treat each service in Transmissions as a node in an LLM graph.\n   - Implement validators at critical points in the pipeline to catch and correct errors early.\n   - Categorize services into different node types (e.g., classifiers, writers, code generators) and tailor validation approaches accordingly.\n\n3. Input Validation:\n   - Add input validators to ensure only appropriate data enters the pipeline.\n   - Implement checks for topic relevance, query complexity, and anomaly detection.\n\n4. Monitoring and Metric Evolution:\n   - Set up a system to continuously monitor the performance of each service.\n   - Regularly reassess if the chosen metrics still align with project goals.\n   - Consider implementing an LLM agent to suggest metric updates based on production data.\n\n5. Dynamic Few-shot Learning:\n   - Implement a system to dynamically retrieve relevant examples for each service based on input similarity.\n   - Store labeled examples in a database with timestamps for recency tracking.\n   - Experiment with different retrieval strategies, including weighting by recency and incorporating diversity.\n\n6. Continual Improvement:\n   - Use insights from monitoring to iteratively improve prompts and pipeline configurations.\n   - Implement an automated system to update few-shot examples based on successful outputs.\n   - Regularly review and \"fix\" low-scoring outputs, documenting changes for team learning.\n\n7. Logging and Analysis:\n   - Set up comprehensive logging of inputs, outputs, and metric scores for each service.\n   - Use this data to identify patterns and areas for improvement.\n\n8. Database Integration:\n   - Consider integrating validators directly into the database system used by Transmissions.\n   - Explore implementing validators as database triggers or stored procedures.\n   - Look into using database views for flexible metric computation.\n\nBy implementing these concepts, the Transmissions project could create a self-improving system that continually enhances its performance based on real-world usage, leading to more robust and effective data processing pipelines.\n\nQ1: How might we adapt the concept of LLM-as-a-judge for validating outputs in the Transmissions project?\nQ2: What strategies could we use to efficiently store and retrieve relevant few-shot examples for each service in the Transmissions pipeline?\nQ3: How could we implement a system for automatically updating and improving prompts based on successful outputs in Transmissions?",
  "sender": "assistant",
  "created_at": "2024-09-27T02:22:20.532778Z",
  "updated_at": "2024-09-27T02:22:20.532778Z",
  "attachments": [],
  "files": []
}

## 6

{
  "uuid": "5d29e1ba-d52f-4fb7-b901-b35abb90dc4d",
  "text": "Q3",
  "sender": "human",
  "created_at": "2024-09-27T02:23:41.017464Z",
  "updated_at": "2024-09-27T02:23:41.017464Z",
  "attachments": [],
  "files": []
}

## 7

{
  "uuid": "92ad538e-4f6b-4663-9c43-1a3170c11d87",
  "text": " To implement a system for automatically updating and improving prompts based on successful outputs in Transmissions, we could follow these steps:\n\n1. Logging and Scoring:\n   // Log all inputs, outputs, and their performance scores\n   function logServiceExecution(serviceId, input, output, score) {\n     database.insert('service_logs', {\n       serviceId,\n       input,\n       output,\n       score,\n       timestamp: Date.now()\n     });\n   }\n\n2. Identifying High-Performing Outputs:\n   // Query the database for recent high-scoring outputs\n   async function getTopPerformers(serviceId, limit = 10) {\n     return database.query(`\n       SELECT * FROM service_logs\n       WHERE serviceId = ? AND score > threshold\n       ORDER BY score DESC, timestamp DESC\n       LIMIT ?\n     `, [serviceId, limit]);\n   }\n\n3. Extracting Patterns:\n   // Use an LLM to analyze top-performing outputs and extract patterns\n   async function extractPatterns(topPerformers) {\n     const prompt = `Analyze these high-performing outputs and extract common patterns or strategies:\n       ${JSON.stringify(topPerformers)}\n       Provide a summary of key patterns found.`;\n     return await llmClient.complete(prompt);\n   }\n\n4. Generating Improved Prompts:\n   // Use extracted patterns to generate an improved prompt\n   async function generateImprovedPrompt(serviceId, currentPrompt, patterns) {\n     const prompt = `Current prompt: \"${currentPrompt}\"\n       Patterns from successful outputs: ${patterns}\n       Generate an improved prompt incorporating these patterns.`;\n     return await llmClient.complete(prompt);\n   }\n\n5. Validation and A/B Testing:\n   // Test the new prompt against the current one\n   async function testNewPrompt(serviceId, currentPrompt, newPrompt) {\n     const testCases = await getRecentInputs(serviceId, 100);\n     const currentResults = await runPrompt(currentPrompt, testCases);\n     const newResults = await runPrompt(newPrompt, testCases);\n     return compareResults(currentResults, newResults);\n   }\n\n6. Prompt Update Workflow:\n   async function updatePrompt(serviceId) {\n     const currentPrompt = await getCurrentPrompt(serviceId);\n     const topPerformers = await getTopPerformers(serviceId);\n     const patterns = await extractPatterns(topPerformers);\n     const newPrompt = await generateImprovedPrompt(serviceId, currentPrompt, patterns);\n     const testResults = await testNewPrompt(serviceId, currentPrompt, newPrompt);\n     \n     if (testResults.improvement > threshold) {\n       await updateServicePrompt(serviceId, newPrompt);\n       logPromptUpdate(serviceId, currentPrompt, newPrompt, testResults);\n     }\n   }\n\n7. Scheduling Regular Updates:\n   // Run the update process periodically\n   function schedulePromptUpdates() {\n     const services = getActiveServices();\n     services.forEach(serviceId => {\n       setInterval(() => updatePrompt(serviceId), 24 * 60 * 60 * 1000); // Daily\n     });\n   }\n\nThis system provides a framework for automatically improving prompts based on successful outputs. It involves logging performance, identifying top performers, extracting patterns, generating new prompts, and validating improvements through A/B testing.\n\nQ1: How could we integrate this prompt improvement system with the existing Transmissions architecture?\nQ2: What safeguards should we implement to prevent potential degradation of performance due to automatic prompt updates?\nQ3: How might we extend this system to handle multi-step pipelines where the output of one service affects the input of another?",
  "sender": "assistant",
  "created_at": "2024-09-27T02:23:41.017464Z",
  "updated_at": "2024-09-27T02:23:41.017464Z",
  "attachments": [],
  "files": []
}
