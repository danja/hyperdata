<!DOCTYPE html>
<html lang="en">

<head>
    <title>Tensegrity</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- #:todo remove when stable -->
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">

    <link rel="stylesheet" href="/css/fonts.css" type="text/css" />
    <link rel="stylesheet" href="/css/grid-columns.css" type="text/css" />
    <link rel="stylesheet" href="/css/style.css" type="text/css" />
    <link rel="stylesheet" href="/css/menu.css" type="text/css" />

</head>

<body>
    <header id="main-header">
        <h1 class="h-title">
           The Tensegrity Stack
        </h1>
    </header>
    <div class="grid-container">
        <div class="main-grid-item directory">
            <p><strong><a href="https://github.com/danja/tensegrity">GitHub</a></strong></p>
            <p></p>
        </div>
        <div class="main-grid-item articles">
            <article>
                <!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Semem, Know Thyself</h1>
<p>In use, <a href="https://github.com/danja/semem/">Semem</a> is mostly intended to manage memories accumulated on the fly, relevant to the task at hand. A combination of uploaded docs, interaction history and inferences made. But background knowledge is desirable. Ok, there will be plenty provided by the LLM in use by Semem and there are external connectors to query Wikipedia and Wikidata. But that leaves the question of more local, maybe project-specific info. How does that get into the system? What if we want Semem to be aware of its own documentation?</p>
<h2>SPARQL Store as Integration Point</h2>
<p>Semem has facilities for querying SPARQL stores - it&#39;s a big part of the core functionality, that&#39;s where the knowledge graphs live. So if the material of interest can be placed in such a store, it should be relatively straightforward to access.</p>
<p>Another project in the <a href="https://github.com/danja/tensegrity">Tensegrity Stack</a> is <a href="https://github.com/danja/transmissions">Transmissions</a>, my pipeliney thing. And guess what, I&#39;ve already got a pipeline for walking a directory tree, looking for markdown files and POSTing them into a SPARQL store.</p>
<p>Claude Code has helped me put together the code to pull data from a remote SPARQL store into that of Semem. In this particular instance Semem is using a local Fuseki store, the same one I&#39;m using with Transmissions locally. Named graphs are used to keep things independent. This does mean there&#39;ll be redundancy, but I think it&#39;ll be worth it for the sake of loose coupling. </p>
<pre><code class="language-sh">node examples/ingestion/SPARQLIngest.js --endpoint &quot;http://localhost:3030/semem/query&quot; --template blog-articles --graph &quot;http://tensegrity.it/semem&quot;
</code></pre>
<h2>Markdown to SPARQL Store</h2>
<p>Note to self, these are the bits I&#39;ve created (in the Semem codebase) for the Transmissions side of the operation.</p>
<ul>
<li><code>./del-semem-graph.sh</code> - utility to clear the graph, this is handy for testing</li>
<li><code>docs/tt.ttl</code> - the configuration (paths, graph names etc) for the Transmission</li>
<li><code>docs/endpoints.json</code> - the store defn</li>
<li><code>scripts/transmissions.sh</code> - this runs the Transmission pipeline <code>md-to-sparqlstore</code>, all it contains is :</li>
</ul>
<pre><code class="language-sh">cd ~/hyperdata/transmissions # my local path
./trans -v md-to-sparqlstore ~/hyperdata/semem/docs
</code></pre>
<p>The Transmissions part worked ok (confirmed by querying the store via Fuseki&#39;s UI). Right now the ingestion part is running. It is taking <em>a very long time</em>.</p>
<p>I&#39;ve far from finished confirming that all the parts of the system are working correctly. A meta-issue is that the <strong>operations are really slow</strong>. This is understandable - there&#39;s a lot going on, what with SPARQL queries, embeddings being juggled as well as remote LLM chat completion calls (I&#39;m using Mistral free tier). My next step has to be to set up some metrics, locate the bottlenecks... Ha, is obvious without looking - remote LLM calls. I need to figure out which of these have to happen in real time, which I can have running in the background (queue/scheduler needed).  </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-08-30_know-thyself.html">
        Semem, Know Thyself
    </a>
</p> <em></em><!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Claude : Memory System with ZPT Navigation Implementation</h1>
<h2>Context-Aware Memory That Adapts to Your Perspective</h2>
<p>If you&#39;ve worked with large language models, you&#39;ve likely experienced the frustration of context windows that forget earlier parts of your conversation, or the challenge of helping an AI system understand which pieces of information are relevant to your current task. The Semem project addresses these limitations by implementing a persistent semantic memory system with a novel navigation paradigm called ZPT (Zoom-Pan-Tilt).</p>
<h2>What We&#39;ve Built</h2>
<p>At its core, Semem stores conversations, documents, and extracted concepts in a knowledge graph using RDF/SPARQL technology. But rather than requiring users to write complex graph queries, we&#39;ve implemented an intuitive spatial metaphor for navigating this knowledge space.</p>
<p>The ZPT system works like adjusting a camera:</p>
<ul>
<li><strong>Zoom</strong> controls the level of abstraction: from individual entities and concepts, up through semantic units, full documents, topic communities, and the entire corpus</li>
<li><strong>Pan</strong> filters the domain: temporal ranges, keywords, specific entities, or subject areas</li>
<li><strong>Tilt</strong> changes the view style: keyword-based summaries, embedding similarity clusters, graph relationships, or temporal sequences</li>
</ul>
<h2>A Real Scenario: Research Assistant Workflow</h2>
<p>Consider Sarah, a researcher studying the intersection of ADHD and creativity. She&#39;s been having ongoing conversations with an AI assistant about her work, importing research papers, and storing insights. Here&#39;s how the ZPT system adapts to her changing needs:</p>
<p><strong>Week 1 - Initial Research</strong>
Sarah starts by telling the system about ADHD research papers she&#39;s reading. The system extracts concepts like &quot;attention deficit,&quot; &quot;hyperactivity,&quot; &quot;executive function,&quot; and stores them with vector embeddings for semantic similarity.</p>
<p><strong>Week 2 - Discovering Patterns</strong> 
When Sarah asks &quot;What connections exist between ADHD traits and creative problem-solving?&quot;, she uses:</p>
<ul>
<li><strong>Zoom</strong>: Entity level (individual concepts and their relationships)</li>
<li><strong>Pan</strong>: Keywords filtered to &quot;ADHD, creativity, cognitive flexibility&quot;</li>
<li><strong>Tilt</strong>: Graph view to see relationship networks</li>
</ul>
<p>The system retrieves not just her recent conversations, but connects concepts from papers she read weeks ago, showing how &quot;divergent thinking&quot; relates to both &quot;ADHD traits&quot; and &quot;creative output.&quot;</p>
<p><strong>Week 3 - Writing Phase</strong>
Now writing a literature review, Sarah shifts her perspective:</p>
<ul>
<li><strong>Zoom</strong>: Document level (full papers and substantial text chunks)</li>
<li><strong>Pan</strong>: Temporal filter for &quot;papers published 2020-2024&quot;</li>
<li><strong>Tilt</strong>: Temporal view to see how ideas evolved over time</li>
</ul>
<p>The same underlying knowledge graph serves both use cases, but the navigation system surfaces different aspects based on her current context.</p>
<h2>Technical Implementation</h2>
<p>The system uses several key components working together:</p>
<p><strong>Document Ingestion</strong>: Research papers, blog posts, or other documents get chunked semantically and stored with embeddings. Concepts are extracted and linked in the knowledge graph.</p>
<p><strong>Conversation Memory</strong>: Every interaction is stored with context about what was discussed, when, and how it relates to existing knowledge.</p>
<p><strong>Ragno Layer</strong>: This component decomposes text into semantic units, entities, and relationships using RDF standards, making knowledge machine-readable and queryable.</p>
<p><strong>ZPT Navigation</strong>: The spatial metaphor translates user intentions into precise graph queries without requiring technical expertise.</p>
<h2>Current Capabilities</h2>
<p>Today, you can:</p>
<ul>
<li>Ingest documents from SPARQL endpoints or direct upload</li>
<li>Have ongoing conversations that remember context across sessions</li>
<li>Use the web-based workbench to chunk documents and ask questions</li>
<li>Navigate your knowledge space using the ZPT controls</li>
<li>Get contextually relevant answers that draw from your entire knowledge history</li>
</ul>
<p>The system runs locally or in containerized deployments, with support for multiple LLM providers (Mistral, Claude, Ollama) and persistent storage in SPARQL triple stores.</p>
<h2>What Makes This Different</h2>
<p>Unlike simple RAG (Retrieval-Augmented Generation) systems that match queries to document chunks, or conversational systems that maintain only recent context, this approach treats knowledge as a navigable space. You&#39;re not just searching‚Äîyou&#39;re exploring from different vantage points.</p>
<p>The semantic web foundation means your knowledge connects not just through text similarity, but through meaningful relationships between concepts. When you ask about &quot;attention mechanisms,&quot; the system understands connections to both &quot;neural attention in AI models&quot; and &quot;cognitive attention in psychology&quot; based on how you&#39;ve used these concepts in context.</p>
<p>The result is an AI assistant that grows more useful over time, building a persistent understanding of your interests, expertise, and the conceptual frameworks you use to think about problems. Your conversations and documents become part of a queryable knowledge space that adapts its presentation to match your current perspective and goals.</p>
<h2>Development Progress</h2>
<p>This implementation completes the core memory management system that has been in development. The workbench UI now provides full access to:</p>
<ul>
<li>Memory storage and retrieval through the Ask/Tell interface</li>
<li>Document chunking via the Augment operations</li>
<li>ZPT navigation controls for filtering and organizing knowledge</li>
<li>Real-time console monitoring of memory operations</li>
<li>Cross-session persistence with intelligent relevance scoring</li>
</ul>
<p>The test workflow validation confirmed end-to-end functionality from document ingestion through semantic chunking to contextual question answering, demonstrating that the system successfully retrieves specific information from previously stored context.</p>
<p>Next development phases will focus on adaptive relevance learning, contextual memory clustering, and collaborative memory spaces for team-based knowledge management. </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-08-24_claude_memory-zpt-implementation.html">
        Claude : Memory System with ZPT Navigation Implementation
    </a>
</p> <em></em><!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Claude : Enhanced Ask Operation - User Perspective</h1>
<p><strong>Date</strong>: 2025-08-23<br><strong>Activity</strong>: User experience documentation<br><strong>Status</strong>: Current functionality</p>
<h2>Asking Questions in Semem</h2>
<p>The Ask operation is your main tool for querying stored knowledge in your semantic memory. Simply type your question in natural language and get answers drawn from your stored documents, notes, and concepts.</p>
<h2>Using the Workbench Interface</h2>
<p>In the <strong>Semantic Memory Workbench</strong>, navigate to the Ask section where you&#39;ll find a clean question input area. Type your question and click &quot;üîç Search Knowledge&quot; to get contextual responses based on everything you&#39;ve stored.</p>
<p>For <strong>MCP host users</strong> (Claude Desktop, etc.), you can suggest: <em>&quot;Use the ask tool to query my semantic memory with enhanced options for better results&quot;</em></p>
<h2>Answer Quality Options</h2>
<p>Control how thoroughly the system analyzes your question:</p>
<ul>
<li><strong>Basic</strong>: Quick responses for simple factual questions</li>
<li><strong>Standard</strong>: Balanced approach that works well for most queries (default)</li>
<li><strong>Comprehensive</strong>: Deep analysis with multiple refinement passes for complex research topics</li>
</ul>
<p>In the workbench, look for quality mode options in the Ask panel. MCP users can suggest: <em>&quot;Set the ask mode to comprehensive for detailed analysis&quot;</em></p>
<h2>Knowledge Enhancement Features</h2>
<h3>HyDE Enhancement</h3>
<p>This feature generates hypothetical documents that might contain your answer, improving search accuracy when your question uses different terminology than your stored content. Particularly useful for technical topics or when you&#39;re not sure how something was originally described in your documents.</p>
<h3>Wikipedia Integration</h3>
<p>Expands your answers by incorporating relevant Wikipedia content, giving you broader context beyond your personal knowledge base. Excellent for research topics, historical questions, or when you need authoritative background information.</p>
<h3>Wikidata Integration</h3>
<p>Provides structured, factual information from the Wikidata knowledge graph. Perfect for questions about people, organizations, dates, and relationships. Adds verified factual details that complement your stored content.</p>
<h2>Using the Enhancements</h2>
<p>In the <strong>workbench interface</strong>, you&#39;ll find checkboxes or toggles for each enhancement option in the Ask panel. Enable the ones that suit your question type.</p>
<p>For <strong>MCP host users</strong>, try suggestions like:</p>
<ul>
<li><em>&quot;Ask with HyDE enhancement for better retrieval&quot;</em></li>
<li><em>&quot;Query my knowledge using Wikipedia integration&quot;</em> </li>
<li><em>&quot;Search with Wikidata enhancement for factual details&quot;</em></li>
<li><em>&quot;Use comprehensive mode with all enhancements enabled&quot;</em></li>
</ul>
<h2>Context-Aware Responses</h2>
<p>Your questions automatically consider your current navigation context. If you&#39;ve been exploring a particular topic area using Zoom, Pan, or Tilt operations, your Ask results will be filtered and prioritized based on that context.</p>
<h2>When to Use Each Feature</h2>
<p><strong>Quick daily questions</strong>: Use basic mode without enhancements<br><strong>Research projects</strong>: Enable comprehensive mode with Wikipedia<br><strong>Technical documentation</strong>: Use HyDE when terminology might not match exactly<br><strong>Fact-checking</strong>: Enable Wikidata for verified information<br><strong>Academic work</strong>: Combine all enhancements with comprehensive mode</p>
<p>The enhanced Ask operation turns your stored knowledge into a powerful research assistant, seamlessly blending your personal content with external authoritative sources. </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-08-23_claude_ask-operation-user-guide.html">
        Claude : Enhanced Ask Operation - User Perspective
    </a>
</p> <em></em><!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Claude : Simple Verbs Parameter Synchronization</h1>
<p><strong>Date</strong>: 2025-08-23<br><strong>Activity</strong>: Infrastructure maintenance and API consistency<br><strong>Status</strong>: Completed</p>
<h2>Background</h2>
<p>The Semem system provides semantic memory functionality through two MCP (Model Context Protocol) server implementations: an HTTP server for REST API access and a STDIO server for direct MCP protocol communication. Over time, the HTTP server had evolved to include enhanced parameters for the core &quot;seven simple verbs&quot; operations, while the STDIO server retained older parameter schemas. This created inconsistency between the two interfaces.</p>
<h2>Work Completed</h2>
<h3>Parameter Schema Updates</h3>
<p>Updated the STDIO MCP server tool definitions to match the HTTP server&#39;s parameter shapes:</p>
<p><strong>TELL Operation</strong></p>
<ul>
<li>Added <code>lazy</code> parameter (boolean, default: false) for deferred processing</li>
<li>Maintains backward compatibility with existing three-parameter calls</li>
</ul>
<p><strong>ASK Operation</strong> </p>
<ul>
<li>Added <code>mode</code> parameter supporting basic/standard/comprehensive quality levels</li>
<li>Added <code>useHyDE</code> parameter for hypothetical document embedding enhancement</li>
<li>Added <code>useWikipedia</code> and <code>useWikidata</code> parameters for external knowledge integration</li>
<li>Preserved existing <code>question</code> and <code>useContext</code> parameters</li>
</ul>
<p><strong>AUGMENT Operation</strong></p>
<ul>
<li>Extended operation enum to include: auto, concepts, attributes, relationships, process_lazy, chunk_documents</li>
<li>Added backward compatibility for legacy operations: extract_concepts, generate_embedding, analyze_text  </li>
<li>Introduced <code>options</code> parameter while maintaining support for legacy <code>parameters</code></li>
<li>Implemented automatic parameter migration with debug logging</li>
</ul>
<p><strong>INSPECT Operation</strong></p>
<ul>
<li>Changed default value for <code>details</code> parameter from false to true</li>
<li>Aligns with HTTP server behavior for consistency</li>
</ul>
<h3>Implementation Details</h3>
<p>The work involved two main files:</p>
<ul>
<li><code>/mcp/index.js</code>: Updated tool schema definitions in the ListTools handler</li>
<li><code>/mcp/tools/simple-verbs.js</code>: Modified method signatures and parameter handling logic</li>
</ul>
<p>Key technical approach:</p>
<ul>
<li>Added new optional parameters with sensible defaults</li>
<li>Implemented parameter merging logic for AUGMENT (<code>parameters</code> ‚Üí <code>options</code>)</li>
<li>Extended operation switch statements to handle legacy operation names</li>
<li>Maintained all existing functionality while adding new capabilities</li>
</ul>
<h3>Validation</h3>
<p>Created test script confirming:</p>
<ul>
<li>Module imports successfully without syntax errors</li>
<li>Server starts without initialization failures  </li>
<li>All parameter combinations validate correctly</li>
<li>New and legacy parameter formats are accepted</li>
</ul>
<h2>Technical Outcomes</h2>
<ul>
<li><strong>API Consistency</strong>: Both MCP server implementations now accept identical parameter formats</li>
<li><strong>Backward Compatibility</strong>: All existing tool calls continue to function unchanged</li>
<li><strong>Enhanced Functionality</strong>: STDIO server gains access to advanced features like HyDE enhancement and external knowledge integration</li>
<li><strong>Maintenance Reduction</strong>: Single parameter schema reduces documentation and support overhead</li>
</ul>
<h2>Next Steps</h2>
<p>The synchronized simple verbs interface provides a foundation for:</p>
<ul>
<li>Unified documentation covering both server implementations</li>
<li>Consistent behavior across different access methods</li>
<li>Simplified client development against either server type</li>
</ul>
<p>This work represents infrastructure maintenance rather than feature development, but establishes consistency necessary for reliable system operation across different deployment scenarios.</p>
<h2>Files Modified</h2>
<ul>
<li><code>mcp/index.js</code>: Tool schema definitions updated</li>
<li><code>mcp/tools/simple-verbs.js</code>: Parameter handling logic enhanced</li>
<li>Created validation test script for ongoing verification</li>
</ul>
<p>The changes maintain the principle of non-breaking evolution, ensuring existing integrations continue operating while new capabilities become available through optional parameters. </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-08-23_claude_simple-verbs-synchronization.html">
        Claude : Simple Verbs Parameter Synchronization
    </a>
</p> <em></em><!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Devlog 2025-08-20</h1>
<p>We&#39;ve been mostly focused on <a href="https://github.com/danja/semem">Semem</a> in the past few weeks. The good news is that it now has a new UI. Not so good is that it isn&#39;t quite working.</p>
<p><img src="media/images/2025-08/workbench.png" alt="Semem Workbench"></p>
<p>It calls against the MCP HTTP endpoints so, in theory at least, it should work exactly the same. But the STDIO MCP interface will be a bit out of sync now. Although that shares most of the same underlying code, the calls won&#39;t yet be properly glued together.</p>
<h3>The Verbs</h3>
<ul>
<li>Tell - add data to the memory</li>
<li>Ask - query the memory</li>
<li>Augment - analyse and enhance data in the store</li>
<li>Zoom - set the level of detail of interest</li>
<li>Pan - set the domain of interest</li>
<li>Tilt - set the view of interest</li>
<li>Inspect - details for debugging</li>
</ul>
<p>POST /ask - Query the system
POST /augment - Augment content
POST /upload-document - Upload and process document files
POST /zoom - Set abstraction level
POST /pan - Set domain/filtering
POST /tilt - Set view filter
POST /zpt/navigate - Execute ZPT navigation</p>
<p>I created the Semem repo on 2024-11-18, the Transmissions repo 2024-01-25.
 </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-08-19_devlog.html">
        Devlog 2025-08-20
    </a>
</p> <em></em>
            </article>
        </div>
        <div class="main-grid-item about">
            <!--
            <h2>About</h2>
            
            -->
        </div>
    </div>
    <script src="js/menu.js"></script>
</body>

</html>