<!DOCTYPE html>
<html lang="en">

<head>
    <title>Rawer</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- #:todo remove when stable -->
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">

    <link rel="stylesheet" href="/css/fonts.css" type="text/css" />
    <link rel="stylesheet" href="/css/grid-columns.css" type="text/css" />
    <link rel="stylesheet" href="/css/style.css" type="text/css" />
    <link rel="stylesheet" href="/css/menu.css" type="text/css" />

</head>

<body>
    <header id="main-header">
        <h1 class="h-title">
           Raw<em>er</em>
        </h1>
    </header>
    <div class="grid-container">
        <div class="main-grid-item directory">
            <p><strong>Under Construction</strong></p>
            <p></p>
        </div>
        <div class="main-grid-item articles">
            <article>
                <!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Semem Docs</h1>
<p>I tidy things up a bit and organised the most recent info into something resembling a manual. It&#39;s published on GitHub pages here : <a href="https://danja.github.io/semem/">Semem Documentation</a>
 </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-06-27_docs.html">
        Semem Docs
    </a>
</p> <em></em><!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Semem on the server</h1>
<p>I just went to do something unrelated with Claude chat in the browser and noticed that I&#39;d already added a pointer to Semem MCP running on localhost. Claude can&#39;t see it, so I guess the connection must be made server-side. Ok, let me run Semem MCP on a server. It <em>should</em> be doable using <code>npx semem-mcp</code>, but I want the whole shebang on the server anyway, so I&#39;ll go the long way round.</p>
<p>It should just take :</p>
<pre><code class="language-sh">cd ~/hyperdata
git clone https://github.com/danja/semem.git
cd semem
npm install
npm run mcp:http
# or node mcp/http-server.js
</code></pre>
<p>But the version of node I have on the server is a bit old, so:</p>
<pre><code class="language-sh">sudo npm install n -g
n stable
node --version
# v22.16.0
</code></pre>
<p>I&#39;d better set it up through the nginx proxy to get https:</p>
<pre><code class="language-sh">nano /etc/nginx/sites-available/semem.conf
</code></pre>
<pre><code class="language-sh">server {
    server_name semem.tensegrity.it;

    location / {
        proxy_pass http://127.0.0.1:3000;
        proxy_set_header Host $host;
    }
    listen 80;
}
</code></pre>
<p>Add a DNS entry :</p>
<pre><code class="language-sh">semem 10800 IN CNAME tensegrity.it.
</code></pre>
<pre><code class="language-sh">ln -s /etc/nginx/sites-available/semem.conf /etc/nginx/sites-enabled/semem.conf
nginx -t
systemctl restart nginx
certbot
</code></pre>
<p>Hmm. Bad gateway. Let me try it on port 4600 and make it 127.0.0.1 rather than localhost just in case:</p>
<pre><code class="language-sh">MCP_PORT=4600 MCP_HOST=127.0.0.1 node mcp/http-server.js
</code></pre>
<p>Locally :</p>
<pre><code class="language-sh">curl https://semem.tensegrity.it/health

{&quot;status&quot;:&quot;healthy&quot;,&quot;timestamp&quot;:&quot;2025-06-22T10:17:25.243Z&quot;,&quot;services&quot;:{&quot;memoryManager&quot;:true,&quot;config&quot;:true},&quot;sessions&quot;:0}
</code></pre>
<p>Yay!</p>
<p>Oh.</p>
<p>I forgot that it&#39;s currently relying on Ollama locally.</p>
<p>I&#39;d already got support for different chat providers, but not embeddings providers, that was quasi-hardcoded to Ollama. So I&#39;ve added a bit to <a href="https://github.com/danja/hyperdata-clients">hyperdata-clients</a> and updated the relevant bits of Semem. Which is bound to break things.</p>
<p>PS. Things might be fixed enough.</p>
<p>I&#39;ve made a script containing just :</p>
<pre><code class="language-sh">#!/bin/bash

MCP_PORT=4600 MCP_HOST=127.0.0.1 node mcp/http-server.js
</code></pre>
<p>so then -</p>
<pre><code class="language-sh"> pm2 start start-mcp.sh --name semem-mcp --watch
</code></pre>
<p>and it&#39;s visible from MCP Inspector locally,</p>
<ul>
<li><a href="https://semem.tensegrity.it/mcp">https://semem.tensegrity.it/mcp</a></li>
<li>Streamable HTTP</li>
</ul>
<p>But broken. Hey ho. Next Claude Code cycle.
 </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-06-22_semem-server.html">
        Semem on the server
    </a>
</p> <em></em><!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>HyDE and Seek</h1>
<p>I have been developing recently by doing feature spikes then consolidating. It&#39;s dawned on me, quite a way down the road, that in Semem I did too much of the spike before the consolidate.</p>
<p>Yesterday I wanted to try Semem MCP with the online Claude chat (&#39;Pro&#39;). For this I needed to have the MCP server live online. But hit a snag - the default config uses Ollama locally for chat &amp; embeddings. My server is weedy, Ollama would be too much. So I signed up for a Nomic account to use their free tier for embeddedings, flipped the priority completion LLM to Mistral&#39;s API.</p>
<p>I should have anticipated issues in this part of the config. I&#39;d got the embeddings provider pretty much hardcoded. So I cycled back through <a href="https://github.com/danja/hyperdata-clients">hyperdata-clients</a> to add a common interface, using a factory. (in the same way the chat LLM interface works).</p>
<p>So far so good. I was able to get embeddings using the Nomic API working in one of the examples.</p>
<p>But then as I went to an example that needed chat, it threw up loads of issues around the config setup. It tried to load the Mistral model through Ollama. Oops.</p>
<p>So to resolve this I went to an example that I knew had worked, <code>HyDE</code>, that uses responses from prompts as hypotheticals in subsequent similarity search. Long story short, it took a lot of cycles with Claude Code to get this working again <em>properly</em>.</p>
<p>But this involved changes to core code - <code>Config</code>, <code>MemoryManager</code>... and now there are several breaking tests. Time to cycle back through them.</p>
<p>Ok, tests fixed - they had a lot of hardcoding of strings. Next I need to cycle through the other examples to check. I need to do that anyway to see what each is actually doing. But there are 84 examples in total, I may leave it for today.
 </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/hyde-seek.html">
        HyDE and Seek
    </a>
</p> <em></em><!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Semem/Claude MCP Workflows</h1>
<p>At least some of the Semem tools are now exposed over MCP.  They can be made available in Claude Code by using :</p>
<pre><code class="language-sh">claude mcp add semem npx semem-mcp
</code></pre>
<p>Right now I really need to take a step back. I&#39;ve got a lot of example files that have run at least once:</p>
<pre><code class="language-sh">find ~/hyperdata/semem/examples -type f | wc -l
79
</code></pre>
<p>There&#39;s a good chance many of those no longer work following recent changes in places like the config. I got a bit impatient with tests.</p>
<pre><code class="language-sh">Test Files  84 failed | 72 passed | 8 skipped (164)
Tests  589 failed | 1001 passed | 98 skipped (1688)
Errors  3 errors
</code></pre>
<p>Hmm, I see some are trying to call the Semem HTTP server. Ok, with it running :</p>
<pre><code class="language-sh">Test Files  84 failed | 72 passed | 8 skipped (164)
Tests  590 failed | 1000 passed | 98 skipped (1688)
Errors  2 errors
</code></pre>
<p>Claude had a go at orchestrating workflows. I was curious to see if this might be a good approach because issuing a simple(ish) command like :</p>
<pre><code class="language-prompt">run the enhanced-research-workflow tool on the content of brain-gpt-paper.md
</code></pre>
<p>- would be a neat kind of interaction. But Claude got stuck at this level. After trying and failing to run the workflow directly, it went on to run the individual tools it uses. I asked for the list, below. This is informative. A little confusing because the naming has got a bit obfuscated that to some (unnecessary) name mapping Claude inserted in there. But I should be able to pick out the pieces, look at doing the same workflow using Semem&#39;s HTTP API, orchestrated by <a href="https://github.com/danja/transmissions">Transmissions</a>.</p>
<blockquote>
Tool List in the Workflow

<p>According to the search results, the enhanced-research-workflow tries to execute these tools in sequence:</p>
<ol>
<li>semem_switch_storage_backend - Initialize SPARQL storage</li>
<li>research_ingest_documents - Enhanced document ingestion (custom tool)</li>
<li>semem_extract_concepts - Extract domain-focused concepts</li>
<li>ragno_decompose_corpus - Build knowledge graph with embeddings</li>
<li>ragno_analyze_graph - Comprehensive graph analysis</li>
<li>research_generate_insights - Generate insights (custom tool)</li>
<li>hybrid_search - Setup hybrid search capabilities (custom tool)</li>
<li>semem_generate_response - Generate executive summary</li>
<li>capture_user_feedback - Initialize feedback learning (custom tool)</li>
</ol>
<p>Tool Mapping System</p>
<p>The workflow uses a tool mapping system located in:
File: /flow/hyperdata/semem/mcp/lib/workflow-orchestrator.js</p>
<p>This orchestrator maps workflow tool names to actual MCP tool implementations using the TOOL_MAPPING object and adds the mcp__semem__ prefix for standard tools.</p>
<p>Custom Tool Implementations</p>
<p>The custom tools (like research_ingest_documents, research_generate_insights, etc.) are implemented in:
File: /flow/hyperdata/semem/mcp/tools/research-workflow-tools.js</p>
<p>The error you encountered (&quot;Required tool not available&quot;) occurs because the workflow orchestrator expects these custom tools to be available, but they may not be
properly registered or implemented in the current MCP server setup.</p>
</blockquote>
There's also a log on my desktop under :
```
~/.cache/claude-cli-nodejs/-flow-hyperdata-semem/mcp-logs-semem
```
 
</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-06-22_mcp-workflows.html">
        Semem/Claude MCP Workflows
    </a>
</p> <em></em><!-- ARTICLE CONTENT -->

<article class="post-content">
    <h1>Tensegrity Suite</h1>
<p><strong>Main components</strong></p>
<ul>
<li>Transmissions - pipeline composer, nuts &amp; bolts layer</li>
<li>Semem - semantic memory, knowledgebase interface, system layer</li>
<li>Ontologies - <em>several</em>, abstract layer</li>
<li>TIA Intelligence Agency - the goal is communities of autonomous agents, app layer</li>
<li>Client stuff - assorted tools, app layer</li>
<li>TBox - a container for experimenting with the above, quasi-production layer</li>
</ul>
<p>I gave myself a deadline of <strong>2025-06-21</strong> to get this bunch of things to some kind of minimum viable pseudo-products. If I&#39;m happy with what I&#39;ve got to show at that point, I&#39;ll be doing announcements (haranguing the lists, socials and maybe even ex-girlfriends) on <strong>2025-07-03</strong>. I&#39;m more confident than usual that I&#39;ll have at least <em>something</em> to show, I have got the lion&#39;s share of the hard parts done already. We&#39;ll see.</p>
<p>My dev process is to cycle through the subprojects, spending a day or two on each at time. Not a regular cycle : cross-dependencies often force a different direction; things always take longer than expected; distractions (often trying dev spikes, especially now with <em>vibes</em>).</p>
<h2>Goals</h2>
<p>Have fun. Hopefully produce useful things as a side effect.</p>
<h3>Philosophy</h3>
<p>This comes from wondering about cognition, what can computers, AI achieve? What can I achieve? I&#39;ll use <em>problem solving</em> as a catch-all for intentional state change.</p>
<ol>
<li>Systems can be described in terms of concepts and relationships between those concepts. <em>The human way.</em></li>
<li>Problem solving is mostly about pattern matching. Fitting unknown systems to ones where their behaviour is known.</li>
<li>All known agents, animal or artificial, at any given moment in time, have a finite amount of working memory. This acts as boundary on what they can process.</li>
<li>Concepts can often be decomposed into smaller units.</li>
<li>Groups of associated concepts can be abstracted into meta-concepts.  </li>
<li><em><em>Scale-free thinking for the win!</em></em></li>
</ol>
<h3>Filthy Lucre</h3>
<p>I can see commercial potential in this stuff (nb. a ground rule is that it&#39;s all open source), but the motivation is more an Everest-ian <em>because it&#39;s there</em> challenge, my wanting to get my head around these things and, yeah, better tools for problem-solving would be nice to have. Whatever, this lot maybe best considered an <strong>art/school project</strong>.</p>
<h3>Bag of Stuff</h3>
<p>I have been building these things as quasi-independent projects, each with their own repo. But to start seeing the utility I&#39;m after, they need joining up, which makes a big dependency graph : a <a href="https://en.wikipedia.org/wiki/Tensegrity">tensegrity</a> structure. <em>However</em> many of the dependencies are soft, <code>X helps Y</code>, rather than critical, <code>X needs Y</code> (cf. non-functional requirements). Some parts I&#39;ve been working on for a long time (<em>25 years is a long time in tech, right..?</em>), but the arrival of Deep Learning, especially that around LLMs, forced a reboot. And obviously everything else is continually evolving. <em>Sitting still is not an option.</em></p>
<p>It may seem like there&#39;s a lot of wheel reinvention, but that&#39;s mostly because I&#39;m fed up of (other people&#39;s) frameworks. My own very limited context window needs to be used on the tasty bits, not conventions. I&#39;m favouring ES module style nodejs and/or vanilla browser JS etc. so I don&#39;t have to flip languages so often.</p>
<p>All this stuff is riddled with <a href="https://it.wikipedia.org/wiki/Resource_Description_Framework">RDF</a>. I&#39;ll write a <em>Why RDF?</em> note (again) some time. But the <strong>tl;dr</strong> is twofold : the Web is the best graph knowledgebase in human history; some level of formal ontology definition helps considerably when describing things (especially knowledge).</p>
<h2>Sites</h2>
<p>Hyperdata is an arbitrary umbrella name I&#39;m using for all this stuff. I&#39;ve got the domain</p>
<p><a href="https://hyperdata.it">hyperdata.it</a> and associated hosting for any quasi-reliable service fixtures, docs etc. For experiments, maybe even useful things, I have <a href="https://strandz.it">strandz.it</a>.</p>
<p><a href="https://danny.ayers.name">danny.ayers.name</a> is the dumping ground for everything else.</p>
<h3>TIA</h3>
<p>I&#39;ve many, mostly fuzzy forms which require a lot of hand-waving to describe. The main high-level target is to create communities of intelligent agents which improve on the state of the art in solving arbitrary problems. This aspect I&#39;ve labeled <a href="https://github.com/danja/tia">TIA Intelligence Agency</a> (it has a repo but there&#39;s nothing to see yet). Some example problems below.</p>
<p>For pragmatic reasons, and to keep things conceptually simple, the environment will be multi-user chat over  <a href="https://xmpp.org/">XMPP</a>. A containerized environment will enable repeatable experimentation :</p>
<h2><a href="https://github.com/danja/tbox">tbox</a></h2>
<p>A <code>docker-compose</code> setup with :</p>
<ul>
<li>SPARQL Store (<a href="https://jena.apache.org/documentation/fuseki2/">Fuseki</a>)</li>
<li>XMPP IM Server (<a href="https://prosody.im/">Prosody</a>)</li>
<li>Support for various subprojects (agent boilerplate, all this <em>hyperdata</em> stuff)</li>
<li>Dev tooling (ssh etc, CI/CD, observability/metrics...)</li>
</ul>
<p><strong>Status 2025-05-04 :</strong> it&#39;s largely in place, needs a lot of tidying up, hardening. Only just started with telemetry</p>
<p>The subproject with the most novelty is :</p>
<h2>Semem</h2>
<p>Semantic memory for agents. I want this mostly defined in terms of interfaces (see Vocabs), keep it flexible. Running code essentially helpers, just lowish-level things like concept extraction etc.</p>
<p>Core components :</p>
<ul>
<li>SPARQL Store, RDF support (rdfjs &amp; RDF-Ext libs)</li>
<li>Embeddings vector store &amp; tools (<a href="https://github.com/facebookresearch/faiss">FAISS</a>, <a href="https://huggingface.co/nomic-ai">Nomic</a> embedding model(s))</li>
<li>Resource Store (quick &amp; dirty old school REST)</li>
<li>LLM Clients</li>
<li>Vocabularies</li>
</ul>
<p>The first 4 components are fairly self-explanatory, kind-of off the shelf. What sets it apart is an ontology-backed approach to composition &amp; choreography, <em>see Vocabs</em>...</p>
<p>The background to this is probably worth noting. A while back I had a play with <a href="https://colab.research.google.com/drive/1cRAflpG2v1rS9Nz6xpILpPouuvILpYlL">Graph RAG using SPARQL</a>, working with <a href="https://www.llamaindex.ai/">LlamaIndex</a>. Crude proof of context. I wanted <em>more</em>, but not dependent on the whims of a framework. I stumbled on <a href="https://github.com/caspianmoon/memoripy">memoripy</a> which had most of the skeleton of what I wanted. So my starting point for #:semem was a kind of rewrite that using JS &amp; SPARQL. At that point I shifted my energies back to #:transmissions, meanwhile working on the vocabulary support - very significant, <em>see Vocabs...</em>  </p>
<p><strong>Status 2025-05-04 :</strong> the basics are pretty much in place, although in need of a lot of tweaking, #:transmissions hooking in. Dependencies around here :</p>
<ul>
<li>#:semem needs #:transmissions for proper wiring.</li>
<li>#:semem isHelpedBy <em>vocabs</em> - to enable to good bits</li>
<li>#:semem is helpedBy <em>the utility stuff</em> - docs, visualizations etc.</li>
</ul>
<h2>Transmissions</h2>
<p>This is a pipeliney thing. In fancy speak, a workflow compositor.</p>
<h2>Postcraft</h2>
<h2>TIA Information Agency</h2>
<h2>Vocabs</h2>
<p> These are key. I&#39;ve determined the main things I need are : a knowledge representation language (<em>Ragno</em>), a knowledge navigation language (<em>ZPT</em>), a knowledge communication language (<em>Lingue</em>) and a methodology language (<em>UM</em>).</p>
<p><em>The primary vocab links are placeholders, they&#39;ll hopefully resolve to something useful by 2025-06-21.</em></p>
<h3><a href="https://github.com/danja/ragno">Ragno</a></h3>
<p>The Ragno ontology is used to <strong>describe a knowledgebase</strong>. General-purpose terms are pretty well covered by <a href="https://www.w3.org/2004/02/skos/">SKOS</a> already, what was demanded here were terms for specialisms that would fit well with RAG applications. I&#39;d got a lot sketched out when I stumbled on the paper <a href="https://arxiv.org/abs/2504.11544">NodeRAG: Structuring Graph-based RAG with Heterogenous Nodes</a>. The information model in that has nice intersections with what I was after, so I merged the ideas in. A significant bonus is that the paper describes algorithms for knowledge augmentation and retrieval that are a very good fit for what I was planning with Semem.</p>
<h3><a href="https://github.com/danja/zpt">Zoom, Pan, Tilt (ZPT)</a></h3>
<p>The ZPT ontology is used to <strong>describe a view of (part of) a knowledgebase</strong>. The primary purpose is to navigate the knowledge <em>Universe</em> and <strong>scope the current context</strong> of interest. The core terms are lifted from the world of cinema, with increasing levels of analogy-stretching :</p>
<ul>
<li>Zoom : the layer of abstraction/detail</li>
<li>Pan : the (sub)domain, topics of interest</li>
<li>Tilt : the parametric projection</li>
</ul>
<h3><a href="https://github.com/danja/lingue">Lingue</a></h3>
<p>The Lingue ontology is designed to facilitate communication between agents in a way that is flexible in regard to agent capabilities. It subsumes much of MCP and A2A. A key notion is negotiation, such that an optimal <em>local</em> lingue franca may be determined. Agents will have self-descriptions (? in MCP <strong>cards</strong> in A2A) that may be compared at a fairly mechanical level. Any commonalities can allow promotion to more sophisticated or domain-specific language(s).   </p>
<h3>Avalanche Development Methodology</h3>
<p>It seems clear that recent developments around the use of AI in dev call for a new paradigm in the whole process. I&#39;ve been trying to capture ideas on what works and what doesn&#39;t : <a href="https://github.com/danja/avalanche-methodology">avalanche methodology</a>.</p>
<p><em>Btw, I have a sandpile <a href="https://github.com/danja/sandquake">avalanche simulator</a></em></p>
<h2>Client Stuff</h2>
<h2>Event Bus</h2>
<p>I&#39;ve been hopping between Node.js and vanilla browser JS (with npm, webpack, vitest). A mechanism that seemed to make sense is an event bus I could use in either. So I made a quick &amp; dirty lib - &quot;evb&quot; [4]. Even though there&#39;s very little to it, I&#39;ve published it as an npm package to try and keep some consistency across the things I make.  </p>
<p>farelo postcraft  sheltopusik                        squirt  trestle       wstore
foaf-retro         mcp-o    trans-apps    a2a-o  elfquake  hyperdata          thiki            uve
atuin  evb       hyperdata-clients  node-flow                               tbox   tia    viz
 </p>

</article>
<p class="post-title h-cinzel">
    <a href="https://tensegrity.it/entries/2025-05-04_the-stack.html">
        Tensegrity Suite
    </a>
</p> <em></em>
            </article>
        </div>
        <div class="main-grid-item about">
            <!--
            <h2>About</h2>
            
            -->
        </div>
    </div>
    <script src="js/menu.js"></script>
</body>

</html>