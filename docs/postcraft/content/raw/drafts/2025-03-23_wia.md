# What I'm at, 2025-03

**Muchos.**

I tried to write things out as something like bullet points (below), but it's all very intertwingled. Try another path, start with a case study.

### Goal

I'm strongly convinced that useful **earthquake prediction** is possible with existing tech and data sources (with many caveats). I have spent enough time on the problem to have a strategy that should work for a sole developer given time (and no bills to pay). But it would be terribly inefficient to do it myself. At this point in time I believe someone else will use a similar strategy before very long. Machine learning is a key facet, but developing good algorithms for this is non-trivial.

That's where the AI comes in. Someone else is bound to do it. There a many ways of [insert sweet toki pona expression for doing something].   

*Down a layer*

I think a lot of current activity around AI dev is up it's own backside. 

---

Grr. My SPARQL Update bit isn't doing the replace on the blog entry resources.

Also I was selecting the `modified` for the most recent posts rather that `created`

Three live domains -

* https://danny.ayers.name - blog, more real-world-ish stuff
* https://hyperdata.it - Web/AI tech stuff
* https://strandz.it - placeholder. I'll be putting experimental services there

ELFQuake, newsmonitor

electronics
sites

llms!!!

terrapack



**Current status overall** is basically "*many tools that will enable things in progress, slowly approaching usability*" - ie. **bootstrapping**.

Virtually all code is **node or vanilla JS**.

In order of need-to-know.

* tbox - container
* tia - XMPP agent templates (and vocab)
* linguine - RDF vocab for inter-agent comms
* clients - API connectors
* transmissions - pipeliney thing
* postcraft - doc management thing
* semem - semantic memory using various interfaces (SPARQL knowledgebase, vector store, LLM API connectors)
* zpt - *Zoom, Pan, Tilt* : RDF vocab for knowledgebase navigation
* farelo - getting things done
* squirt - user UI
* hyperdata-desktop - dev UI
* um - unnamed mythodology


## [tbox](https://github.com/danja/tbox)

A Docker setup (based on Alpine Linux, has ssh) to provide a ~~sandpit~~ integrated ecosystem for playing with this stuff. Main components not of my making are [Prosody](https://prosody.im/) XMPP server, [Fuseki](https://jena.apache.org/documentation/fuseki2/) SPARQL store.

**Status** : Mostly set up, I need to cycle through again soon. Fuseki's working well enough for me to use it from other things, Prosody needs a bit more config. I think I've got the environment to the point where it'll pull my bits from github and run - once they're working well enough. It needs something for auth, so far I've only got hacky name/pass.

## tia Intelligence Agency

*I did think of a more usable name the other day, forgotten now...*

This will basically be just a bunch of boilerplate & templates for XMPP agents, making hook into the stuff below avaiable. Aim is to make it easy enough to code up an agent that an agent can do it.

**Status** : I had a play around last year, enough to see what was needed client-wise, got a first-pass running ([dogbot](https://github.com/danja/dogbot)).

## Transmissions

*I should probably do a video demo, there's a lot in here*

This is the pipeliney thing that's been eating most of my time. I started because there were a bunch of things I wanted to make that followed this kind of app-shape. Recurring serendipity has kept me going.

At minimum, pipelines pass along JSON objects as messages (I've also including an RDF dataset in that object here and there). Under the hood they're event-driven, run forward. I've got some flow control & concurrency, but I've only been building that out as & when needed.

The pipelines are defined in RDF Turtle, in a `transmissions.ttl` file, [example here](https://github.com/danja/transmissions/blob/main/src/applications/_pending/claude-json-converter/transmissions.ttl) for breaking a Claudio data dump into dirs/files of text (so I could find things again).

Individual processors, eg. `:FileReader` follow an ultra-simple interface, essentially :
```javascript
messageOut = process(messageIn)
```

The processors are intended to be little operations, like `:FileReader` reads a file and adds its contents to the `message` to be passed on.

The details of how a processor behaves, its *config settings* can come from 3 sources in reverse order of precedence/increasing specificity :

* a `config.ttl` in the same dir as `transmissions.ttl` - this is like default/global settings, eg. with the the [Claude example](https://github.com/danja/transmissions/blob/main/src/applications/_pending/claude-json-converter/config.ttl)
* a `manifest.ttl` that exists in a `target` (this will soon support HTTP URLs, so far only fs, see **Postcraft** below)   
* a `{ "key":"value"}` pair in the message it receives to process

One key bit of the **serendipity** I mentioned is that having all the pipeliney bits happening **automagically**, intended to make things easy for *me*, also happens to make things easier

## Postcraft

This is the canonical example of a Transmissions application.

## ns

### lingue

### zpt
* #:Zoom for the level of detail/abstraction
* #:Pan for scope, domain
* #:Tilt for the mix of layers (*a more literal analogy would be optical filters, but tilt lives with the two other terms which I'm redefining*).

#:lingue = Lingue Franche, inter-agent messaging language

### um


*New vocabs...*

## Put Somewhere...

*Rather bizarrely resuming things I was working on 20+ years ago, artifacts eg. [IdeaGraph](https://hyperdata.it/stuff/ideagraph/) mindmappy knowledgebase thing from 2003, [RDF Process Profile](https://hyperdata.it/xmlns/rpp/2001/), vocab for expressing processes from 2001.*  

A lot of fragments, but there is an overall structure of sort. Naming is for *internal purposes*. All this in GitHub somewhere.
